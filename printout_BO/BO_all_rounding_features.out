command-line args:  {'contaminate': 0, 'iterations': 100, 'num_data': 5000, 'epochs': '1', 'trials': '5', 'evaluation_cuda': '0', 'sample_method': 'random', 'eval_tasks': 'gsm8k', 'experiments_setting': 'ood', 'output_dir': 'output_joint_optimization/results_updated', 'time_limit': '100', 'lora_rank': '128', 'ucb_beta': '0.5', 'limit': '100', 'run_BO_on': 'all_fixed_features', 'model': None}
getting influence from:  influence/
random sentence created: AhvZJ
current eval task:  ['gsm8k']
evaluation tasks and weights:  {'gsm8k': (1.0, 'exact_match,strict-match')}
running BO on both data and model with fixed feature list
commonsense_qa
headqa_en
hellaswag
pubmedqa
sciq
triviaqa
truthfulqa_gen
wikitext
fixed feature list generated:
{9: 0, 10: 0, 11: 0, 12: 0, 13: 0}
{9: 0, 10: 0, 11: 0, 12: 0, 13: 1}
{9: 0, 10: 0, 11: 0, 12: 1, 13: 0}
{9: 0, 10: 0, 11: 0, 12: 1, 13: 1}
{9: 0, 10: 0, 11: 1, 12: 0, 13: 0}
{9: 0, 10: 0, 11: 1, 12: 0, 13: 1}
{9: 0, 10: 0, 11: 1, 12: 1, 13: 0}
{9: 0, 10: 0, 11: 1, 12: 1, 13: 1}
{9: 0, 10: 1, 11: 0, 12: 0, 13: 0}
{9: 0, 10: 1, 11: 0, 12: 0, 13: 1}
{9: 0, 10: 1, 11: 0, 12: 1, 13: 0}
{9: 0, 10: 1, 11: 0, 12: 1, 13: 1}
{9: 0, 10: 1, 11: 1, 12: 0, 13: 0}
{9: 0, 10: 1, 11: 1, 12: 0, 13: 1}
{9: 0, 10: 1, 11: 1, 12: 1, 13: 0}
{9: 0, 10: 1, 11: 1, 12: 1, 13: 1}
{9: 1, 10: 0, 11: 0, 12: 0, 13: 0}
{9: 1, 10: 0, 11: 0, 12: 0, 13: 1}
{9: 1, 10: 0, 11: 0, 12: 1, 13: 0}
{9: 1, 10: 0, 11: 0, 12: 1, 13: 1}
{9: 1, 10: 0, 11: 1, 12: 0, 13: 0}
{9: 1, 10: 0, 11: 1, 12: 0, 13: 1}
{9: 1, 10: 0, 11: 1, 12: 1, 13: 0}
{9: 1, 10: 0, 11: 1, 12: 1, 13: 1}
{9: 1, 10: 1, 11: 0, 12: 0, 13: 0}
{9: 1, 10: 1, 11: 0, 12: 0, 13: 1}
{9: 1, 10: 1, 11: 0, 12: 1, 13: 0}
{9: 1, 10: 1, 11: 0, 12: 1, 13: 1}
{9: 1, 10: 1, 11: 1, 12: 0, 13: 0}
{9: 1, 10: 1, 11: 1, 12: 0, 13: 1}
{9: 1, 10: 1, 11: 1, 12: 1, 13: 0}
{9: 1, 10: 1, 11: 1, 12: 1, 13: 1}
iteration:  0
input_X:  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 16, 1, 1, 1, 1, 1, 72, 0.05]
mixing data with method:  random
arranging lora config with parameters:  72 0.05 16 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 79,036,416 || all params: 8,109,297,664 || trainable%: 0.9746
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
})]
length of training data:  5000
command-line args:  {'contaminate': 0, 'iterations': 100, 'num_data': 5000, 'epochs': '1', 'trials': '5', 'evaluation_cuda': '0', 'sample_method': 'random', 'eval_tasks': 'gsm8k', 'experiments_setting': 'ood', 'output_dir': 'output_joint_optimization/results_updated', 'time_limit': '100', 'lora_rank': '128', 'ucb_beta': '0.5', 'limit': '100', 'run_BO_on': 'all_fixed_features', 'model': None}
getting influence from:  influence/
random sentence created: vJkkf
current eval task:  ['gsm8k']
evaluation tasks and weights:  {'gsm8k': (1.0, 'exact_match,strict-match')}
running BO on both data and model with fixed feature list
commonsense_qa
headqa_en
hellaswag
pubmedqa
sciq
triviaqa
truthfulqa_gen
wikitext
fixed feature list generated:
{9: 0, 10: 0, 11: 0, 12: 0, 13: 0}
{9: 0, 10: 0, 11: 0, 12: 0, 13: 1}
{9: 0, 10: 0, 11: 0, 12: 1, 13: 0}
{9: 0, 10: 0, 11: 0, 12: 1, 13: 1}
{9: 0, 10: 0, 11: 1, 12: 0, 13: 0}
{9: 0, 10: 0, 11: 1, 12: 0, 13: 1}
{9: 0, 10: 0, 11: 1, 12: 1, 13: 0}
{9: 0, 10: 0, 11: 1, 12: 1, 13: 1}
{9: 0, 10: 1, 11: 0, 12: 0, 13: 0}
{9: 0, 10: 1, 11: 0, 12: 0, 13: 1}
{9: 0, 10: 1, 11: 0, 12: 1, 13: 0}
{9: 0, 10: 1, 11: 0, 12: 1, 13: 1}
{9: 0, 10: 1, 11: 1, 12: 0, 13: 0}
{9: 0, 10: 1, 11: 1, 12: 0, 13: 1}
{9: 0, 10: 1, 11: 1, 12: 1, 13: 0}
{9: 0, 10: 1, 11: 1, 12: 1, 13: 1}
{9: 1, 10: 0, 11: 0, 12: 0, 13: 0}
{9: 1, 10: 0, 11: 0, 12: 0, 13: 1}
{9: 1, 10: 0, 11: 0, 12: 1, 13: 0}
{9: 1, 10: 0, 11: 0, 12: 1, 13: 1}
{9: 1, 10: 0, 11: 1, 12: 0, 13: 0}
{9: 1, 10: 0, 11: 1, 12: 0, 13: 1}
{9: 1, 10: 0, 11: 1, 12: 1, 13: 0}
{9: 1, 10: 0, 11: 1, 12: 1, 13: 1}
{9: 1, 10: 1, 11: 0, 12: 0, 13: 0}
{9: 1, 10: 1, 11: 0, 12: 0, 13: 1}
{9: 1, 10: 1, 11: 0, 12: 1, 13: 0}
{9: 1, 10: 1, 11: 0, 12: 1, 13: 1}
{9: 1, 10: 1, 11: 1, 12: 0, 13: 0}
{9: 1, 10: 1, 11: 1, 12: 0, 13: 1}
{9: 1, 10: 1, 11: 1, 12: 1, 13: 0}
{9: 1, 10: 1, 11: 1, 12: 1, 13: 1}
iteration:  0
input_X:  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 16, 1, 1, 1, 1, 1, 72, 0.05]
mixing data with method:  random
arranging lora config with parameters:  72 0.05 16 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 79,036,416 || all params: 8,109,297,664 || trainable%: 0.9746
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
})]
length of training data:  5000
training model...
trainable params: 79,036,416 || all params: 8,109,297,664 || trainable%: 0.9746
{'loss': 1.7495, 'grad_norm': 0.3209359049797058, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.1185, 'grad_norm': 1.0086132287979126, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 0.9254, 'grad_norm': 0.6443654894828796, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.323, 'grad_norm': 0.7516469955444336, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.0282, 'grad_norm': 0.7004236578941345, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.0031, 'grad_norm': 0.5411151051521301, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 108.3456, 'train_samples_per_second': 46.149, 'train_steps_per_second': 5.769, 'train_loss': 1.15574802051891, 'epoch': 0.21}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_0/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.37
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05]]
proposed candidate before processing: tensor([0.1320, 0.0412, 0.0666, 0.3804, 0.0453, 0.0872, 0.0189, 0.2285, 0.7649,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0406, 0.0897])
proposed candidate after normalizing: [tensor(0.1320), 0, tensor(0.0666), tensor(0.3804), 0, tensor(0.0872), 0, tensor(0.2285), 24, 0, 0, 0, 0, 0, 5, 0.08970030397176743]
iteration:  1
input_X:  [tensor(0.1320), 0, tensor(0.0666), tensor(0.3804), 0, tensor(0.0872), 0, tensor(0.2285), 24, 0, 0, 0, 0, 0, 5, 0.08970030397176743]
mixing data with method:  random
arranging lora config with parameters:  5 0.08970030397176743 24 [0, 0, 0, 0, 0]
[]
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)]]
proposed candidate before processing: tensor([0.1265, 0.1256, 0.0871, 0.0900, 0.1257, 0.1015, 0.1582, 0.1853, 0.2839,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7487, 0.0549])
proposed candidate after normalizing: [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), 9, 1, 1, 1, 1, 1, 96, 0.05487086623907089]
iteration:  2
input_X:  [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), 9, 1, 1, 1, 1, 1, 96, 0.05487086623907089]
mixing data with method:  random
arranging lora config with parameters:  96 0.05487086623907089 9 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 93,855,744 || all params: 8,124,116,992 || trainable%: 1.1553
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1265)
number of datapoints needed (ratio * total):  632
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1256)
number of datapoints needed (ratio * total):  628
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0871)
number of datapoints needed (ratio * total):  435
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0900)
number of datapoints needed (ratio * total):  450
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1257)
number of datapoints needed (ratio * total):  628
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1015)
number of datapoints needed (ratio * total):  507
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1582)
number of datapoints needed (ratio * total):  791
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1853)
number of datapoints needed (ratio * total):  926
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 632
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 628
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 435
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 450
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 628
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 507
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 791
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 926
})]
length of training data:  4997
training model...
trainable params: 93,855,744 || all params: 8,124,116,992 || trainable%: 1.1553
{'loss': 1.3958, 'grad_norm': 0.4815824031829834, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 0.9904, 'grad_norm': 1.05626380443573, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.4448, 'grad_norm': 0.6086840629577637, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.2552, 'grad_norm': 0.9692512154579163, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.343, 'grad_norm': 0.4289804697036743, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.0908, 'grad_norm': 0.7488913536071777, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0509, 'train_samples_per_second': 49.945, 'train_steps_per_second': 6.247, 'train_loss': 1.2525494939131703, 'epoch': 0.22}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_2/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)]]
proposed candidate before processing: tensor([0.1274, 0.1260, 0.0626, 0.0673, 0.1261, 0.0863, 0.1798, 0.2245, 0.1437,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8697, 0.0580])
proposed candidate after normalizing: [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), 5, 1, 1, 1, 1, 1, 111, 0.05803591385483742]
iteration:  3
input_X:  [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), 5, 1, 1, 1, 1, 1, 111, 0.05803591385483742]
mixing data with method:  random
arranging lora config with parameters:  111 0.05803591385483742 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 64,422,912 || all params: 8,094,684,160 || trainable%: 0.7959
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1274)
number of datapoints needed (ratio * total):  637
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1260)
number of datapoints needed (ratio * total):  630
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0626)
number of datapoints needed (ratio * total):  312
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0673)
number of datapoints needed (ratio * total):  336
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1261)
number of datapoints needed (ratio * total):  630
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0863)
number of datapoints needed (ratio * total):  431
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1798)
number of datapoints needed (ratio * total):  899
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2245)
number of datapoints needed (ratio * total):  1122
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 637
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 630
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 312
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 336
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 630
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 431
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 899
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1122
})]
length of training data:  4997
training model...
trainable params: 64,422,912 || all params: 8,094,684,160 || trainable%: 0.7959
{'loss': 1.9243, 'grad_norm': 0.9350427389144897, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.0913, 'grad_norm': 0.39657244086265564, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.7368, 'grad_norm': 0.44331657886505127, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3799, 'grad_norm': 0.6944347620010376, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.4554, 'grad_norm': 0.5502980947494507, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1051, 'grad_norm': 0.6656253933906555, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.0257, 'grad_norm': 0.44584864377975464, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1087, 'train_samples_per_second': 49.916, 'train_steps_per_second': 6.243, 'train_loss': 1.389196470672009, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_3/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.68
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)]]
proposed candidate before processing: tensor([0.1236, 0.1265, 0.0615, 0.0262, 0.1257, 0.0530, 0.2146, 0.2689, 0.0804,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7345, 0.0626])
proposed candidate after normalizing: [tensor(0.1236), tensor(0.1265), tensor(0.0615), 0, tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), 3, 1, 1, 1, 1, 1, 94, 0.06263022869825363]
iteration:  4
input_X:  [tensor(0.1236), tensor(0.1265), tensor(0.0615), 0, tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), 3, 1, 1, 1, 1, 1, 94, 0.06263022869825363]
mixing data with method:  random
arranging lora config with parameters:  94 0.06263022869825363 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 34,578,432 || all params: 8,064,839,680 || trainable%: 0.4288
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1236), tensor(0.1265), tensor(0.0615), 0, tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1236)
number of datapoints needed (ratio * total):  618
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1265)
number of datapoints needed (ratio * total):  632
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0615)
number of datapoints needed (ratio * total):  307
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1257)
number of datapoints needed (ratio * total):  628
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0530)
number of datapoints needed (ratio * total):  265
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2146)
number of datapoints needed (ratio * total):  1072
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2689)
number of datapoints needed (ratio * total):  1344
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 618
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 632
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 307
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 628
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 265
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1072
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1344
})]
length of training data:  4866
training model...
trainable params: 34,578,432 || all params: 8,064,839,680 || trainable%: 0.4288
{'loss': 1.7445, 'grad_norm': 0.9930063486099243, 'learning_rate': 0.0002949916527545909, 'epoch': 0.03}
{'loss': 1.5931, 'grad_norm': 0.9989227652549744, 'learning_rate': 0.0002849749582637729, 'epoch': 0.07}
{'loss': 1.5089, 'grad_norm': 0.6616718173027039, 'learning_rate': 0.00027495826377295493, 'epoch': 0.1}
{'loss': 1.377, 'grad_norm': 0.61284339427948, 'learning_rate': 0.0002649415692821369, 'epoch': 0.13}
{'loss': 1.4545, 'grad_norm': 0.9841107130050659, 'learning_rate': 0.0002549248747913189, 'epoch': 0.16}
{'loss': 1.3366, 'grad_norm': 0.387585312128067, 'learning_rate': 0.0002449081803005008, 'epoch': 0.2}
{'loss': 1.3748, 'grad_norm': 0.827586829662323, 'learning_rate': 0.00023489148580968278, 'epoch': 0.23}
{'loss': 1.5569, 'grad_norm': 0.5626280903816223, 'learning_rate': 0.00022487479131886475, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3461, 'train_samples_per_second': 48.492, 'train_steps_per_second': 6.069, 'train_loss': 1.4784748682718791, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_4/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.6
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)]]
proposed candidate before processing: tensor([0.1184, 0.1150, 0.0552, 0.1162, 0.1483, 0.1610, 0.1499, 0.1361, 0.1344,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9051, 0.0483])
proposed candidate after normalizing: [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), 4, 1, 1, 1, 1, 1, 116, 0.04832034185528755]
iteration:  5
input_X:  [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), 4, 1, 1, 1, 1, 1, 116, 0.04832034185528755]
mixing data with method:  random
arranging lora config with parameters:  116 0.04832034185528755 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 31,834,112 || all params: 8,062,095,360 || trainable%: 0.3949
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1184)
number of datapoints needed (ratio * total):  591
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1150)
number of datapoints needed (ratio * total):  574
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0552)
number of datapoints needed (ratio * total):  276
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1162)
number of datapoints needed (ratio * total):  580
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1483)
number of datapoints needed (ratio * total):  741
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1610)
number of datapoints needed (ratio * total):  804
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1499)
number of datapoints needed (ratio * total):  749
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1361)
number of datapoints needed (ratio * total):  680
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 591
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 574
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 276
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 580
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 741
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 804
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 749
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 680
})]
length of training data:  4995
training model...
trainable params: 31,834,112 || all params: 8,062,095,360 || trainable%: 0.3949
{'loss': 2.0664, 'grad_norm': 0.6532528400421143, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.2884, 'grad_norm': 1.154758095741272, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.0981, 'grad_norm': 0.5798628926277161, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.1502, 'grad_norm': 0.6489574313163757, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.6399, 'grad_norm': 0.6422482132911682, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1006, 'grad_norm': 0.8225441575050354, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.2537, 'grad_norm': 1.376872181892395, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5782, 'train_samples_per_second': 49.663, 'train_steps_per_second': 6.214, 'train_loss': 1.3402342796325684, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_5/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.52
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)]]
proposed candidate before processing: tensor([0.1426, 0.1402, 0.0818, 0.0477, 0.0966, 0.0223, 0.1804, 0.2885, 0.2629,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9048, 0.0657])
proposed candidate after normalizing: [tensor(0.1426), tensor(0.1402), tensor(0.0818), 0, tensor(0.0966), 0, tensor(0.1804), tensor(0.2885), 8, 1, 1, 1, 1, 1, 116, 0.06569799035787582]
iteration:  6
input_X:  [tensor(0.1426), tensor(0.1402), tensor(0.0818), 0, tensor(0.0966), 0, tensor(0.1804), tensor(0.2885), 8, 1, 1, 1, 1, 1, 116, 0.06569799035787582]
mixing data with method:  random
arranging lora config with parameters:  116 0.06569799035787582 8 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 63,668,224 || all params: 8,093,929,472 || trainable%: 0.7866
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1426), tensor(0.1402), tensor(0.0818), 0, tensor(0.0966), 0, tensor(0.1804), tensor(0.2885)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1426)
number of datapoints needed (ratio * total):  713
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1402)
number of datapoints needed (ratio * total):  700
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0818)
number of datapoints needed (ratio * total):  408
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0966)
number of datapoints needed (ratio * total):  482
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1804)
number of datapoints needed (ratio * total):  902
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2885)
number of datapoints needed (ratio * total):  1442
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 713
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 700
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 408
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 482
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 902
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1442
})]
length of training data:  4647
training model...
trainable params: 63,668,224 || all params: 8,093,929,472 || trainable%: 0.7866
{'loss': 2.2005, 'grad_norm': 0.6572036743164062, 'learning_rate': 0.00029474605954465846, 'epoch': 0.03}
{'loss': 1.7779, 'grad_norm': 0.7901034355163574, 'learning_rate': 0.0002842381786339754, 'epoch': 0.07}
{'loss': 1.5755, 'grad_norm': 0.4582004249095917, 'learning_rate': 0.00027373029772329244, 'epoch': 0.1}
{'loss': 1.6428, 'grad_norm': 1.0931389331817627, 'learning_rate': 0.00026322241681260946, 'epoch': 0.14}
{'loss': 1.4589, 'grad_norm': 0.8351338505744934, 'learning_rate': 0.0002527145359019264, 'epoch': 0.17}
{'loss': 1.5952, 'grad_norm': 0.47846025228500366, 'learning_rate': 0.0002422066549912434, 'epoch': 0.21}
{'loss': 1.6559, 'grad_norm': 0.4122251272201538, 'learning_rate': 0.0002316987740805604, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6069, 'train_samples_per_second': 46.19, 'train_steps_per_second': 5.775, 'train_loss': 1.6454568059189514, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_6/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.58
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)]]
proposed candidate before processing: tensor([0.1524, 0.1278, 0.1060, 0.0820, 0.0976, 0.0684, 0.1300, 0.2357, 0.1491,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8001, 0.0642])
proposed candidate after normalizing: [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), 5, 1, 1, 1, 1, 1, 102, 0.06419891119003296]
iteration:  7
input_X:  [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), 5, 1, 1, 1, 1, 1, 102, 0.06419891119003296]
mixing data with method:  random
arranging lora config with parameters:  102 0.06419891119003296 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 58,865,664 || all params: 8,089,126,912 || trainable%: 0.7277
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1524)
number of datapoints needed (ratio * total):  762
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1278)
number of datapoints needed (ratio * total):  638
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1060)
number of datapoints needed (ratio * total):  530
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0820)
number of datapoints needed (ratio * total):  410
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0976)
number of datapoints needed (ratio * total):  488
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0684)
number of datapoints needed (ratio * total):  342
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1300)
number of datapoints needed (ratio * total):  649
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2357)
number of datapoints needed (ratio * total):  1178
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 762
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 638
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 530
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 410
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 488
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 342
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 649
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1178
})]
length of training data:  4997
training model...
trainable params: 58,865,664 || all params: 8,089,126,912 || trainable%: 0.7277
{'loss': 1.5575, 'grad_norm': 0.988594114780426, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.1609, 'grad_norm': 2.340615749359131, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.2455, 'grad_norm': 0.5368170142173767, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.2398, 'grad_norm': 1.1651474237442017, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.4175, 'grad_norm': 0.6981043815612793, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.7423, 'grad_norm': 0.5617238283157349, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.1369, 'grad_norm': 1.7018637657165527, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2751, 'train_samples_per_second': 49.833, 'train_steps_per_second': 6.233, 'train_loss': 1.3555164975179752, 'epoch': 0.23}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_7/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.7
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)]]
proposed candidate before processing: tensor([0.1714, 0.1508, 0.1295, 0.0595, 0.1071, 0.0481, 0.1317, 0.2019, 0.1442,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8237, 0.0738])
proposed candidate after normalizing: [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), 0, tensor(0.1317), tensor(0.2019), 5, 1, 1, 1, 1, 1, 105, 0.07377563416957855]
iteration:  8
input_X:  [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), 0, tensor(0.1317), tensor(0.2019), 5, 1, 1, 1, 1, 1, 105, 0.07377563416957855]
mixing data with method:  random
arranging lora config with parameters:  105 0.07377563416957855 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 36,019,200 || all params: 8,066,280,448 || trainable%: 0.4465
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), 0, tensor(0.1317), tensor(0.2019)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1714)
number of datapoints needed (ratio * total):  856
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1508)
number of datapoints needed (ratio * total):  754
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1295)
number of datapoints needed (ratio * total):  647
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0595)
number of datapoints needed (ratio * total):  297
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1071)
number of datapoints needed (ratio * total):  535
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1317)
number of datapoints needed (ratio * total):  658
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2019)
number of datapoints needed (ratio * total):  1009
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 856
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 754
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 647
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 297
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 535
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 658
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1009
})]
length of training data:  4756
training model...
trainable params: 36,019,200 || all params: 8,066,280,448 || trainable%: 0.4465
{'loss': 1.6742, 'grad_norm': 1.1163488626480103, 'learning_rate': 0.00029487179487179484, 'epoch': 0.03}
{'loss': 1.5014, 'grad_norm': 0.8182982802391052, 'learning_rate': 0.00028461538461538457, 'epoch': 0.07}
{'loss': 1.1713, 'grad_norm': 0.4502369463443756, 'learning_rate': 0.0002743589743589743, 'epoch': 0.1}
{'loss': 1.5347, 'grad_norm': 0.8452393412590027, 'learning_rate': 0.0002641025641025641, 'epoch': 0.13}
{'loss': 1.3879, 'grad_norm': 0.6111968159675598, 'learning_rate': 0.0002538461538461538, 'epoch': 0.17}
{'loss': 1.355, 'grad_norm': 0.5693578124046326, 'learning_rate': 0.00024358974358974357, 'epoch': 0.2}
{'loss': 1.5191, 'grad_norm': 0.4236687123775482, 'learning_rate': 0.0002333333333333333, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6328, 'train_samples_per_second': 47.261, 'train_steps_per_second': 5.913, 'train_loss': 1.448298875977393, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_8/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)]]
proposed candidate before processing: tensor([0.1308, 0.0843, 0.1003, 0.1129, 0.0862, 0.1152, 0.1169, 0.2534, 0.1606,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8117, 0.0434])
proposed candidate after normalizing: [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), 5, 1, 1, 1, 1, 1, 104, 0.04340554401278496]
iteration:  9
input_X:  [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), 5, 1, 1, 1, 1, 1, 104, 0.04340554401278496]
mixing data with method:  random
arranging lora config with parameters:  104 0.04340554401278496 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 35,676,160 || all params: 8,065,937,408 || trainable%: 0.4423
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1308)
number of datapoints needed (ratio * total):  654
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0843)
number of datapoints needed (ratio * total):  421
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1003)
number of datapoints needed (ratio * total):  501
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1129)
number of datapoints needed (ratio * total):  564
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0862)
number of datapoints needed (ratio * total):  430
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1152)
number of datapoints needed (ratio * total):  575
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1169)
number of datapoints needed (ratio * total):  584
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2534)
number of datapoints needed (ratio * total):  1266
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 654
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 421
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 501
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 564
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 430
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 575
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 584
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1266
})]
length of training data:  4995
training model...
trainable params: 35,676,160 || all params: 8,065,937,408 || trainable%: 0.4423
{'loss': 2.2689, 'grad_norm': 0.6392196416854858, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.6197, 'grad_norm': 0.5437621474266052, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3129, 'grad_norm': 0.8841387629508972, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.7118, 'grad_norm': 0.8657360076904297, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.6344, 'grad_norm': 0.47271281480789185, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.0971, 'grad_norm': 1.1270365715026855, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.1494, 'grad_norm': 1.1231539249420166, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.211, 'train_samples_per_second': 49.845, 'train_steps_per_second': 6.237, 'train_loss': 1.5455158787804681, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_9/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.56
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)]]
proposed candidate before processing: tensor([0.1502, 0.1867, 0.0382, 0.0917, 0.1296, 0.0486, 0.1531, 0.2019, 0.1613,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7945, 0.0896])
proposed candidate after normalizing: [tensor(0.1502), tensor(0.1867), 0, tensor(0.0917), tensor(0.1296), 0, tensor(0.1531), tensor(0.2019), 5, 1, 1, 1, 1, 1, 102, 0.08960119634866714]
iteration:  10
input_X:  [tensor(0.1502), tensor(0.1867), 0, tensor(0.0917), tensor(0.1296), 0, tensor(0.1531), tensor(0.2019), 5, 1, 1, 1, 1, 1, 102, 0.08960119634866714]
mixing data with method:  random
arranging lora config with parameters:  102 0.08960119634866714 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 34,990,080 || all params: 8,065,251,328 || trainable%: 0.4338
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1502), tensor(0.1867), 0, tensor(0.0917), tensor(0.1296), 0, tensor(0.1531), tensor(0.2019)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1502)
number of datapoints needed (ratio * total):  751
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1867)
number of datapoints needed (ratio * total):  933
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0917)
number of datapoints needed (ratio * total):  458
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1296)
number of datapoints needed (ratio * total):  648
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1531)
number of datapoints needed (ratio * total):  765
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2019)
number of datapoints needed (ratio * total):  1009
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 751
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 933
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 458
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 648
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 765
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1009
})]
length of training data:  4564
training model...
trainable params: 34,990,080 || all params: 8,065,251,328 || trainable%: 0.4338
{'loss': 2.2062, 'grad_norm': 0.4076310992240906, 'learning_rate': 0.00029465240641711227, 'epoch': 0.04}
{'loss': 1.7511, 'grad_norm': 0.8117069602012634, 'learning_rate': 0.00028395721925133686, 'epoch': 0.07}
{'loss': 1.3017, 'grad_norm': 2.037882089614868, 'learning_rate': 0.0002732620320855615, 'epoch': 0.11}
{'loss': 1.5803, 'grad_norm': 1.0050636529922485, 'learning_rate': 0.0002625668449197861, 'epoch': 0.14}
{'loss': 1.3405, 'grad_norm': 0.4420548379421234, 'learning_rate': 0.0002518716577540107, 'epoch': 0.18}
{'loss': 1.6405, 'grad_norm': 0.6453396677970886, 'learning_rate': 0.00024117647058823527, 'epoch': 0.21}
{'loss': 1.3101, 'grad_norm': 0.8517459630966187, 'learning_rate': 0.0002304812834224599, 'epoch': 0.25}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2571, 'train_samples_per_second': 45.523, 'train_steps_per_second': 5.695, 'train_loss': 1.5485572691886655, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_10/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.55
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)]]
proposed candidate before processing: tensor([0.1504, 0.1063, 0.1651, 0.0036, 0.1202, 0.0673, 0.1858, 0.2013, 0.1618,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8196, 0.0476])
proposed candidate after normalizing: [tensor(0.1504), tensor(0.1063), tensor(0.1651), 0, tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), 5, 1, 1, 1, 1, 1, 105, 0.047613490372896194]
iteration:  11
input_X:  [tensor(0.1504), tensor(0.1063), tensor(0.1651), 0, tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), 5, 1, 1, 1, 1, 1, 105, 0.047613490372896194]
mixing data with method:  random
arranging lora config with parameters:  105 0.047613490372896194 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 36,019,200 || all params: 8,066,280,448 || trainable%: 0.4465
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1504), tensor(0.1063), tensor(0.1651), 0, tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1504)
number of datapoints needed (ratio * total):  751
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1063)
number of datapoints needed (ratio * total):  531
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1651)
number of datapoints needed (ratio * total):  825
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1202)
number of datapoints needed (ratio * total):  600
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0673)
number of datapoints needed (ratio * total):  336
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1858)
number of datapoints needed (ratio * total):  929
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2013)
number of datapoints needed (ratio * total):  1006
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 751
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 531
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 825
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 600
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 336
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 929
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1006
})]
length of training data:  4978
training model...
trainable params: 36,019,200 || all params: 8,066,280,448 || trainable%: 0.4465
{'loss': 1.8209, 'grad_norm': 0.9731025099754333, 'learning_rate': 0.00029510603588907015, 'epoch': 0.03}
{'loss': 1.4842, 'grad_norm': 0.5609641075134277, 'learning_rate': 0.00028531810766721044, 'epoch': 0.06}
{'loss': 1.3336, 'grad_norm': 0.5321020483970642, 'learning_rate': 0.00027553017944535074, 'epoch': 0.1}
{'loss': 1.2987, 'grad_norm': 0.7710438966751099, 'learning_rate': 0.00026574225122349103, 'epoch': 0.13}
{'loss': 1.2651, 'grad_norm': 1.184828758239746, 'learning_rate': 0.0002559543230016313, 'epoch': 0.16}
{'loss': 1.4375, 'grad_norm': 0.5999953150749207, 'learning_rate': 0.0002461663947797716, 'epoch': 0.19}
{'loss': 1.4238, 'grad_norm': 0.4474237859249115, 'learning_rate': 0.00023637846655791189, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6504, 'train_samples_per_second': 49.458, 'train_steps_per_second': 6.19, 'train_loss': 1.4171857165682846, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_11/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.55
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)]]
proposed candidate before processing: tensor([0.1063, 0.1544, 0.1179, 0.0731, 0.0975, 0.0498, 0.1291, 0.2719, 0.1284,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8178, 0.0684])
proposed candidate after normalizing: [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), 0, tensor(0.1291), tensor(0.2719), 4, 1, 1, 1, 1, 1, 105, 0.06837198138237]
iteration:  12
input_X:  [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), 0, tensor(0.1291), tensor(0.2719), 4, 1, 1, 1, 1, 1, 105, 0.06837198138237]
mixing data with method:  random
arranging lora config with parameters:  105 0.06837198138237 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 36,019,200 || all params: 8,066,280,448 || trainable%: 0.4465
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), 0, tensor(0.1291), tensor(0.2719)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1063)
number of datapoints needed (ratio * total):  531
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1544)
number of datapoints needed (ratio * total):  771
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1179)
number of datapoints needed (ratio * total):  589
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0731)
number of datapoints needed (ratio * total):  365
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0975)
number of datapoints needed (ratio * total):  487
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1291)
number of datapoints needed (ratio * total):  645
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2719)
number of datapoints needed (ratio * total):  1359
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 531
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 771
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 589
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 365
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 487
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 645
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1359
})]
length of training data:  4747
training model...
trainable params: 36,019,200 || all params: 8,066,280,448 || trainable%: 0.4465
{'loss': 1.6421, 'grad_norm': 0.5937771797180176, 'learning_rate': 0.00029486301369863015, 'epoch': 0.03}
{'loss': 0.9169, 'grad_norm': 0.6506527066230774, 'learning_rate': 0.0002845890410958904, 'epoch': 0.07}
{'loss': 1.6685, 'grad_norm': 1.11075758934021, 'learning_rate': 0.0002743150684931507, 'epoch': 0.1}
{'loss': 1.3147, 'grad_norm': 0.853131115436554, 'learning_rate': 0.0002640410958904109, 'epoch': 0.13}
{'loss': 1.3963, 'grad_norm': 0.8728457689285278, 'learning_rate': 0.0002537671232876712, 'epoch': 0.17}
{'loss': 1.5244, 'grad_norm': 1.2413407564163208, 'learning_rate': 0.0002434931506849315, 'epoch': 0.2}
{'loss': 1.3628, 'grad_norm': 2.7935733795166016, 'learning_rate': 0.00023321917808219177, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5946, 'train_samples_per_second': 47.189, 'train_steps_per_second': 5.905, 'train_loss': 1.4131413868495397, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_12/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.69
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)]]
proposed candidate before processing: tensor([0.0691, 0.1361, 0.1156, 0.0982, 0.1183, 0.0701, 0.1345, 0.2580, 0.1343,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8183, 0.0485])
proposed candidate after normalizing: [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), 4, 1, 1, 1, 1, 1, 105, 0.048471368849277496]
iteration:  13
input_X:  [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), 4, 1, 1, 1, 1, 1, 105, 0.048471368849277496]
mixing data with method:  random
arranging lora config with parameters:  105 0.048471368849277496 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 28,815,360 || all params: 8,059,076,608 || trainable%: 0.3576
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0691)
number of datapoints needed (ratio * total):  345
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1361)
number of datapoints needed (ratio * total):  680
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1156)
number of datapoints needed (ratio * total):  578
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0982)
number of datapoints needed (ratio * total):  491
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1183)
number of datapoints needed (ratio * total):  591
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0701)
number of datapoints needed (ratio * total):  350
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1345)
number of datapoints needed (ratio * total):  672
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2580)
number of datapoints needed (ratio * total):  1290
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 345
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 680
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 578
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 491
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 591
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 350
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 672
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1290
})]
length of training data:  4997
training model...
trainable params: 28,815,360 || all params: 8,059,076,608 || trainable%: 0.3576
{'loss': 2.2422, 'grad_norm': 0.4340425133705139, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.2978, 'grad_norm': 1.4399058818817139, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.5272, 'grad_norm': 1.1411138772964478, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.6207, 'grad_norm': 1.1304301023483276, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.672, 'grad_norm': 0.37174513936042786, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.528, 'grad_norm': 0.7253433465957642, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.2956, 'grad_norm': 0.7126978635787964, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2123, 'train_samples_per_second': 49.864, 'train_steps_per_second': 6.237, 'train_loss': 1.5764954884847004, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_13/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.56
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)]]
proposed candidate before processing: tensor([0.1405, 0.0846, 0.1176, 0.0612, 0.0654, 0.1495, 0.1247, 0.2567, 0.1356,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7731, 0.0564])
proposed candidate after normalizing: [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), 4, 1, 1, 1, 1, 1, 99, 0.056369565427303314]
iteration:  14
input_X:  [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), 4, 1, 1, 1, 1, 1, 99, 0.056369565427303314]
mixing data with method:  random
arranging lora config with parameters:  99 0.056369565427303314 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 27,168,768 || all params: 8,057,430,016 || trainable%: 0.3372
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1405)
number of datapoints needed (ratio * total):  702
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0846)
number of datapoints needed (ratio * total):  422
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1176)
number of datapoints needed (ratio * total):  587
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0612)
number of datapoints needed (ratio * total):  305
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0654)
number of datapoints needed (ratio * total):  326
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1495)
number of datapoints needed (ratio * total):  747
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1247)
number of datapoints needed (ratio * total):  623
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2567)
number of datapoints needed (ratio * total):  1283
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 702
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 422
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 587
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 305
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 326
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 747
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 623
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1283
})]
length of training data:  4995
training model...
trainable params: 27,168,768 || all params: 8,057,430,016 || trainable%: 0.3372
{'loss': 2.4062, 'grad_norm': 0.6672376990318298, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5813, 'grad_norm': 0.47413966059684753, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.367, 'grad_norm': 0.889289915561676, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.5326, 'grad_norm': 0.7005782723426819, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.7052, 'grad_norm': 0.4529622793197632, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1603, 'grad_norm': 0.843902587890625, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.3907, 'grad_norm': 1.4038236141204834, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2616, 'train_samples_per_second': 49.82, 'train_steps_per_second': 6.234, 'train_loss': 1.5890906143188477, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_14/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)]]
proposed candidate before processing: tensor([0.1611, 0.0948, 0.1200, 0.0661, 0.1155, 0.0453, 0.1637, 0.2335, 0.1349,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8009, 0.0433])
proposed candidate after normalizing: [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), 0, tensor(0.1637), tensor(0.2335), 4, 1, 1, 1, 1, 1, 103, 0.0433359369635582]
iteration:  15
input_X:  [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), 0, tensor(0.1637), tensor(0.2335), 4, 1, 1, 1, 1, 1, 103, 0.0433359369635582]
mixing data with method:  random
arranging lora config with parameters:  103 0.0433359369635582 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 28,266,496 || all params: 8,058,527,744 || trainable%: 0.3508
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), 0, tensor(0.1637), tensor(0.2335)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1611)
number of datapoints needed (ratio * total):  805
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0948)
number of datapoints needed (ratio * total):  473
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1200)
number of datapoints needed (ratio * total):  599
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0661)
number of datapoints needed (ratio * total):  330
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1155)
number of datapoints needed (ratio * total):  577
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1637)
number of datapoints needed (ratio * total):  818
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2335)
number of datapoints needed (ratio * total):  1167
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 805
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 473
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 599
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 330
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 577
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 818
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1167
})]
length of training data:  4769
training model...
trainable params: 28,266,496 || all params: 8,058,527,744 || trainable%: 0.3508
{'loss': 2.0146, 'grad_norm': 1.3196241855621338, 'learning_rate': 0.00029488926746166946, 'epoch': 0.03}
{'loss': 1.68, 'grad_norm': 2.08345890045166, 'learning_rate': 0.0002846678023850085, 'epoch': 0.07}
{'loss': 1.6045, 'grad_norm': 0.792583167552948, 'learning_rate': 0.0002744463373083475, 'epoch': 0.1}
{'loss': 1.4948, 'grad_norm': 0.48899316787719727, 'learning_rate': 0.00026422487223168653, 'epoch': 0.13}
{'loss': 1.2682, 'grad_norm': 0.9525026679039001, 'learning_rate': 0.0002540034071550255, 'epoch': 0.17}
{'loss': 1.4406, 'grad_norm': 0.6827448010444641, 'learning_rate': 0.00024378194207836453, 'epoch': 0.2}
{'loss': 1.2905, 'grad_norm': 0.3180397152900696, 'learning_rate': 0.00023356047700170358, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1397, 'train_samples_per_second': 47.623, 'train_steps_per_second': 5.962, 'train_loss': 1.5257477355159186, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_15/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.51
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)]]
proposed candidate before processing: tensor([0.1403, 0.1855, 0.0932, 0.0171, 0.0385, 0.1274, 0.1123, 0.2857, 0.1450,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8198, 0.1000])
proposed candidate after normalizing: [tensor(0.1403), tensor(0.1855), tensor(0.0932), 0, 0, tensor(0.1274), tensor(0.1123), tensor(0.2857), 5, 1, 1, 1, 1, 1, 105, 0.10000000149011612]
iteration:  16
input_X:  [tensor(0.1403), tensor(0.1855), tensor(0.0932), 0, 0, tensor(0.1274), tensor(0.1123), tensor(0.2857), 5, 1, 1, 1, 1, 1, 105, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  105 0.10000000149011612 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 36,019,200 || all params: 8,066,280,448 || trainable%: 0.4465
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1403), tensor(0.1855), tensor(0.0932), 0, 0, tensor(0.1274), tensor(0.1123), tensor(0.2857)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1403)
number of datapoints needed (ratio * total):  701
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1855)
number of datapoints needed (ratio * total):  927
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0932)
number of datapoints needed (ratio * total):  466
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  tensor(0.1274)
number of datapoints needed (ratio * total):  637
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1123)
number of datapoints needed (ratio * total):  561
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2857)
number of datapoints needed (ratio * total):  1428
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 701
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 927
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 466
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 637
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 561
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1428
})]
length of training data:  4720
training model...
trainable params: 36,019,200 || all params: 8,066,280,448 || trainable%: 0.4465
{'loss': 2.1891, 'grad_norm': 0.7293573617935181, 'learning_rate': 0.0002948275862068965, 'epoch': 0.03}
{'loss': 1.7302, 'grad_norm': 1.041149377822876, 'learning_rate': 0.0002844827586206896, 'epoch': 0.07}
{'loss': 1.6581, 'grad_norm': 0.8719284534454346, 'learning_rate': 0.0002741379310344827, 'epoch': 0.1}
{'loss': 1.1405, 'grad_norm': 1.566169261932373, 'learning_rate': 0.00026379310344827584, 'epoch': 0.14}
{'loss': 1.1887, 'grad_norm': 0.3861449062824249, 'learning_rate': 0.00025344827586206895, 'epoch': 0.17}
{'loss': 1.4757, 'grad_norm': 0.7778922319412231, 'learning_rate': 0.00024310344827586203, 'epoch': 0.2}
{'loss': 1.5624, 'grad_norm': 0.7632532119750977, 'learning_rate': 0.00023275862068965515, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5309, 'train_samples_per_second': 46.951, 'train_steps_per_second': 5.869, 'train_loss': 1.491830886938633, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_16/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.59
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1239, 0.1674, 0.1063, 0.0759, 0.0393, 0.0679, 0.1631, 0.2562, 0.1483,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8154, 0.1000])
proposed candidate after normalizing: [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), 0, tensor(0.0679), tensor(0.1631), tensor(0.2562), 5, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
iteration:  17
input_X:  [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), 0, tensor(0.0679), tensor(0.1631), tensor(0.2562), 5, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  104 0.10000000149011612 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 35,676,160 || all params: 8,065,937,408 || trainable%: 0.4423
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), 0, tensor(0.0679), tensor(0.1631), tensor(0.2562)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1239)
number of datapoints needed (ratio * total):  619
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1674)
number of datapoints needed (ratio * total):  837
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1063)
number of datapoints needed (ratio * total):  531
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0759)
number of datapoints needed (ratio * total):  379
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  tensor(0.0679)
number of datapoints needed (ratio * total):  339
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1631)
number of datapoints needed (ratio * total):  815
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2562)
number of datapoints needed (ratio * total):  1281
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 619
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 837
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 531
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 379
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 339
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 815
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1281
})]
length of training data:  4801
training model...
trainable params: 35,676,160 || all params: 8,065,937,408 || trainable%: 0.4423
{'loss': 2.2746, 'grad_norm': 0.7193495035171509, 'learning_rate': 0.00029492385786802027, 'epoch': 0.03}
{'loss': 1.3526, 'grad_norm': 0.6801040768623352, 'learning_rate': 0.00028477157360406086, 'epoch': 0.07}
{'loss': 1.6525, 'grad_norm': 0.45440608263015747, 'learning_rate': 0.0002746192893401015, 'epoch': 0.1}
{'loss': 1.3146, 'grad_norm': 1.0254155397415161, 'learning_rate': 0.0002644670050761421, 'epoch': 0.13}
{'loss': 1.3403, 'grad_norm': 0.5220637917518616, 'learning_rate': 0.00025431472081218273, 'epoch': 0.17}
{'loss': 1.3058, 'grad_norm': 0.6865572929382324, 'learning_rate': 0.0002441624365482233, 'epoch': 0.2}
{'loss': 1.1924, 'grad_norm': 0.6287283897399902, 'learning_rate': 0.00023401015228426394, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5117, 'train_samples_per_second': 47.766, 'train_steps_per_second': 5.979, 'train_loss': 1.4650613875002474, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_17/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.56
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1460, 0.1556, 0.0772, 0.0201, 0.1508, 0.0872, 0.0730, 0.2900, 0.1579,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8165, 0.0801])
proposed candidate after normalizing: [tensor(0.1460), tensor(0.1556), tensor(0.0772), 0, tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), 5, 1, 1, 1, 1, 1, 105, 0.08009422570466995]
iteration:  18
input_X:  [tensor(0.1460), tensor(0.1556), tensor(0.0772), 0, tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), 5, 1, 1, 1, 1, 1, 105, 0.08009422570466995]
mixing data with method:  random
arranging lora config with parameters:  105 0.08009422570466995 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 36,019,200 || all params: 8,066,280,448 || trainable%: 0.4465
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1460), tensor(0.1556), tensor(0.0772), 0, tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1460)
number of datapoints needed (ratio * total):  730
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1556)
number of datapoints needed (ratio * total):  778
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0772)
number of datapoints needed (ratio * total):  386
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1508)
number of datapoints needed (ratio * total):  753
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0872)
number of datapoints needed (ratio * total):  436
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0730)
number of datapoints needed (ratio * total):  365
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2900)
number of datapoints needed (ratio * total):  1450
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 730
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 778
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 386
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 753
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 436
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 365
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1450
})]
length of training data:  4898
training model...
trainable params: 36,019,200 || all params: 8,066,280,448 || trainable%: 0.4465
{'loss': 2.5181, 'grad_norm': 0.48408329486846924, 'learning_rate': 0.0002950248756218905, 'epoch': 0.03}
{'loss': 1.7453, 'grad_norm': 1.234381914138794, 'learning_rate': 0.00028507462686567164, 'epoch': 0.07}
{'loss': 1.6272, 'grad_norm': 0.7392922639846802, 'learning_rate': 0.00027512437810945273, 'epoch': 0.1}
{'loss': 1.3585, 'grad_norm': 1.2012165784835815, 'learning_rate': 0.0002651741293532338, 'epoch': 0.13}
{'loss': 1.7383, 'grad_norm': 0.7405291199684143, 'learning_rate': 0.0002552238805970149, 'epoch': 0.16}
{'loss': 1.3904, 'grad_norm': 0.4294397234916687, 'learning_rate': 0.000245273631840796, 'epoch': 0.2}
{'loss': 1.5912, 'grad_norm': 0.9665154814720154, 'learning_rate': 0.0002353233830845771, 'epoch': 0.23}
{'loss': 1.2625, 'grad_norm': 0.879146933555603, 'learning_rate': 0.0002253731343283582, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.058, 'train_samples_per_second': 48.952, 'train_steps_per_second': 6.126, 'train_loss': 1.6502164965090544, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_18/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.6
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)]]
proposed candidate before processing: tensor([0.1483, 0.1783, 0.0753, 0.0357, 0.1003, 0.0848, 0.1101, 0.2670, 0.1559,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8135, 0.0224])
proposed candidate after normalizing: [tensor(0.1483), tensor(0.1783), tensor(0.0753), 0, tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), 5, 1, 1, 1, 1, 1, 104, 0.02239757403731346]
iteration:  19
input_X:  [tensor(0.1483), tensor(0.1783), tensor(0.0753), 0, tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), 5, 1, 1, 1, 1, 1, 104, 0.02239757403731346]
mixing data with method:  random
arranging lora config with parameters:  104 0.02239757403731346 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 35,676,160 || all params: 8,065,937,408 || trainable%: 0.4423
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1483), tensor(0.1783), tensor(0.0753), 0, tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1483)
number of datapoints needed (ratio * total):  741
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1783)
number of datapoints needed (ratio * total):  891
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0753)
number of datapoints needed (ratio * total):  376
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1003)
number of datapoints needed (ratio * total):  501
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0848)
number of datapoints needed (ratio * total):  424
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1101)
number of datapoints needed (ratio * total):  550
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2670)
number of datapoints needed (ratio * total):  1335
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 741
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 891
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 376
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 501
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 424
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 550
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1335
})]
length of training data:  4818
training model...
trainable params: 35,676,160 || all params: 8,065,937,408 || trainable%: 0.4423
{'loss': 2.1132, 'grad_norm': 1.1001571416854858, 'learning_rate': 0.0002949409780775716, 'epoch': 0.03}
{'loss': 1.7076, 'grad_norm': 0.5228049755096436, 'learning_rate': 0.00028482293423271496, 'epoch': 0.07}
{'loss': 1.5092, 'grad_norm': 1.2554799318313599, 'learning_rate': 0.00027470489038785835, 'epoch': 0.1}
{'loss': 1.2979, 'grad_norm': 0.5749078392982483, 'learning_rate': 0.00026458684654300164, 'epoch': 0.13}
{'loss': 1.2631, 'grad_norm': 0.5949151515960693, 'learning_rate': 0.000254468802698145, 'epoch': 0.17}
{'loss': 1.4489, 'grad_norm': 0.834442138671875, 'learning_rate': 0.00024435075885328837, 'epoch': 0.2}
{'loss': 1.5218, 'grad_norm': 1.2503902912139893, 'learning_rate': 0.00023423271500843168, 'epoch': 0.23}
{'loss': 1.6078, 'grad_norm': 0.7496626973152161, 'learning_rate': 0.00022411467116357502, 'epoch': 0.27}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3165, 'train_samples_per_second': 48.028, 'train_steps_per_second': 6.011, 'train_loss': 1.5605586063787804, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_19/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.54
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)]]
proposed candidate before processing: tensor([0.1111, 0.1107, 0.0937, 0.0530, 0.1394, 0.0814, 0.1363, 0.2744, 0.1649,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8204, 0.1000])
proposed candidate after normalizing: [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), 5, 1, 1, 1, 1, 1, 105, 0.10000000149011612]
iteration:  20
input_X:  [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), 5, 1, 1, 1, 1, 1, 105, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  105 0.10000000149011612 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 36,019,200 || all params: 8,066,280,448 || trainable%: 0.4465
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1111)
number of datapoints needed (ratio * total):  555
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1107)
number of datapoints needed (ratio * total):  553
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0937)
number of datapoints needed (ratio * total):  468
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0530)
number of datapoints needed (ratio * total):  264
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1394)
number of datapoints needed (ratio * total):  697
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0814)
number of datapoints needed (ratio * total):  406
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1363)
number of datapoints needed (ratio * total):  681
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2744)
number of datapoints needed (ratio * total):  1372
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 555
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 553
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 468
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 264
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 697
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 406
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 681
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1372
})]
length of training data:  4996
training model...
trainable params: 36,019,200 || all params: 8,066,280,448 || trainable%: 0.4465
{'loss': 2.25, 'grad_norm': 1.0326606035232544, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5605, 'grad_norm': 0.6746055483818054, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.4143, 'grad_norm': 0.7834897041320801, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3559, 'grad_norm': 0.6184573173522949, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.5663, 'grad_norm': 1.1171590089797974, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.518, 'grad_norm': 0.87995445728302, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.6001, 'grad_norm': 0.7279236912727356, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3523, 'train_samples_per_second': 49.785, 'train_steps_per_second': 6.228, 'train_loss': 1.6126394526163736, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_20/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.58
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1664, 0.1386, 0.1198, 0.0840, 0.0973, 0.0472, 0.0966, 0.2502, 0.1286,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8257, 0.1000])
proposed candidate after normalizing: [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), 0, tensor(0.0966), tensor(0.2502), 4, 1, 1, 1, 1, 1, 106, 0.10000000149011612]
iteration:  21
input_X:  [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), 0, tensor(0.0966), tensor(0.2502), 4, 1, 1, 1, 1, 1, 106, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  106 0.10000000149011612 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 36,293,632 || all params: 8,066,554,880 || trainable%: 0.4499
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), 0, tensor(0.0966), tensor(0.2502)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1664)
number of datapoints needed (ratio * total):  832
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1386)
number of datapoints needed (ratio * total):  693
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1198)
number of datapoints needed (ratio * total):  598
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0840)
number of datapoints needed (ratio * total):  419
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0973)
number of datapoints needed (ratio * total):  486
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0966)
number of datapoints needed (ratio * total):  482
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2502)
number of datapoints needed (ratio * total):  1250
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 832
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 693
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 598
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 419
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 486
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 482
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1250
})]
length of training data:  4760
training model...
trainable params: 36,293,632 || all params: 8,066,554,880 || trainable%: 0.4499
{'loss': 1.6539, 'grad_norm': 1.0945439338684082, 'learning_rate': 0.00029487179487179484, 'epoch': 0.03}
{'loss': 1.4296, 'grad_norm': 0.5451851487159729, 'learning_rate': 0.00028461538461538457, 'epoch': 0.07}
{'loss': 1.5814, 'grad_norm': 1.1158905029296875, 'learning_rate': 0.0002743589743589743, 'epoch': 0.1}
{'loss': 1.2183, 'grad_norm': 0.9140756726264954, 'learning_rate': 0.0002641025641025641, 'epoch': 0.13}
{'loss': 1.3863, 'grad_norm': 0.6675686836242676, 'learning_rate': 0.0002538461538461538, 'epoch': 0.17}
{'loss': 1.3308, 'grad_norm': 0.4710436463356018, 'learning_rate': 0.00024358974358974357, 'epoch': 0.2}
{'loss': 1.2951, 'grad_norm': 0.6764564514160156, 'learning_rate': 0.0002333333333333333, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.455, 'train_samples_per_second': 47.384, 'train_steps_per_second': 5.923, 'train_loss': 1.3888988064395056, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_21/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1814, 0.1379, 0.1238, 0.0916, 0.0934, 0.0395, 0.0796, 0.2527, 0.1175,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8294, 0.1000])
proposed candidate after normalizing: [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), 0, tensor(0.0796), tensor(0.2527), 4, 1, 1, 1, 1, 1, 106, 0.10000000149011612]
iteration:  22
input_X:  [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), 0, tensor(0.0796), tensor(0.2527), 4, 1, 1, 1, 1, 1, 106, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  106 0.10000000149011612 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 29,089,792 || all params: 8,059,351,040 || trainable%: 0.3609
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), 0, tensor(0.0796), tensor(0.2527)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1814)
number of datapoints needed (ratio * total):  907
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1379)
number of datapoints needed (ratio * total):  689
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1238)
number of datapoints needed (ratio * total):  619
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0916)
number of datapoints needed (ratio * total):  458
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0934)
number of datapoints needed (ratio * total):  466
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0796)
number of datapoints needed (ratio * total):  398
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2527)
number of datapoints needed (ratio * total):  1263
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 907
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 689
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 619
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 458
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 466
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 398
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1263
})]
length of training data:  4800
training model...
trainable params: 29,089,792 || all params: 8,059,351,040 || trainable%: 0.3609
{'loss': 2.4586, 'grad_norm': 1.7252622842788696, 'learning_rate': 0.0002949152542372881, 'epoch': 0.03}
{'loss': 1.6823, 'grad_norm': 0.5422084331512451, 'learning_rate': 0.0002847457627118644, 'epoch': 0.07}
{'loss': 1.3174, 'grad_norm': 0.8548988103866577, 'learning_rate': 0.00027457627118644066, 'epoch': 0.1}
{'loss': 1.3677, 'grad_norm': 0.6658970713615417, 'learning_rate': 0.0002644067796610169, 'epoch': 0.13}
{'loss': 1.3621, 'grad_norm': 0.6374111175537109, 'learning_rate': 0.00025423728813559317, 'epoch': 0.17}
{'loss': 1.6512, 'grad_norm': 0.5050966143608093, 'learning_rate': 0.00024406779661016948, 'epoch': 0.2}
{'loss': 1.6048, 'grad_norm': 0.6263532042503357, 'learning_rate': 0.00023389830508474576, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5651, 'train_samples_per_second': 47.73, 'train_steps_per_second': 5.966, 'train_loss': 1.634306979505983, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_22/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1884, 0.1311, 0.0665, 0.0672, 0.0518, 0.1329, 0.1164, 0.2457, 0.1429,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7854, 0.1000])
proposed candidate after normalizing: [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), 5, 1, 1, 1, 1, 1, 101, 0.10000000149011612]
iteration:  23
input_X:  [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), 5, 1, 1, 1, 1, 1, 101, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  101 0.10000000149011612 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 34,647,040 || all params: 8,064,908,288 || trainable%: 0.4296
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1884)
number of datapoints needed (ratio * total):  942
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1311)
number of datapoints needed (ratio * total):  655
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0665)
number of datapoints needed (ratio * total):  332
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0672)
number of datapoints needed (ratio * total):  335
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0518)
number of datapoints needed (ratio * total):  259
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1329)
number of datapoints needed (ratio * total):  664
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1164)
number of datapoints needed (ratio * total):  582
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2457)
number of datapoints needed (ratio * total):  1228
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 942
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 655
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 332
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 335
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 259
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 664
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 582
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1228
})]
length of training data:  4997
training model...
trainable params: 34,647,040 || all params: 8,064,908,288 || trainable%: 0.4296
{'loss': 2.4511, 'grad_norm': 0.7129031419754028, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5602, 'grad_norm': 1.712967872619629, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3065, 'grad_norm': 0.8285428881645203, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3694, 'grad_norm': 0.7587922215461731, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3943, 'grad_norm': 0.738334596157074, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.4752, 'grad_norm': 0.6043429374694824, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.2288, 'grad_norm': 1.0275402069091797, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.7166, 'train_samples_per_second': 49.614, 'train_steps_per_second': 6.206, 'train_loss': 1.5156988416399275, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_23/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.57
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1573, 0.1516, 0.1515, 0.0945, 0.1109, 0.0125, 0.0671, 0.2546, 0.1100,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8457, 0.1000])
proposed candidate after normalizing: [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), 0, tensor(0.0671), tensor(0.2546), 4, 1, 1, 1, 1, 1, 108, 0.10000000149011612]
iteration:  24
input_X:  [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), 0, tensor(0.0671), tensor(0.2546), 4, 1, 1, 1, 1, 1, 108, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  108 0.10000000149011612 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 36,568,064 || all params: 8,066,829,312 || trainable%: 0.4533
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), 0, tensor(0.0671), tensor(0.2546)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1573)
number of datapoints needed (ratio * total):  786
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1516)
number of datapoints needed (ratio * total):  758
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1515)
number of datapoints needed (ratio * total):  757
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0945)
number of datapoints needed (ratio * total):  472
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1109)
number of datapoints needed (ratio * total):  554
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0671)
number of datapoints needed (ratio * total):  335
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2546)
number of datapoints needed (ratio * total):  1273
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 786
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 758
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 757
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 472
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 554
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 335
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1273
})]
length of training data:  4935
training model...
trainable params: 36,568,064 || all params: 8,066,829,312 || trainable%: 0.4533
{'loss': 1.3655, 'grad_norm': 0.9857729077339172, 'learning_rate': 0.0002950576606260296, 'epoch': 0.03}
{'loss': 1.4497, 'grad_norm': 0.5499286651611328, 'learning_rate': 0.0002851729818780889, 'epoch': 0.06}
{'loss': 1.6625, 'grad_norm': 0.6945973634719849, 'learning_rate': 0.00027528830313014826, 'epoch': 0.1}
{'loss': 1.0033, 'grad_norm': 1.057900309562683, 'learning_rate': 0.00026540362438220755, 'epoch': 0.13}
{'loss': 1.3941, 'grad_norm': 1.5949054956436157, 'learning_rate': 0.00025551894563426684, 'epoch': 0.16}
{'loss': 1.0991, 'grad_norm': 0.45788896083831787, 'learning_rate': 0.0002456342668863262, 'epoch': 0.19}
{'loss': 1.5755, 'grad_norm': 1.1881011724472046, 'learning_rate': 0.00023574958813838548, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0253, 'train_samples_per_second': 49.338, 'train_steps_per_second': 6.168, 'train_loss': 1.3622760607136621, 'epoch': 0.23}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_24/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.64
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1552, 0.1515, 0.1560, 0.0940, 0.1112, 0.0147, 0.0644, 0.2531, 0.1111,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8429, 0.1000])
proposed candidate after normalizing: [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), 0, tensor(0.0644), tensor(0.2531), 4, 1, 1, 1, 1, 1, 108, 0.10000000149011612]
iteration:  25
input_X:  [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), 0, tensor(0.0644), tensor(0.2531), 4, 1, 1, 1, 1, 1, 108, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  108 0.10000000149011612 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 29,638,656 || all params: 8,059,899,904 || trainable%: 0.3677
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), 0, tensor(0.0644), tensor(0.2531)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1552)
number of datapoints needed (ratio * total):  775
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1515)
number of datapoints needed (ratio * total):  757
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1560)
number of datapoints needed (ratio * total):  780
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0940)
number of datapoints needed (ratio * total):  469
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1112)
number of datapoints needed (ratio * total):  556
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0644)
number of datapoints needed (ratio * total):  321
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2531)
number of datapoints needed (ratio * total):  1265
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 775
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 757
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 780
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 469
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 556
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 321
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1265
})]
length of training data:  4923
training model...
trainable params: 29,638,656 || all params: 8,059,899,904 || trainable%: 0.3677
{'loss': 1.8014, 'grad_norm': 2.091277599334717, 'learning_rate': 0.000295049504950495, 'epoch': 0.03}
{'loss': 1.4872, 'grad_norm': 0.8954320549964905, 'learning_rate': 0.0002851485148514851, 'epoch': 0.06}
{'loss': 1.3316, 'grad_norm': 0.7788939476013184, 'learning_rate': 0.0002752475247524752, 'epoch': 0.1}
{'loss': 1.6929, 'grad_norm': 0.5066192150115967, 'learning_rate': 0.00026534653465346534, 'epoch': 0.13}
{'loss': 1.7073, 'grad_norm': 0.47032201290130615, 'learning_rate': 0.00025544554455445543, 'epoch': 0.16}
{'loss': 1.4152, 'grad_norm': 0.9116621613502502, 'learning_rate': 0.0002455445544554455, 'epoch': 0.19}
{'loss': 1.4034, 'grad_norm': 1.16627836227417, 'learning_rate': 0.00023564356435643561, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6554, 'train_samples_per_second': 48.909, 'train_steps_per_second': 6.12, 'train_loss': 1.5339926646815405, 'epoch': 0.23}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_25/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.6
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1448, 0.1390, 0.1303, 0.0771, 0.0968, 0.0702, 0.1080, 0.2338, 0.1475,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8023, 0.1000])
proposed candidate after normalizing: [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), 5, 1, 1, 1, 1, 1, 103, 0.10000000149011612]
iteration:  26
input_X:  [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), 5, 1, 1, 1, 1, 1, 103, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  103 0.10000000149011612 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 35,333,120 || all params: 8,065,594,368 || trainable%: 0.4381
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1448)
number of datapoints needed (ratio * total):  723
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1390)
number of datapoints needed (ratio * total):  695
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1303)
number of datapoints needed (ratio * total):  651
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0771)
number of datapoints needed (ratio * total):  385
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0968)
number of datapoints needed (ratio * total):  484
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0702)
number of datapoints needed (ratio * total):  350
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1080)
number of datapoints needed (ratio * total):  540
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2338)
number of datapoints needed (ratio * total):  1169
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 723
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 695
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 651
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 385
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 484
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 350
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 540
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1169
})]
length of training data:  4997
training model...
trainable params: 35,333,120 || all params: 8,065,594,368 || trainable%: 0.4381
{'loss': 2.0948, 'grad_norm': 0.4649353325366974, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5019, 'grad_norm': 0.8860936164855957, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.7092, 'grad_norm': 0.7188998460769653, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3653, 'grad_norm': 0.528999924659729, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3756, 'grad_norm': 0.381521999835968, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.313, 'grad_norm': 0.6523118019104004, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.1967, 'grad_norm': 3.621570348739624, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2225, 'train_samples_per_second': 49.859, 'train_steps_per_second': 6.236, 'train_loss': 1.5029547976122961, 'epoch': 0.23}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_26/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.55
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)]]
proposed candidate before processing: tensor([1.7740e-01, 1.3567e-01, 9.2532e-02, 8.6520e-02, 1.0406e-01, 9.0700e-19,
        1.1014e-01, 2.9368e-01, 7.7061e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.8091e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), 0, tensor(0.1101), tensor(0.2937), 2, 1, 1, 1, 1, 1, 113, 0.10000000149011612]
iteration:  27
input_X:  [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), 0, tensor(0.1101), tensor(0.2937), 2, 1, 1, 1, 1, 1, 113, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  113 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 36,705,280 || all params: 8,066,966,528 || trainable%: 0.4550
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), 0, tensor(0.1101), tensor(0.2937)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1774)
number of datapoints needed (ratio * total):  887
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1357)
number of datapoints needed (ratio * total):  678
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0925)
number of datapoints needed (ratio * total):  462
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0865)
number of datapoints needed (ratio * total):  432
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1041)
number of datapoints needed (ratio * total):  520
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1101)
number of datapoints needed (ratio * total):  550
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2937)
number of datapoints needed (ratio * total):  1468
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 887
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 678
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 462
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 432
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 520
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 550
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1468
})]
length of training data:  4997
training model...
trainable params: 36,705,280 || all params: 8,066,966,528 || trainable%: 0.4550
{'loss': 1.816, 'grad_norm': 0.5962839722633362, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.3504, 'grad_norm': 0.7120522856712341, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.6063, 'grad_norm': 0.4125659167766571, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.2856, 'grad_norm': 0.9132756590843201, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3014, 'grad_norm': 0.5672963857650757, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.3536, 'grad_norm': 0.7136213183403015, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.2665, 'grad_norm': 0.39913609623908997, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0462, 'train_samples_per_second': 49.947, 'train_steps_per_second': 6.247, 'train_loss': 1.4106130468434301, 'epoch': 0.23}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_27/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.64
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1794, 0.1346, 0.0901, 0.0846, 0.1019, 0.0000, 0.1100, 0.2994, 0.0634,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8879, 0.1000])
proposed candidate after normalizing: [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), 0, tensor(0.1100), tensor(0.2994), 2, 1, 1, 1, 1, 1, 114, 0.10000000149011612]
iteration:  28
input_X:  [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), 0, tensor(0.1100), tensor(0.2994), 2, 1, 1, 1, 1, 1, 114, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  114 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 15,642,624 || all params: 8,045,903,872 || trainable%: 0.1944
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), 0, tensor(0.1100), tensor(0.2994)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1794)
number of datapoints needed (ratio * total):  897
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1346)
number of datapoints needed (ratio * total):  673
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0901)
number of datapoints needed (ratio * total):  450
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0846)
number of datapoints needed (ratio * total):  423
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1019)
number of datapoints needed (ratio * total):  509
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1100)
number of datapoints needed (ratio * total):  549
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2994)
number of datapoints needed (ratio * total):  1497
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 897
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 673
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 450
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 423
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 509
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 549
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1497
})]
length of training data:  4998
training model...
trainable params: 15,642,624 || all params: 8,045,903,872 || trainable%: 0.1944
{'loss': 2.3886, 'grad_norm': 1.7505154609680176, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 2.0731, 'grad_norm': 0.5133302807807922, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.5818, 'grad_norm': 1.2367362976074219, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.59, 'grad_norm': 0.6767144799232483, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.7025, 'grad_norm': 0.8329648971557617, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.7757, 'grad_norm': 0.7264208197593689, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.6517, 'grad_norm': 0.38720405101776123, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3437, 'train_samples_per_second': 49.809, 'train_steps_per_second': 6.229, 'train_loss': 1.7954070245897449, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_28/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.6
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)]]
proposed candidate before processing: tensor([1.7988e-01, 1.3221e-01, 9.3500e-02, 1.0191e-01, 1.1085e-01, 1.3688e-18,
        9.6109e-02, 2.8555e-01, 1.3212e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.5005e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), 0, tensor(0.0961), tensor(0.2855), 4, 1, 1, 1, 1, 1, 109, 0.10000000149011612]
iteration:  29
input_X:  [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), 0, tensor(0.0961), tensor(0.2855), 4, 1, 1, 1, 1, 1, 109, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  109 0.10000000149011612 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 29,913,088 || all params: 8,060,174,336 || trainable%: 0.3711
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), 0, tensor(0.0961), tensor(0.2855)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1799)
number of datapoints needed (ratio * total):  899
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1322)
number of datapoints needed (ratio * total):  661
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0935)
number of datapoints needed (ratio * total):  467
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1019)
number of datapoints needed (ratio * total):  509
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1108)
number of datapoints needed (ratio * total):  554
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0961)
number of datapoints needed (ratio * total):  480
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2855)
number of datapoints needed (ratio * total):  1427
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 899
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 661
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 467
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 509
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 554
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 480
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1427
})]
length of training data:  4997
training model...
trainable params: 29,913,088 || all params: 8,060,174,336 || trainable%: 0.3711
{'loss': 2.4357, 'grad_norm': 0.3672935962677002, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5477, 'grad_norm': 1.4502960443496704, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.8285, 'grad_norm': 1.0184085369110107, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.7054, 'grad_norm': 0.3482300639152527, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.5613, 'grad_norm': 0.9962288737297058, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.5316, 'grad_norm': 0.6307389736175537, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.6113, 'grad_norm': 0.5145811438560486, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6082, 'train_samples_per_second': 49.668, 'train_steps_per_second': 6.212, 'train_loss': 1.7283646979299532, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_29/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1911, 0.1235, 0.0808, 0.1159, 0.1106, 0.0000, 0.0816, 0.2964, 0.1470,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8453, 0.1000])
proposed candidate after normalizing: [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), 0, tensor(0.0816), tensor(0.2964), 5, 1, 1, 1, 1, 1, 108, 0.10000000149011612]
iteration:  30
input_X:  [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), 0, tensor(0.0816), tensor(0.2964), 5, 1, 1, 1, 1, 1, 108, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  108 0.10000000149011612 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 37,048,320 || all params: 8,067,309,568 || trainable%: 0.4592
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), 0, tensor(0.0816), tensor(0.2964)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1911)
number of datapoints needed (ratio * total):  955
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1235)
number of datapoints needed (ratio * total):  617
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0808)
number of datapoints needed (ratio * total):  404
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1159)
number of datapoints needed (ratio * total):  579
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1106)
number of datapoints needed (ratio * total):  553
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0816)
number of datapoints needed (ratio * total):  408
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2964)
number of datapoints needed (ratio * total):  1481
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 955
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 617
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 404
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 579
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 553
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 408
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1481
})]
length of training data:  4997
training model...
trainable params: 37,048,320 || all params: 8,067,309,568 || trainable%: 0.4592
{'loss': 2.2519, 'grad_norm': 0.6019854545593262, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.7396, 'grad_norm': 3.4401912689208984, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.4321, 'grad_norm': 0.4817332923412323, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.6371, 'grad_norm': 1.128385066986084, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.657, 'grad_norm': 0.4241584241390228, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.6103, 'grad_norm': 0.7241085767745972, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.4496, 'grad_norm': 1.383741855621338, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3272, 'train_samples_per_second': 49.807, 'train_steps_per_second': 6.23, 'train_loss': 1.647971353015384, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_30/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)]]
proposed candidate before processing: tensor([1.8824e-01, 1.3025e-01, 8.7157e-02, 1.0386e-01, 1.1111e-01, 1.7312e-17,
        9.7109e-02, 2.8228e-01, 1.4081e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.5326e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), 0, tensor(0.0971), tensor(0.2823), 5, 1, 1, 1, 1, 1, 109, 0.10000000149011612]
iteration:  31
input_X:  [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), 0, tensor(0.0971), tensor(0.2823), 5, 1, 1, 1, 1, 1, 109, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  109 0.10000000149011612 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 37,391,360 || all params: 8,067,652,608 || trainable%: 0.4635
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), 0, tensor(0.0971), tensor(0.2823)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1882)
number of datapoints needed (ratio * total):  941
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1303)
number of datapoints needed (ratio * total):  651
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0872)
number of datapoints needed (ratio * total):  435
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1039)
number of datapoints needed (ratio * total):  519
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1111)
number of datapoints needed (ratio * total):  555
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0971)
number of datapoints needed (ratio * total):  485
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2823)
number of datapoints needed (ratio * total):  1411
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 941
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 651
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 435
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 519
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 555
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 485
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1411
})]
length of training data:  4997
training model...
trainable params: 37,391,360 || all params: 8,067,652,608 || trainable%: 0.4635
{'loss': 2.2278, 'grad_norm': 0.9521927833557129, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.4241, 'grad_norm': 0.6866723895072937, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.5752, 'grad_norm': 0.9104543328285217, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.6007, 'grad_norm': 0.7065001130104065, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.6631, 'grad_norm': 1.0420200824737549, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.3541, 'grad_norm': 0.6156116127967834, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.4064, 'grad_norm': 1.1421934366226196, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4304, 'train_samples_per_second': 49.756, 'train_steps_per_second': 6.223, 'train_loss': 1.5890652675502348, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_31/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.55
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1349, 0.1529, 0.1240, 0.0708, 0.1087, 0.0151, 0.1072, 0.2864, 0.0561,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8582, 0.1000])
proposed candidate after normalizing: [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), 0, tensor(0.1072), tensor(0.2864), 2, 1, 1, 1, 1, 1, 110, 0.10000000149011612]
iteration:  32
input_X:  [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), 0, tensor(0.1072), tensor(0.2864), 2, 1, 1, 1, 1, 1, 110, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  110 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 37,528,576 || all params: 8,067,789,824 || trainable%: 0.4652
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), 0, tensor(0.1072), tensor(0.2864)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1349)
number of datapoints needed (ratio * total):  674
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1529)
number of datapoints needed (ratio * total):  764
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1240)
number of datapoints needed (ratio * total):  620
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0708)
number of datapoints needed (ratio * total):  354
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1087)
number of datapoints needed (ratio * total):  543
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1072)
number of datapoints needed (ratio * total):  535
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2864)
number of datapoints needed (ratio * total):  1432
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 674
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 764
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 620
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 354
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 543
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 535
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1432
})]
length of training data:  4922
training model...
trainable params: 37,528,576 || all params: 8,067,789,824 || trainable%: 0.4652
{'loss': 1.6528, 'grad_norm': 0.7207888960838318, 'learning_rate': 0.000295049504950495, 'epoch': 0.03}
{'loss': 1.446, 'grad_norm': 0.7148330807685852, 'learning_rate': 0.0002851485148514851, 'epoch': 0.06}
{'loss': 1.2508, 'grad_norm': 0.8367384672164917, 'learning_rate': 0.0002752475247524752, 'epoch': 0.1}
{'loss': 1.211, 'grad_norm': 0.47281891107559204, 'learning_rate': 0.00026534653465346534, 'epoch': 0.13}
{'loss': 1.3439, 'grad_norm': 0.47648265957832336, 'learning_rate': 0.00025544554455445543, 'epoch': 0.16}
{'loss': 1.2229, 'grad_norm': 0.6695959568023682, 'learning_rate': 0.0002455445544554455, 'epoch': 0.19}
{'loss': 1.3149, 'grad_norm': 0.7995293736457825, 'learning_rate': 0.00023564356435643561, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.019, 'train_samples_per_second': 49.211, 'train_steps_per_second': 6.159, 'train_loss': 1.35304881764107, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_32/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1454, 0.1493, 0.1212, 0.0653, 0.1083, 0.0264, 0.1102, 0.2739, 0.0685,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8735, 0.1000])
proposed candidate after normalizing: [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), 0, tensor(0.1102), tensor(0.2739), 2, 1, 1, 1, 1, 1, 112, 0.10000000149011612]
iteration:  33
input_X:  [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), 0, tensor(0.1102), tensor(0.2739), 2, 1, 1, 1, 1, 1, 112, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  112 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 15,368,192 || all params: 8,045,629,440 || trainable%: 0.1910
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), 0, tensor(0.1102), tensor(0.2739)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1454)
number of datapoints needed (ratio * total):  727
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1493)
number of datapoints needed (ratio * total):  746
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1212)
number of datapoints needed (ratio * total):  606
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0653)
number of datapoints needed (ratio * total):  326
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1083)
number of datapoints needed (ratio * total):  541
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1102)
number of datapoints needed (ratio * total):  550
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2739)
number of datapoints needed (ratio * total):  1369
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 727
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 746
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 606
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 326
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 541
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 550
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1369
})]
length of training data:  4865
training model...
trainable params: 15,368,192 || all params: 8,045,629,440 || trainable%: 0.1910
{'loss': 2.2506, 'grad_norm': 1.6126915216445923, 'learning_rate': 0.0002949916527545909, 'epoch': 0.03}
{'loss': 1.811, 'grad_norm': 0.4058004915714264, 'learning_rate': 0.0002849749582637729, 'epoch': 0.07}
{'loss': 1.8041, 'grad_norm': 0.9293832182884216, 'learning_rate': 0.00027495826377295493, 'epoch': 0.1}
{'loss': 1.231, 'grad_norm': 1.6674144268035889, 'learning_rate': 0.0002649415692821369, 'epoch': 0.13}
{'loss': 1.5875, 'grad_norm': 0.6428447961807251, 'learning_rate': 0.0002549248747913189, 'epoch': 0.16}
{'loss': 1.4258, 'grad_norm': 1.134113073348999, 'learning_rate': 0.0002449081803005008, 'epoch': 0.2}
{'loss': 1.4436, 'grad_norm': 0.7675066590309143, 'learning_rate': 0.00023489148580968278, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1541, 'train_samples_per_second': 48.575, 'train_steps_per_second': 6.081, 'train_loss': 1.6295765422334607, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_33/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1441, 0.1522, 0.1255, 0.0546, 0.1092, 0.0327, 0.1100, 0.2716, 0.0434,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8937, 0.1000])
proposed candidate after normalizing: [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), 0, tensor(0.1100), tensor(0.2716), 1, 1, 1, 1, 1, 1, 114, 0.10000000149011612]
iteration:  34
input_X:  [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), 0, tensor(0.1100), tensor(0.2716), 1, 1, 1, 1, 1, 1, 114, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  114 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 15,505,408 || all params: 8,045,766,656 || trainable%: 0.1927
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), 0, tensor(0.1100), tensor(0.2716)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1441)
number of datapoints needed (ratio * total):  720
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1522)
number of datapoints needed (ratio * total):  761
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1255)
number of datapoints needed (ratio * total):  627
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0546)
number of datapoints needed (ratio * total):  272
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1092)
number of datapoints needed (ratio * total):  546
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1100)
number of datapoints needed (ratio * total):  549
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2716)
number of datapoints needed (ratio * total):  1358
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 720
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 761
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 627
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 272
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 546
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 549
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1358
})]
length of training data:  4833
training model...
trainable params: 15,505,408 || all params: 8,045,766,656 || trainable%: 0.1927
{'loss': 1.3988, 'grad_norm': 1.1410633325576782, 'learning_rate': 0.00029495798319327727, 'epoch': 0.03}
{'loss': 1.7578, 'grad_norm': 1.04351007938385, 'learning_rate': 0.0002848739495798319, 'epoch': 0.07}
{'loss': 1.5754, 'grad_norm': 0.4335596561431885, 'learning_rate': 0.0002747899159663865, 'epoch': 0.1}
{'loss': 1.4623, 'grad_norm': 0.5112147331237793, 'learning_rate': 0.00026470588235294115, 'epoch': 0.13}
{'loss': 1.4008, 'grad_norm': 0.636069118976593, 'learning_rate': 0.00025462184873949575, 'epoch': 0.17}
{'loss': 1.4798, 'grad_norm': 0.549743115901947, 'learning_rate': 0.0002445378151260504, 'epoch': 0.2}
{'loss': 1.3109, 'grad_norm': 1.0869239568710327, 'learning_rate': 0.000234453781512605, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6397, 'train_samples_per_second': 48.023, 'train_steps_per_second': 6.012, 'train_loss': 1.4717304056340998, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_34/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1452, 0.1533, 0.1282, 0.0473, 0.1093, 0.0387, 0.1086, 0.2694, 0.0234,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9088, 0.1000])
proposed candidate after normalizing: [tensor(0.1452), tensor(0.1533), tensor(0.1282), 0, tensor(0.1093), 0, tensor(0.1086), tensor(0.2694), 1, 1, 1, 1, 1, 1, 116, 0.10000000149011612]
iteration:  35
input_X:  [tensor(0.1452), tensor(0.1533), tensor(0.1282), 0, tensor(0.1093), 0, tensor(0.1086), tensor(0.2694), 1, 1, 1, 1, 1, 1, 116, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  116 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,958,528 || all params: 8,038,219,776 || trainable%: 0.0990
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1452), tensor(0.1533), tensor(0.1282), 0, tensor(0.1093), 0, tensor(0.1086), tensor(0.2694)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1452)
number of datapoints needed (ratio * total):  726
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1533)
number of datapoints needed (ratio * total):  766
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1282)
number of datapoints needed (ratio * total):  641
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1093)
number of datapoints needed (ratio * total):  546
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1086)
number of datapoints needed (ratio * total):  542
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2694)
number of datapoints needed (ratio * total):  1347
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 726
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 766
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 641
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 546
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 542
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1347
})]
length of training data:  4568
training model...
trainable params: 7,958,528 || all params: 8,038,219,776 || trainable%: 0.0990
{'loss': 2.4506, 'grad_norm': 3.1767048835754395, 'learning_rate': 0.00029465240641711227, 'epoch': 0.04}
{'loss': 2.1096, 'grad_norm': 0.7770178914070129, 'learning_rate': 0.00028395721925133686, 'epoch': 0.07}
{'loss': 2.0016, 'grad_norm': 0.6844533085823059, 'learning_rate': 0.0002732620320855615, 'epoch': 0.11}
{'loss': 1.4852, 'grad_norm': 0.7907050251960754, 'learning_rate': 0.0002625668449197861, 'epoch': 0.14}
{'loss': 1.6954, 'grad_norm': 0.6486240029335022, 'learning_rate': 0.0002518716577540107, 'epoch': 0.18}
{'loss': 1.4051, 'grad_norm': 1.0670576095581055, 'learning_rate': 0.00024117647058823527, 'epoch': 0.21}
{'loss': 1.9088, 'grad_norm': 1.061570644378662, 'learning_rate': 0.0002304812834224599, 'epoch': 0.25}
{'loss': 1.2323, 'grad_norm': 0.7395153045654297, 'learning_rate': 0.00021978609625668448, 'epoch': 0.28}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2266, 'train_samples_per_second': 45.577, 'train_steps_per_second': 5.697, 'train_loss': 1.7932674856071014, 'epoch': 0.29}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_35/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1710, 0.1430, 0.1161, 0.0919, 0.0427, 0.0740, 0.0856, 0.2756, 0.0729,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8720, 0.0760])
proposed candidate after normalizing: [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), 0, tensor(0.0740), tensor(0.0856), tensor(0.2756), 2, 1, 1, 1, 1, 1, 112, 0.07601064443588257]
iteration:  36
input_X:  [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), 0, tensor(0.0740), tensor(0.0856), tensor(0.2756), 2, 1, 1, 1, 1, 1, 112, 0.07601064443588257]
mixing data with method:  random
arranging lora config with parameters:  112 0.07601064443588257 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 15,368,192 || all params: 8,045,629,440 || trainable%: 0.1910
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), 0, tensor(0.0740), tensor(0.0856), tensor(0.2756)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1710)
number of datapoints needed (ratio * total):  854
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1430)
number of datapoints needed (ratio * total):  715
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1161)
number of datapoints needed (ratio * total):  580
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0919)
number of datapoints needed (ratio * total):  459
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  tensor(0.0740)
number of datapoints needed (ratio * total):  369
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0856)
number of datapoints needed (ratio * total):  427
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2756)
number of datapoints needed (ratio * total):  1378
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 854
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 715
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 580
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 459
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 369
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 427
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1378
})]
length of training data:  4782
training model...
trainable params: 15,368,192 || all params: 8,045,629,440 || trainable%: 0.1910
{'loss': 2.2269, 'grad_norm': 6.110394477844238, 'learning_rate': 0.0002948979591836734, 'epoch': 0.03}
{'loss': 1.757, 'grad_norm': 0.9989601969718933, 'learning_rate': 0.0002846938775510204, 'epoch': 0.07}
{'loss': 1.9863, 'grad_norm': 1.2762342691421509, 'learning_rate': 0.00027448979591836734, 'epoch': 0.1}
{'loss': 1.4828, 'grad_norm': 0.7609325647354126, 'learning_rate': 0.00026428571428571424, 'epoch': 0.13}
{'loss': 1.6107, 'grad_norm': 0.8811291456222534, 'learning_rate': 0.0002540816326530612, 'epoch': 0.17}
{'loss': 1.655, 'grad_norm': 0.7903017401695251, 'learning_rate': 0.00024387755102040816, 'epoch': 0.2}
{'loss': 1.3623, 'grad_norm': 1.474612832069397, 'learning_rate': 0.00023367346938775506, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2625, 'train_samples_per_second': 47.695, 'train_steps_per_second': 5.964, 'train_loss': 1.7085977735973539, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_36/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)]]
proposed candidate before processing: tensor([0.1634, 0.1384, 0.1138, 0.0812, 0.0611, 0.0701, 0.1033, 0.2686, 0.0831,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8635, 0.0758])
proposed candidate after normalizing: [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), 3, 1, 1, 1, 1, 1, 111, 0.0757765844464302]
iteration:  37
input_X:  [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), 3, 1, 1, 1, 1, 1, 111, 0.0757765844464302]
mixing data with method:  random
arranging lora config with parameters:  111 0.0757765844464302 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 22,846,464 || all params: 8,053,107,712 || trainable%: 0.2837
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1634)
number of datapoints needed (ratio * total):  817
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1384)
number of datapoints needed (ratio * total):  692
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1138)
number of datapoints needed (ratio * total):  568
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0812)
number of datapoints needed (ratio * total):  406
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0611)
number of datapoints needed (ratio * total):  305
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0701)
number of datapoints needed (ratio * total):  350
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1033)
number of datapoints needed (ratio * total):  516
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2686)
number of datapoints needed (ratio * total):  1343
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 817
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 692
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 568
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 406
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 305
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 350
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 516
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1343
})]
length of training data:  4997
training model...
trainable params: 22,846,464 || all params: 8,053,107,712 || trainable%: 0.2837
{'loss': 2.2725, 'grad_norm': 0.5865077376365662, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.7023, 'grad_norm': 0.6826184391975403, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.7926, 'grad_norm': 0.37666085362434387, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.6615, 'grad_norm': 1.2029120922088623, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.5434, 'grad_norm': 0.7978774309158325, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.3507, 'grad_norm': 1.6762295961380005, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.3265, 'grad_norm': 0.9521912336349487, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4978, 'train_samples_per_second': 49.722, 'train_steps_per_second': 6.219, 'train_loss': 1.642643709440489, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_37/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.57
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)]]
proposed candidate before processing: tensor([0.1435, 0.1619, 0.1365, 0.0674, 0.1030, 0.0275, 0.0791, 0.2812, 0.0102,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8968, 0.1000])
proposed candidate after normalizing: [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), 0, tensor(0.0791), tensor(0.2812), 0, 1, 1, 1, 1, 1, 115, 0.10000000149011612]
iteration:  38
input_X:  [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), 0, tensor(0.0791), tensor(0.2812), 0, 1, 1, 1, 1, 1, 115, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  115 0.10000000149011612 0 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1520, 0.0939, 0.0981, 0.0907, 0.1682, 0.0746, 0.1090, 0.2134, 0.2707,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7505, 0.1000])
proposed candidate after normalizing: [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), 9, 1, 1, 1, 1, 1, 96, 0.10000000149011612]
iteration:  39
input_X:  [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), 9, 1, 1, 1, 1, 1, 96, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  96 0.10000000149011612 9 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 59,277,312 || all params: 8,089,538,560 || trainable%: 0.7328
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1520)
number of datapoints needed (ratio * total):  759
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0939)
number of datapoints needed (ratio * total):  469
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0981)
number of datapoints needed (ratio * total):  490
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0907)
number of datapoints needed (ratio * total):  453
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1682)
number of datapoints needed (ratio * total):  841
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0746)
number of datapoints needed (ratio * total):  372
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1090)
number of datapoints needed (ratio * total):  545
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2134)
number of datapoints needed (ratio * total):  1067
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 759
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 469
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 490
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 453
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 841
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 372
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 545
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1067
})]
length of training data:  4996
training model...
trainable params: 59,277,312 || all params: 8,089,538,560 || trainable%: 0.7328
{'loss': 2.096, 'grad_norm': 1.2810349464416504, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.2896, 'grad_norm': 0.4808919429779053, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3308, 'grad_norm': 0.7413771152496338, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3848, 'grad_norm': 0.8264418244361877, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.4476, 'grad_norm': 0.37338823080062866, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.4114, 'grad_norm': 0.75199955701828, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.5558, 'grad_norm': 1.8636631965637207, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5369, 'train_samples_per_second': 49.693, 'train_steps_per_second': 6.217, 'train_loss': 1.510314103629854, 'epoch': 0.23}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_39/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.53
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)]]
proposed candidate before processing: tensor([1.3980e-01, 1.5190e-01, 1.0095e-01, 3.4559e-19, 1.1986e-01, 2.9575e-02,
        1.6311e-01, 2.9481e-01, 9.6001e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 9.5444e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1398), tensor(0.1519), tensor(0.1009), 0, tensor(0.1199), 0, tensor(0.1631), tensor(0.2948), 3, 1, 1, 1, 1, 1, 122, 0.10000000149011612]
iteration:  40
input_X:  [tensor(0.1398), tensor(0.1519), tensor(0.1009), 0, tensor(0.1199), 0, tensor(0.1631), tensor(0.2948), 3, 1, 1, 1, 1, 1, 122, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  122 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 64,628,736 || all params: 8,094,889,984 || trainable%: 0.7984
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1398), tensor(0.1519), tensor(0.1009), 0, tensor(0.1199), 0, tensor(0.1631), tensor(0.2948)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1398)
number of datapoints needed (ratio * total):  699
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1519)
number of datapoints needed (ratio * total):  759
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1009)
number of datapoints needed (ratio * total):  504
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1199)
number of datapoints needed (ratio * total):  599
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1631)
number of datapoints needed (ratio * total):  815
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2948)
number of datapoints needed (ratio * total):  1474
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 699
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 759
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 504
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 599
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 815
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1474
})]
length of training data:  4850
training model...
trainable params: 64,628,736 || all params: 8,094,889,984 || trainable%: 0.7984
{'loss': 1.7981, 'grad_norm': 0.7905853986740112, 'learning_rate': 0.0002949748743718593, 'epoch': 0.03}
{'loss': 1.2748, 'grad_norm': 0.6584811806678772, 'learning_rate': 0.0002849246231155779, 'epoch': 0.07}
{'loss': 1.3822, 'grad_norm': 0.6088328957557678, 'learning_rate': 0.0002748743718592965, 'epoch': 0.1}
{'loss': 1.4384, 'grad_norm': 0.489692747592926, 'learning_rate': 0.0002648241206030151, 'epoch': 0.13}
{'loss': 1.3076, 'grad_norm': 0.7288740873336792, 'learning_rate': 0.0002547738693467337, 'epoch': 0.16}
{'loss': 1.3903, 'grad_norm': 0.37137335538864136, 'learning_rate': 0.00024472361809045227, 'epoch': 0.2}
{'loss': 1.2031, 'grad_norm': 0.6580158472061157, 'learning_rate': 0.00023467336683417084, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5287, 'train_samples_per_second': 48.245, 'train_steps_per_second': 6.038, 'train_loss': 1.402914191192051, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_40/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1442, 0.1544, 0.1008, 0.0000, 0.1232, 0.0335, 0.1531, 0.2908, 0.0957,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9405, 0.1000])
proposed candidate after normalizing: [tensor(0.1442), tensor(0.1544), tensor(0.1008), 0, tensor(0.1232), 0, tensor(0.1531), tensor(0.2908), 3, 1, 1, 1, 1, 1, 120, 0.10000000149011612]
iteration:  41
input_X:  [tensor(0.1442), tensor(0.1544), tensor(0.1008), 0, tensor(0.1232), 0, tensor(0.1531), tensor(0.2908), 3, 1, 1, 1, 1, 1, 120, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  120 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 24,698,880 || all params: 8,054,960,128 || trainable%: 0.3066
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1442), tensor(0.1544), tensor(0.1008), 0, tensor(0.1232), 0, tensor(0.1531), tensor(0.2908)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1442)
number of datapoints needed (ratio * total):  721
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1544)
number of datapoints needed (ratio * total):  771
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1008)
number of datapoints needed (ratio * total):  503
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1232)
number of datapoints needed (ratio * total):  616
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1531)
number of datapoints needed (ratio * total):  765
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2908)
number of datapoints needed (ratio * total):  1453
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 721
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 771
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 503
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 616
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 765
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1453
})]
length of training data:  4829
training model...
trainable params: 24,698,880 || all params: 8,054,960,128 || trainable%: 0.3066
{'loss': 2.2267, 'grad_norm': 1.281480312347412, 'learning_rate': 0.00029494949494949493, 'epoch': 0.03}
{'loss': 1.8268, 'grad_norm': 4.066915512084961, 'learning_rate': 0.0002848484848484848, 'epoch': 0.07}
{'loss': 1.6335, 'grad_norm': 0.5354596972465515, 'learning_rate': 0.0002747474747474747, 'epoch': 0.1}
{'loss': 1.6731, 'grad_norm': 0.40275150537490845, 'learning_rate': 0.00026464646464646464, 'epoch': 0.13}
{'loss': 1.3234, 'grad_norm': 0.5589962601661682, 'learning_rate': 0.0002545454545454545, 'epoch': 0.17}
{'loss': 1.5627, 'grad_norm': 0.446786493062973, 'learning_rate': 0.00024444444444444443, 'epoch': 0.2}
{'loss': 1.3993, 'grad_norm': 0.3826906681060791, 'learning_rate': 0.00023434343434343432, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.7124, 'train_samples_per_second': 47.948, 'train_steps_per_second': 5.997, 'train_loss': 1.640966684390337, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_41/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.61
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1566, 0.1835, 0.1151, 0.0324, 0.0795, 0.0017, 0.1695, 0.2616, 0.0942,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8967, 0.1000])
proposed candidate after normalizing: [tensor(0.1566), tensor(0.1835), tensor(0.1151), 0, tensor(0.0795), 0, tensor(0.1695), tensor(0.2616), 3, 1, 1, 1, 1, 1, 115, 0.10000000149011612]
iteration:  42
input_X:  [tensor(0.1566), tensor(0.1835), tensor(0.1151), 0, tensor(0.0795), 0, tensor(0.1695), tensor(0.2616), 3, 1, 1, 1, 1, 1, 115, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  115 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 23,669,760 || all params: 8,053,931,008 || trainable%: 0.2939
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1566), tensor(0.1835), tensor(0.1151), 0, tensor(0.0795), 0, tensor(0.1695), tensor(0.2616)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1566)
number of datapoints needed (ratio * total):  783
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1835)
number of datapoints needed (ratio * total):  917
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1151)
number of datapoints needed (ratio * total):  575
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0795)
number of datapoints needed (ratio * total):  397
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1695)
number of datapoints needed (ratio * total):  847
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2616)
number of datapoints needed (ratio * total):  1308
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 783
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 917
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 575
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 397
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 847
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1308
})]
length of training data:  4827
training model...
trainable params: 23,669,760 || all params: 8,053,931,008 || trainable%: 0.2939
{'loss': 2.245, 'grad_norm': 0.6623477935791016, 'learning_rate': 0.00029494949494949493, 'epoch': 0.03}
{'loss': 1.4886, 'grad_norm': 0.8112929463386536, 'learning_rate': 0.0002848484848484848, 'epoch': 0.07}
{'loss': 1.7779, 'grad_norm': 0.6010546088218689, 'learning_rate': 0.0002747474747474747, 'epoch': 0.1}
{'loss': 1.5107, 'grad_norm': 0.8410532474517822, 'learning_rate': 0.00026464646464646464, 'epoch': 0.13}
{'loss': 1.3203, 'grad_norm': 1.079505205154419, 'learning_rate': 0.0002545454545454545, 'epoch': 0.17}
{'loss': 1.3923, 'grad_norm': 0.6258867979049683, 'learning_rate': 0.00024444444444444443, 'epoch': 0.2}
{'loss': 1.3741, 'grad_norm': 0.7585841417312622, 'learning_rate': 0.00023434343434343432, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6231, 'train_samples_per_second': 47.971, 'train_steps_per_second': 6.003, 'train_loss': 1.5575019068687967, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_42/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1367, 0.1075, 0.0941, 0.0168, 0.1496, 0.0673, 0.1055, 0.3226, 0.0955,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8817, 0.1000])
proposed candidate after normalizing: [tensor(0.1367), tensor(0.1075), tensor(0.0941), 0, tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), 3, 1, 1, 1, 1, 1, 113, 0.10000000149011612]
iteration:  43
input_X:  [tensor(0.1367), tensor(0.1075), tensor(0.0941), 0, tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), 3, 1, 1, 1, 1, 1, 113, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  113 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 23,258,112 || all params: 8,053,519,360 || trainable%: 0.2888
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1367), tensor(0.1075), tensor(0.0941), 0, tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1367)
number of datapoints needed (ratio * total):  683
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1075)
number of datapoints needed (ratio * total):  537
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0941)
number of datapoints needed (ratio * total):  470
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1496)
number of datapoints needed (ratio * total):  748
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0673)
number of datapoints needed (ratio * total):  336
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1055)
number of datapoints needed (ratio * total):  527
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3226)
number of datapoints needed (ratio * total):  1613
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 683
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 537
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 470
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 748
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 336
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 527
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1613
})]
length of training data:  4914
training model...
trainable params: 23,258,112 || all params: 8,053,519,360 || trainable%: 0.2888
{'loss': 2.4774, 'grad_norm': 2.9917995929718018, 'learning_rate': 0.0002950413223140496, 'epoch': 0.03}
{'loss': 1.8688, 'grad_norm': 1.346015453338623, 'learning_rate': 0.0002851239669421488, 'epoch': 0.07}
{'loss': 1.7924, 'grad_norm': 1.6191513538360596, 'learning_rate': 0.0002752066115702479, 'epoch': 0.1}
{'loss': 1.3805, 'grad_norm': 2.378537654876709, 'learning_rate': 0.0002652892561983471, 'epoch': 0.13}
{'loss': 1.5113, 'grad_norm': 0.5665228366851807, 'learning_rate': 0.00025537190082644627, 'epoch': 0.16}
{'loss': 1.6248, 'grad_norm': 0.7650730013847351, 'learning_rate': 0.00024545454545454545, 'epoch': 0.2}
{'loss': 1.66, 'grad_norm': 0.9345031976699829, 'learning_rate': 0.00023553719008264463, 'epoch': 0.23}
{'loss': 1.22, 'grad_norm': 1.3017829656600952, 'learning_rate': 0.00022561983471074378, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2936, 'train_samples_per_second': 48.996, 'train_steps_per_second': 6.132, 'train_loss': 1.6968483325250134, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_43/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.61
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1098, 0.1353, 0.1138, 0.0922, 0.0741, 0.0180, 0.1636, 0.2932, 0.0944,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9202, 0.1000])
proposed candidate after normalizing: [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), 0, tensor(0.1636), tensor(0.2932), 3, 1, 1, 1, 1, 1, 118, 0.10000000149011612]
iteration:  44
input_X:  [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), 0, tensor(0.1636), tensor(0.2932), 3, 1, 1, 1, 1, 1, 118, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  118 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 24,287,232 || all params: 8,054,548,480 || trainable%: 0.3015
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), 0, tensor(0.1636), tensor(0.2932)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1098)
number of datapoints needed (ratio * total):  548
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1353)
number of datapoints needed (ratio * total):  676
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1138)
number of datapoints needed (ratio * total):  569
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0922)
number of datapoints needed (ratio * total):  460
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0741)
number of datapoints needed (ratio * total):  370
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1636)
number of datapoints needed (ratio * total):  817
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2932)
number of datapoints needed (ratio * total):  1466
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 548
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 676
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 569
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 460
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 370
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 817
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1466
})]
length of training data:  4906
training model...
trainable params: 24,287,232 || all params: 8,054,548,480 || trainable%: 0.3015
{'loss': 2.2847, 'grad_norm': 0.8729068636894226, 'learning_rate': 0.0002950331125827814, 'epoch': 0.03}
{'loss': 1.9467, 'grad_norm': 1.862032175064087, 'learning_rate': 0.00028509933774834435, 'epoch': 0.07}
{'loss': 1.7187, 'grad_norm': 0.760432779788971, 'learning_rate': 0.0002751655629139073, 'epoch': 0.1}
{'loss': 1.7393, 'grad_norm': 0.5412049293518066, 'learning_rate': 0.00026523178807947017, 'epoch': 0.13}
{'loss': 1.3542, 'grad_norm': 1.8285417556762695, 'learning_rate': 0.0002552980132450331, 'epoch': 0.16}
{'loss': 1.2952, 'grad_norm': 2.079261302947998, 'learning_rate': 0.000245364238410596, 'epoch': 0.2}
{'loss': 1.5874, 'grad_norm': 1.0877971649169922, 'learning_rate': 0.00023543046357615892, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.7172, 'train_samples_per_second': 48.711, 'train_steps_per_second': 6.096, 'train_loss': 1.6778628198724044, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_44/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.61
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)]]
proposed candidate before processing: tensor([1.8484e-01, 1.9579e-01, 1.0624e-01, 2.3785e-18, 1.4262e-01, 4.1777e-02,
        1.0604e-01, 2.2270e-01, 8.7956e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.6958e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1848), tensor(0.1958), tensor(0.1062), 0, tensor(0.1426), 0, tensor(0.1060), tensor(0.2227), 3, 1, 1, 1, 1, 1, 111, 0.10000000149011612]
iteration:  45
input_X:  [tensor(0.1848), tensor(0.1958), tensor(0.1062), 0, tensor(0.1426), 0, tensor(0.1060), tensor(0.2227), 3, 1, 1, 1, 1, 1, 111, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  111 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 22,846,464 || all params: 8,053,107,712 || trainable%: 0.2837
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1848), tensor(0.1958), tensor(0.1062), 0, tensor(0.1426), 0, tensor(0.1060), tensor(0.2227)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1848)
number of datapoints needed (ratio * total):  924
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1958)
number of datapoints needed (ratio * total):  978
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1062)
number of datapoints needed (ratio * total):  531
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1426)
number of datapoints needed (ratio * total):  713
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1060)
number of datapoints needed (ratio * total):  530
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2227)
number of datapoints needed (ratio * total):  1113
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 924
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 978
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 531
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 713
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 530
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1113
})]
length of training data:  4789
training model...
trainable params: 22,846,464 || all params: 8,053,107,712 || trainable%: 0.2837
{'loss': 1.9974, 'grad_norm': 0.6178949475288391, 'learning_rate': 0.0002949066213921901, 'epoch': 0.03}
{'loss': 1.6189, 'grad_norm': 1.3503338098526, 'learning_rate': 0.00028471986417657043, 'epoch': 0.07}
{'loss': 1.4093, 'grad_norm': 0.9457380175590515, 'learning_rate': 0.0002745331069609507, 'epoch': 0.1}
{'loss': 1.3449, 'grad_norm': 1.3297749757766724, 'learning_rate': 0.00026434634974533105, 'epoch': 0.13}
{'loss': 0.9332, 'grad_norm': 0.9732027649879456, 'learning_rate': 0.00025415959252971134, 'epoch': 0.17}
{'loss': 1.3326, 'grad_norm': 4.251399040222168, 'learning_rate': 0.00024397283531409167, 'epoch': 0.2}
{'loss': 1.0408, 'grad_norm': 0.5474039912223816, 'learning_rate': 0.00023378607809847199, 'epoch': 0.23}
{'loss': 1.3254, 'grad_norm': 0.4584292769432068, 'learning_rate': 0.0002235993208828523, 'epoch': 0.27}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3098, 'train_samples_per_second': 47.742, 'train_steps_per_second': 5.971, 'train_loss': 1.383009844356113, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_45/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.58
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1723, 0.0949, 0.1357, 0.0448, 0.0597, 0.0211, 0.1395, 0.3320, 0.0964,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8722, 0.1000])
proposed candidate after normalizing: [tensor(0.1723), tensor(0.0949), tensor(0.1357), 0, tensor(0.0597), 0, tensor(0.1395), tensor(0.3320), 3, 1, 1, 1, 1, 1, 112, 0.10000000149011612]
iteration:  46
input_X:  [tensor(0.1723), tensor(0.0949), tensor(0.1357), 0, tensor(0.0597), 0, tensor(0.1395), tensor(0.3320), 3, 1, 1, 1, 1, 1, 112, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  112 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 23,052,288 || all params: 8,053,313,536 || trainable%: 0.2862
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1723), tensor(0.0949), tensor(0.1357), 0, tensor(0.0597), 0, tensor(0.1395), tensor(0.3320)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1723)
number of datapoints needed (ratio * total):  861
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0949)
number of datapoints needed (ratio * total):  474
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1357)
number of datapoints needed (ratio * total):  678
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0597)
number of datapoints needed (ratio * total):  298
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1395)
number of datapoints needed (ratio * total):  697
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3320)
number of datapoints needed (ratio * total):  1659
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 861
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 474
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 678
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 298
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 697
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1659
})]
length of training data:  4667
training model...
trainable params: 23,052,288 || all params: 8,053,313,536 || trainable%: 0.2862
{'loss': 2.6107, 'grad_norm': 2.729696750640869, 'learning_rate': 0.00029477351916376305, 'epoch': 0.03}
{'loss': 2.1812, 'grad_norm': 1.6279891729354858, 'learning_rate': 0.00028432055749128915, 'epoch': 0.07}
{'loss': 1.4279, 'grad_norm': 1.4555448293685913, 'learning_rate': 0.0002738675958188153, 'epoch': 0.1}
{'loss': 1.718, 'grad_norm': 0.6981697082519531, 'learning_rate': 0.00026341463414634146, 'epoch': 0.14}
{'loss': 1.6478, 'grad_norm': 0.6570343971252441, 'learning_rate': 0.00025296167247386756, 'epoch': 0.17}
{'loss': 1.5024, 'grad_norm': 2.3765146732330322, 'learning_rate': 0.0002425087108013937, 'epoch': 0.21}
{'loss': 1.7468, 'grad_norm': 0.39323824644088745, 'learning_rate': 0.00023205574912891984, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2699, 'train_samples_per_second': 46.544, 'train_steps_per_second': 5.824, 'train_loss': 1.80934203363234, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_46/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)]]
proposed candidate before processing: tensor([1.1661e-01, 1.1837e-01, 1.4796e-01, 1.0273e-17, 8.2867e-02, 8.5260e-02,
        1.9024e-01, 2.5870e-01, 5.9666e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 9.8736e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1166), tensor(0.1184), tensor(0.1480), 0, tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), 2, 1, 1, 1, 1, 1, 126, 0.10000000149011612]
iteration:  47
input_X:  [tensor(0.1166), tensor(0.1184), tensor(0.1480), 0, tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), 2, 1, 1, 1, 1, 1, 126, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  126 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 24,973,312 || all params: 8,055,234,560 || trainable%: 0.3100
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1166), tensor(0.1184), tensor(0.1480), 0, tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1166)
number of datapoints needed (ratio * total):  583
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1184)
number of datapoints needed (ratio * total):  591
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1480)
number of datapoints needed (ratio * total):  739
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0829)
number of datapoints needed (ratio * total):  414
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0853)
number of datapoints needed (ratio * total):  426
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1902)
number of datapoints needed (ratio * total):  951
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2587)
number of datapoints needed (ratio * total):  1293
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 583
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 591
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 739
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 414
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 426
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 951
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1293
})]
length of training data:  4997
training model...
trainable params: 24,973,312 || all params: 8,055,234,560 || trainable%: 0.3100
{'loss': 1.6604, 'grad_norm': 0.5457329154014587, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.2366, 'grad_norm': 0.5855756402015686, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.2215, 'grad_norm': 0.9515398144721985, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.4211, 'grad_norm': 0.7601978182792664, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.1917, 'grad_norm': 0.9442537426948547, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.365, 'grad_norm': 0.8794176578521729, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 0.9821, 'grad_norm': 0.6473482251167297, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.621, 'train_samples_per_second': 49.662, 'train_steps_per_second': 6.211, 'train_loss': 1.3117941838161202, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_47/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.6
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)]]
proposed candidate before processing: tensor([0.0862, 0.1868, 0.1546, 0.0134, 0.0776, 0.0160, 0.1040, 0.3615, 0.0766,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8892, 0.1000])
proposed candidate after normalizing: [tensor(0.0862), tensor(0.1868), tensor(0.1546), 0, tensor(0.0776), 0, tensor(0.1040), tensor(0.3615), 2, 1, 1, 1, 1, 1, 114, 0.10000000149011612]
iteration:  48
input_X:  [tensor(0.0862), tensor(0.1868), tensor(0.1546), 0, tensor(0.0776), 0, tensor(0.1040), tensor(0.3615), 2, 1, 1, 1, 1, 1, 114, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  114 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 15,642,624 || all params: 8,045,903,872 || trainable%: 0.1944
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0862), tensor(0.1868), tensor(0.1546), 0, tensor(0.0776), 0, tensor(0.1040), tensor(0.3615)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0862)
number of datapoints needed (ratio * total):  430
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1868)
number of datapoints needed (ratio * total):  934
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1546)
number of datapoints needed (ratio * total):  772
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0776)
number of datapoints needed (ratio * total):  387
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1040)
number of datapoints needed (ratio * total):  520
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3615)
number of datapoints needed (ratio * total):  1807
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 430
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 934
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 772
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 387
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 520
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1807
})]
length of training data:  4850
training model...
trainable params: 15,642,624 || all params: 8,045,903,872 || trainable%: 0.1944
{'loss': 2.4792, 'grad_norm': 1.675957202911377, 'learning_rate': 0.0002949748743718593, 'epoch': 0.03}
{'loss': 1.8949, 'grad_norm': 0.6878703236579895, 'learning_rate': 0.0002849246231155779, 'epoch': 0.07}
{'loss': 1.9557, 'grad_norm': 0.6930302381515503, 'learning_rate': 0.0002748743718592965, 'epoch': 0.1}
{'loss': 1.6922, 'grad_norm': 0.4355899393558502, 'learning_rate': 0.0002648241206030151, 'epoch': 0.13}
{'loss': 1.6716, 'grad_norm': 0.9714266061782837, 'learning_rate': 0.0002547738693467337, 'epoch': 0.16}
{'loss': 1.5998, 'grad_norm': 0.6439809203147888, 'learning_rate': 0.00024472361809045227, 'epoch': 0.2}
{'loss': 1.2319, 'grad_norm': 0.5105912685394287, 'learning_rate': 0.00023467336683417084, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.19, 'train_samples_per_second': 48.408, 'train_steps_per_second': 6.058, 'train_loss': 1.7758129468930313, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_48/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.7
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)]]
proposed candidate before processing: tensor([5.6782e-02, 2.1655e-01, 1.6701e-01, 3.8625e-19, 6.1343e-02, 3.0999e-03,
        8.5411e-02, 4.0980e-01, 7.9072e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.8422e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.0568), tensor(0.2166), tensor(0.1670), 0, tensor(0.0613), 0, tensor(0.0854), tensor(0.4098), 3, 1, 1, 1, 1, 1, 113, 0.10000000149011612]
iteration:  49
input_X:  [tensor(0.0568), tensor(0.2166), tensor(0.1670), 0, tensor(0.0613), 0, tensor(0.0854), tensor(0.4098), 3, 1, 1, 1, 1, 1, 113, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  113 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 23,258,112 || all params: 8,053,519,360 || trainable%: 0.2888
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0568), tensor(0.2166), tensor(0.1670), 0, tensor(0.0613), 0, tensor(0.0854), tensor(0.4098)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0568)
number of datapoints needed (ratio * total):  283
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2166)
number of datapoints needed (ratio * total):  1082
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1670)
number of datapoints needed (ratio * total):  835
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0613)
number of datapoints needed (ratio * total):  306
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0854)
number of datapoints needed (ratio * total):  427
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.4098)
number of datapoints needed (ratio * total):  2049
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 283
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1082
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 835
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 306
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 427
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 2049
})]
length of training data:  4982
training model...
trainable params: 23,258,112 || all params: 8,053,519,360 || trainable%: 0.2888
{'loss': 2.2153, 'grad_norm': 0.7022159695625305, 'learning_rate': 0.00029510603588907015, 'epoch': 0.03}
{'loss': 1.7882, 'grad_norm': 0.7730570435523987, 'learning_rate': 0.00028531810766721044, 'epoch': 0.06}
{'loss': 1.8201, 'grad_norm': 0.4949142634868622, 'learning_rate': 0.00027553017944535074, 'epoch': 0.1}
{'loss': 1.269, 'grad_norm': 0.9347051382064819, 'learning_rate': 0.00026574225122349103, 'epoch': 0.13}
{'loss': 1.3896, 'grad_norm': 0.643338143825531, 'learning_rate': 0.0002559543230016313, 'epoch': 0.16}
{'loss': 1.6696, 'grad_norm': 0.31112921237945557, 'learning_rate': 0.0002461663947797716, 'epoch': 0.19}
{'loss': 1.5694, 'grad_norm': 0.3353790044784546, 'learning_rate': 0.00023637846655791189, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0656, 'train_samples_per_second': 49.787, 'train_steps_per_second': 6.226, 'train_loss': 1.6920902406847156, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_49/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)]]
proposed candidate before processing: tensor([0.0721, 0.2182, 0.1098, 0.0000, 0.0654, 0.0364, 0.1061, 0.3920, 0.0717,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9068, 0.1000])
proposed candidate after normalizing: [tensor(0.0721), tensor(0.2182), tensor(0.1098), 0, tensor(0.0654), 0, tensor(0.1061), tensor(0.3920), 2, 1, 1, 1, 1, 1, 116, 0.10000000149011612]
iteration:  50
input_X:  [tensor(0.0721), tensor(0.2182), tensor(0.1098), 0, tensor(0.0654), 0, tensor(0.1061), tensor(0.3920), 2, 1, 1, 1, 1, 1, 116, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  116 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 23,669,760 || all params: 8,053,931,008 || trainable%: 0.2939
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0721), tensor(0.2182), tensor(0.1098), 0, tensor(0.0654), 0, tensor(0.1061), tensor(0.3920)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0721)
number of datapoints needed (ratio * total):  360
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2182)
number of datapoints needed (ratio * total):  1090
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1098)
number of datapoints needed (ratio * total):  548
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0654)
number of datapoints needed (ratio * total):  327
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1061)
number of datapoints needed (ratio * total):  530
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3920)
number of datapoints needed (ratio * total):  1960
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 360
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1090
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 548
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 327
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 530
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1960
})]
length of training data:  4815
training model...
trainable params: 23,669,760 || all params: 8,053,931,008 || trainable%: 0.2939
{'loss': 2.1139, 'grad_norm': 0.7833811640739441, 'learning_rate': 0.0002949324324324324, 'epoch': 0.03}
{'loss': 1.6963, 'grad_norm': 0.5170372724533081, 'learning_rate': 0.0002847972972972973, 'epoch': 0.07}
{'loss': 1.6898, 'grad_norm': 1.2938975095748901, 'learning_rate': 0.0002746621621621621, 'epoch': 0.1}
{'loss': 1.4685, 'grad_norm': 0.7957823872566223, 'learning_rate': 0.000264527027027027, 'epoch': 0.13}
{'loss': 1.5515, 'grad_norm': 0.5988488793373108, 'learning_rate': 0.0002543918918918919, 'epoch': 0.17}
{'loss': 1.7066, 'grad_norm': 0.5878424644470215, 'learning_rate': 0.00024425675675675675, 'epoch': 0.2}
{'loss': 1.3697, 'grad_norm': 0.653428316116333, 'learning_rate': 0.00023412162162162159, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0983, 'train_samples_per_second': 48.103, 'train_steps_per_second': 6.014, 'train_loss': 1.6149922734812687, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_50/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)]]
proposed candidate before processing: tensor([7.8168e-02, 1.5671e-01, 2.2979e-01, 0.0000e+00, 8.7019e-02, 8.3463e-18,
        9.2883e-02, 3.5542e-01, 7.2369e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.7843e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.0782), tensor(0.1567), tensor(0.2298), 0, tensor(0.0870), 0, tensor(0.0929), tensor(0.3554), 2, 1, 1, 1, 1, 1, 112, 0.10000000149011612]
iteration:  51
input_X:  [tensor(0.0782), tensor(0.1567), tensor(0.2298), 0, tensor(0.0870), 0, tensor(0.0929), tensor(0.3554), 2, 1, 1, 1, 1, 1, 112, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  112 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 15,368,192 || all params: 8,045,629,440 || trainable%: 0.1910
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0782), tensor(0.1567), tensor(0.2298), 0, tensor(0.0870), 0, tensor(0.0929), tensor(0.3554)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0782)
number of datapoints needed (ratio * total):  390
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1567)
number of datapoints needed (ratio * total):  783
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.2298)
number of datapoints needed (ratio * total):  1148
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0870)
number of datapoints needed (ratio * total):  435
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0929)
number of datapoints needed (ratio * total):  464
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3554)
number of datapoints needed (ratio * total):  1777
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 390
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 783
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1148
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 435
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 464
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1777
})]
length of training data:  4997
training model...
trainable params: 15,368,192 || all params: 8,045,629,440 || trainable%: 0.1910
{'loss': 2.4174, 'grad_norm': 0.9939389228820801, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5894, 'grad_norm': 0.6782426834106445, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.6776, 'grad_norm': 0.5300418734550476, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.5362, 'grad_norm': 0.6180168986320496, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.4233, 'grad_norm': 0.8439741730690002, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.7337, 'grad_norm': 0.7185682058334351, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.3925, 'grad_norm': 0.48447439074516296, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2854, 'train_samples_per_second': 49.828, 'train_steps_per_second': 6.232, 'train_loss': 1.679344529273526, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_51/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.64
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)]]
proposed candidate before processing: tensor([0.0940, 0.2135, 0.1208, 0.0081, 0.0627, 0.0000, 0.0676, 0.4333, 0.1098,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8847, 0.1000])
proposed candidate after normalizing: [tensor(0.0940), tensor(0.2135), tensor(0.1208), 0, tensor(0.0627), 0, tensor(0.0676), tensor(0.4333), 4, 1, 1, 1, 1, 1, 113, 0.10000000149011612]
iteration:  52
input_X:  [tensor(0.0940), tensor(0.2135), tensor(0.1208), 0, tensor(0.0627), 0, tensor(0.0676), tensor(0.4333), 4, 1, 1, 1, 1, 1, 113, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  113 0.10000000149011612 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 31,010,816 || all params: 8,061,272,064 || trainable%: 0.3847
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0940), tensor(0.2135), tensor(0.1208), 0, tensor(0.0627), 0, tensor(0.0676), tensor(0.4333)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0940)
number of datapoints needed (ratio * total):  469
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2135)
number of datapoints needed (ratio * total):  1067
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1208)
number of datapoints needed (ratio * total):  604
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0627)
number of datapoints needed (ratio * total):  313
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0676)
number of datapoints needed (ratio * total):  338
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.4333)
number of datapoints needed (ratio * total):  2166
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 469
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1067
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 604
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 313
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 338
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 2166
})]
length of training data:  4957
training model...
trainable params: 31,010,816 || all params: 8,061,272,064 || trainable%: 0.3847
{'loss': 2.255, 'grad_norm': 0.7117720246315002, 'learning_rate': 0.0002950819672131147, 'epoch': 0.03}
{'loss': 1.7011, 'grad_norm': 2.8147690296173096, 'learning_rate': 0.00028524590163934424, 'epoch': 0.06}
{'loss': 2.0288, 'grad_norm': 0.3753727674484253, 'learning_rate': 0.00027540983606557377, 'epoch': 0.1}
{'loss': 1.6858, 'grad_norm': 0.5851454734802246, 'learning_rate': 0.00026557377049180324, 'epoch': 0.13}
{'loss': 1.8309, 'grad_norm': 0.9086145758628845, 'learning_rate': 0.00025573770491803277, 'epoch': 0.16}
{'loss': 1.7353, 'grad_norm': 0.46540284156799316, 'learning_rate': 0.0002459016393442623, 'epoch': 0.19}
{'loss': 1.5359, 'grad_norm': 0.512294352054596, 'learning_rate': 0.00023606557377049177, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3682, 'train_samples_per_second': 49.388, 'train_steps_per_second': 6.177, 'train_loss': 1.811913464864095, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_52/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)]]
proposed candidate before processing: tensor([1.4313e-01, 2.0665e-01, 1.2984e-01, 1.2880e-17, 7.3261e-03, 1.3375e-18,
        1.0239e-01, 4.1066e-01, 7.4968e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.9476e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1431), tensor(0.2067), tensor(0.1298), 0, 0, 0, tensor(0.1024), tensor(0.4107), 2, 1, 1, 1, 1, 1, 115, 0.10000000149011612]
iteration:  53
input_X:  [tensor(0.1431), tensor(0.2067), tensor(0.1298), 0, 0, 0, tensor(0.1024), tensor(0.4107), 2, 1, 1, 1, 1, 1, 115, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  115 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 31,285,248 || all params: 8,061,546,496 || trainable%: 0.3881
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1431), tensor(0.2067), tensor(0.1298), 0, 0, 0, tensor(0.1024), tensor(0.4107)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1431)
number of datapoints needed (ratio * total):  715
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2067)
number of datapoints needed (ratio * total):  1033
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1298)
number of datapoints needed (ratio * total):  649
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1024)
number of datapoints needed (ratio * total):  511
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.4107)
number of datapoints needed (ratio * total):  2053
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 715
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1033
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 649
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 511
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 2053
})]
length of training data:  4961
training model...
trainable params: 31,285,248 || all params: 8,061,546,496 || trainable%: 0.3881
{'loss': 2.107, 'grad_norm': 0.5167065262794495, 'learning_rate': 0.0002950900163666121, 'epoch': 0.03}
{'loss': 1.805, 'grad_norm': 0.4130566418170929, 'learning_rate': 0.0002852700490998363, 'epoch': 0.06}
{'loss': 1.3831, 'grad_norm': 1.0969427824020386, 'learning_rate': 0.00027545008183306056, 'epoch': 0.1}
{'loss': 1.8709, 'grad_norm': 0.5532855987548828, 'learning_rate': 0.0002656301145662848, 'epoch': 0.13}
{'loss': 2.0464, 'grad_norm': 0.786919355392456, 'learning_rate': 0.000255810147299509, 'epoch': 0.16}
{'loss': 1.299, 'grad_norm': 0.34934014081954956, 'learning_rate': 0.0002459901800327332, 'epoch': 0.19}
{'loss': 1.3947, 'grad_norm': 0.5885937213897705, 'learning_rate': 0.00023617021276595742, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4738, 'train_samples_per_second': 49.376, 'train_steps_per_second': 6.181, 'train_loss': 1.674260793947706, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_53/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.68
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)]]
proposed candidate before processing: tensor([1.6291e-01, 1.9337e-01, 1.2229e-01, 0.0000e+00, 0.0000e+00, 1.4908e-19,
        7.9992e-02, 4.4144e-01, 7.4964e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 9.1441e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1629), tensor(0.1934), tensor(0.1223), 0, 0, 0, tensor(0.0800), tensor(0.4414), 2, 1, 1, 1, 1, 1, 117, 0.10000000149011612]
iteration:  54
input_X:  [tensor(0.1629), tensor(0.1934), tensor(0.1223), 0, 0, 0, tensor(0.0800), tensor(0.4414), 2, 1, 1, 1, 1, 1, 117, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  117 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 16,054,272 || all params: 8,046,315,520 || trainable%: 0.1995
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1629), tensor(0.1934), tensor(0.1223), 0, 0, 0, tensor(0.0800), tensor(0.4414)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1629)
number of datapoints needed (ratio * total):  814
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1934)
number of datapoints needed (ratio * total):  966
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1223)
number of datapoints needed (ratio * total):  611
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0800)
number of datapoints needed (ratio * total):  399
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.4414)
number of datapoints needed (ratio * total):  2207
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 814
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 966
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 611
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 399
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 2207
})]
length of training data:  4997
training model...
trainable params: 16,054,272 || all params: 8,046,315,520 || trainable%: 0.1995
{'loss': 2.3905, 'grad_norm': 1.6493091583251953, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.9238, 'grad_norm': 0.530868649482727, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.9549, 'grad_norm': 0.41248151659965515, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.811, 'grad_norm': 0.839189350605011, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.9197, 'grad_norm': 1.3164852857589722, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.7694, 'grad_norm': 1.9747774600982666, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.7363, 'grad_norm': 0.5282752513885498, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0435, 'train_samples_per_second': 49.948, 'train_steps_per_second': 6.247, 'train_loss': 1.9205862299601237, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_54/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.61
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)]]
proposed candidate before processing: tensor([0.0543, 0.2659, 0.1477, 0.0216, 0.0467, 0.0000, 0.1420, 0.3218, 0.0699,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8692, 0.1000])
proposed candidate after normalizing: [tensor(0.0543), tensor(0.2659), tensor(0.1477), 0, 0, 0, tensor(0.1420), tensor(0.3218), 2, 1, 1, 1, 1, 1, 111, 0.10000000149011612]
iteration:  55
input_X:  [tensor(0.0543), tensor(0.2659), tensor(0.1477), 0, 0, 0, tensor(0.1420), tensor(0.3218), 2, 1, 1, 1, 1, 1, 111, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  111 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 15,230,976 || all params: 8,045,492,224 || trainable%: 0.1893
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0543), tensor(0.2659), tensor(0.1477), 0, 0, 0, tensor(0.1420), tensor(0.3218)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0543)
number of datapoints needed (ratio * total):  271
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2659)
number of datapoints needed (ratio * total):  1329
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1477)
number of datapoints needed (ratio * total):  738
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1420)
number of datapoints needed (ratio * total):  709
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3218)
number of datapoints needed (ratio * total):  1608
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 271
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1329
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 738
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 709
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1608
})]
length of training data:  4655
training model...
trainable params: 15,230,976 || all params: 8,045,492,224 || trainable%: 0.1893
{'loss': 2.1913, 'grad_norm': 0.498620867729187, 'learning_rate': 0.0002947552447552447, 'epoch': 0.03}
{'loss': 1.9794, 'grad_norm': 0.5357699990272522, 'learning_rate': 0.0002842657342657343, 'epoch': 0.07}
{'loss': 1.6226, 'grad_norm': 0.6955962181091309, 'learning_rate': 0.0002737762237762238, 'epoch': 0.1}
{'loss': 1.6565, 'grad_norm': 0.5372545123100281, 'learning_rate': 0.0002632867132867133, 'epoch': 0.14}
{'loss': 1.612, 'grad_norm': 0.3813892900943756, 'learning_rate': 0.0002527972027972028, 'epoch': 0.17}
{'loss': 1.4648, 'grad_norm': 0.5686149001121521, 'learning_rate': 0.0002423076923076923, 'epoch': 0.21}
{'loss': 1.2865, 'grad_norm': 0.45150187611579895, 'learning_rate': 0.0002318181818181818, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.398, 'train_samples_per_second': 46.365, 'train_steps_per_second': 5.797, 'train_loss': 1.7063221232096355, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_55/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)]]
proposed candidate before processing: tensor([1.0382e-01, 1.7497e-01, 1.3661e-01, 5.2956e-18, 9.0128e-02, 3.9827e-18,
        9.1908e-02, 4.0256e-01, 1.2004e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 9.2591e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1038), tensor(0.1750), tensor(0.1366), 0, tensor(0.0901), 0, tensor(0.0919), tensor(0.4026), 4, 1, 1, 1, 1, 1, 119, 0.10000000149011612]
iteration:  56
input_X:  [tensor(0.1038), tensor(0.1750), tensor(0.1366), 0, tensor(0.0901), 0, tensor(0.0919), tensor(0.4026), 4, 1, 1, 1, 1, 1, 119, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  119 0.10000000149011612 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 32,657,408 || all params: 8,062,918,656 || trainable%: 0.4050
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1038), tensor(0.1750), tensor(0.1366), 0, tensor(0.0901), 0, tensor(0.0919), tensor(0.4026)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1038)
number of datapoints needed (ratio * total):  519
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1750)
number of datapoints needed (ratio * total):  874
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1366)
number of datapoints needed (ratio * total):  683
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0901)
number of datapoints needed (ratio * total):  450
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0919)
number of datapoints needed (ratio * total):  459
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.4026)
number of datapoints needed (ratio * total):  2012
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 519
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 874
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 683
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 450
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 459
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 2012
})]
length of training data:  4997
training model...
trainable params: 32,657,408 || all params: 8,062,918,656 || trainable%: 0.4050
{'loss': 2.4762, 'grad_norm': 0.8448669910430908, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.6259, 'grad_norm': 0.7344328761100769, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.8996, 'grad_norm': 0.7153571844100952, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.6688, 'grad_norm': 0.5692277550697327, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.7526, 'grad_norm': 0.6343405246734619, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.282, 'grad_norm': 1.308478832244873, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.8095, 'grad_norm': 0.4369141757488251, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0462, 'train_samples_per_second': 49.947, 'train_steps_per_second': 6.247, 'train_loss': 1.7718277609111457, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_56/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1430, 0.1418, 0.1205, 0.0245, 0.1227, 0.0605, 0.1410, 0.2460, 0.0322,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9113, 0.0992])
proposed candidate after normalizing: [tensor(0.1430), tensor(0.1418), tensor(0.1205), 0, tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), 1, 1, 1, 1, 1, 1, 117, 0.0991612896323204]
iteration:  57
input_X:  [tensor(0.1430), tensor(0.1418), tensor(0.1205), 0, tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), 1, 1, 1, 1, 1, 1, 117, 0.0991612896323204]
mixing data with method:  random
arranging lora config with parameters:  117 0.0991612896323204 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 32,520,192 || all params: 8,062,781,440 || trainable%: 0.4033
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1430), tensor(0.1418), tensor(0.1205), 0, tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1430)
number of datapoints needed (ratio * total):  715
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1418)
number of datapoints needed (ratio * total):  708
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1205)
number of datapoints needed (ratio * total):  602
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1227)
number of datapoints needed (ratio * total):  613
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0605)
number of datapoints needed (ratio * total):  302
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1410)
number of datapoints needed (ratio * total):  705
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2460)
number of datapoints needed (ratio * total):  1229
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 715
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 708
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 602
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 613
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 302
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 705
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1229
})]
length of training data:  4874
training model...
trainable params: 32,520,192 || all params: 8,062,781,440 || trainable%: 0.4033
{'loss': 1.5318, 'grad_norm': 0.7859521508216858, 'learning_rate': 0.00029499999999999996, 'epoch': 0.03}
{'loss': 0.902, 'grad_norm': 0.4073922634124756, 'learning_rate': 0.000285, 'epoch': 0.07}
{'loss': 1.1596, 'grad_norm': 0.5481805801391602, 'learning_rate': 0.00027499999999999996, 'epoch': 0.1}
{'loss': 1.217, 'grad_norm': 1.1043957471847534, 'learning_rate': 0.000265, 'epoch': 0.13}
{'loss': 1.0277, 'grad_norm': 0.3654117286205292, 'learning_rate': 0.00025499999999999996, 'epoch': 0.16}
{'loss': 1.2904, 'grad_norm': 0.39037463068962097, 'learning_rate': 0.000245, 'epoch': 0.2}
{'loss': 1.2755, 'grad_norm': 1.019453763961792, 'learning_rate': 0.00023499999999999997, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5703, 'train_samples_per_second': 48.464, 'train_steps_per_second': 6.065, 'train_loss': 1.205900413759293, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_57/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)]]
proposed candidate before processing: tensor([0.1186, 0.1725, 0.0117, 0.0730, 0.0150, 0.0671, 0.0844, 0.4577, 0.1276,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7956, 0.0813])
proposed candidate after normalizing: [tensor(0.1186), tensor(0.1725), 0, tensor(0.0730), 0, tensor(0.0671), tensor(0.0844), tensor(0.4577), 4, 1, 1, 1, 1, 1, 102, 0.08130604773759842]
iteration:  58
input_X:  [tensor(0.1186), tensor(0.1725), 0, tensor(0.0730), 0, tensor(0.0671), tensor(0.0844), tensor(0.4577), 4, 1, 1, 1, 1, 1, 102, 0.08130604773759842]
mixing data with method:  random
arranging lora config with parameters:  102 0.08130604773759842 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 27,992,064 || all params: 8,058,253,312 || trainable%: 0.3474
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1186), tensor(0.1725), 0, tensor(0.0730), 0, tensor(0.0671), tensor(0.0844), tensor(0.4577)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1186)
number of datapoints needed (ratio * total):  593
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1725)
number of datapoints needed (ratio * total):  862
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0730)
number of datapoints needed (ratio * total):  365
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  tensor(0.0671)
number of datapoints needed (ratio * total):  335
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0844)
number of datapoints needed (ratio * total):  421
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.4577)
number of datapoints needed (ratio * total):  2288
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 593
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 862
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 365
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 335
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 421
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 2288
})]
length of training data:  4864
training model...
trainable params: 27,992,064 || all params: 8,058,253,312 || trainable%: 0.3474
{'loss': 2.5701, 'grad_norm': 0.8328296542167664, 'learning_rate': 0.00029498327759197324, 'epoch': 0.03}
{'loss': 2.0899, 'grad_norm': 0.3630159795284271, 'learning_rate': 0.00028494983277591973, 'epoch': 0.07}
{'loss': 1.878, 'grad_norm': 1.0070148706436157, 'learning_rate': 0.00027491638795986616, 'epoch': 0.1}
{'loss': 1.5254, 'grad_norm': 2.2960867881774902, 'learning_rate': 0.0002648829431438127, 'epoch': 0.13}
{'loss': 2.0163, 'grad_norm': 1.0169340372085571, 'learning_rate': 0.0002548494983277592, 'epoch': 0.16}
{'loss': 1.5962, 'grad_norm': 0.6262949705123901, 'learning_rate': 0.00024481605351170567, 'epoch': 0.2}
{'loss': 1.5129, 'grad_norm': 1.013892650604248, 'learning_rate': 0.00023478260869565215, 'epoch': 0.23}
{'loss': 1.8819, 'grad_norm': 0.4695224165916443, 'learning_rate': 0.00022474916387959864, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.455, 'train_samples_per_second': 48.42, 'train_steps_per_second': 6.052, 'train_loss': 1.8884130083484414, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_58/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)]]
proposed candidate before processing: tensor([0.1649, 0.0615, 0.0436, 0.0565, 0.0807, 0.1065, 0.0967, 0.3895, 0.1477,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8986, 0.0642])
proposed candidate after normalizing: [tensor(0.1649), tensor(0.0615), 0, tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), 5, 1, 1, 1, 1, 1, 115, 0.06416884809732437]
iteration:  59
input_X:  [tensor(0.1649), tensor(0.0615), 0, tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), 5, 1, 1, 1, 1, 1, 115, 0.06416884809732437]
mixing data with method:  random
arranging lora config with parameters:  115 0.06416884809732437 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 39,449,600 || all params: 8,069,710,848 || trainable%: 0.4889
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1649), tensor(0.0615), 0, tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1649)
number of datapoints needed (ratio * total):  824
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0615)
number of datapoints needed (ratio * total):  307
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0565)
number of datapoints needed (ratio * total):  282
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0807)
number of datapoints needed (ratio * total):  403
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1065)
number of datapoints needed (ratio * total):  532
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0967)
number of datapoints needed (ratio * total):  483
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3895)
number of datapoints needed (ratio * total):  1947
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 824
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 307
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 282
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 403
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 532
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 483
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1947
})]
length of training data:  4778
training model...
trainable params: 39,449,600 || all params: 8,069,710,848 || trainable%: 0.4889
{'loss': 2.4809, 'grad_norm': 0.5947350263595581, 'learning_rate': 0.0002948979591836734, 'epoch': 0.03}
{'loss': 2.067, 'grad_norm': 0.5155624151229858, 'learning_rate': 0.0002846938775510204, 'epoch': 0.07}
{'loss': 1.6902, 'grad_norm': 0.7751176357269287, 'learning_rate': 0.00027448979591836734, 'epoch': 0.1}
{'loss': 1.8766, 'grad_norm': 0.8465639352798462, 'learning_rate': 0.00026428571428571424, 'epoch': 0.13}
{'loss': 1.9965, 'grad_norm': 0.7950358390808105, 'learning_rate': 0.0002540816326530612, 'epoch': 0.17}
{'loss': 1.8145, 'grad_norm': 0.8357858061790466, 'learning_rate': 0.00024387755102040816, 'epoch': 0.2}
{'loss': 1.608, 'grad_norm': 0.7333483099937439, 'learning_rate': 0.00023367346938775506, 'epoch': 0.23}
{'loss': 1.7774, 'grad_norm': 0.2991645634174347, 'learning_rate': 0.00022346938775510205, 'epoch': 0.27}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5759, 'train_samples_per_second': 47.506, 'train_steps_per_second': 5.946, 'train_loss': 1.9055736610688359, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_59/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)]]
proposed candidate before processing: tensor([4.7455e-02, 2.0791e-01, 1.6346e-01, 2.4434e-03, 9.0004e-02, 3.3881e-21,
        1.9240e-01, 2.9632e-01, 1.2545e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.0923e-01, 8.3862e-02])
proposed candidate after normalizing: [0, tensor(0.2079), tensor(0.1635), 0, tensor(0.0900), 0, tensor(0.1924), tensor(0.2963), 4, 1, 1, 1, 1, 1, 104, 0.08386187255382538]
iteration:  60
input_X:  [0, tensor(0.2079), tensor(0.1635), 0, tensor(0.0900), 0, tensor(0.1924), tensor(0.2963), 4, 1, 1, 1, 1, 1, 104, 0.08386187255382538]
mixing data with method:  random
arranging lora config with parameters:  104 0.08386187255382538 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 36,430,848 || all params: 8,066,692,096 || trainable%: 0.4516
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.2079), tensor(0.1635), 0, tensor(0.0900), 0, tensor(0.1924), tensor(0.2963)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.2079)
number of datapoints needed (ratio * total):  1039
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1635)
number of datapoints needed (ratio * total):  817
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0900)
number of datapoints needed (ratio * total):  450
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1924)
number of datapoints needed (ratio * total):  962
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2963)
number of datapoints needed (ratio * total):  1481
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1039
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 817
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 450
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 962
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1481
})]
length of training data:  4749
training model...
trainable params: 36,430,848 || all params: 8,066,692,096 || trainable%: 0.4516
{'loss': 1.6976, 'grad_norm': 0.4906580150127411, 'learning_rate': 0.00029486301369863015, 'epoch': 0.03}
{'loss': 1.2668, 'grad_norm': 1.4497623443603516, 'learning_rate': 0.0002845890410958904, 'epoch': 0.07}
{'loss': 1.3462, 'grad_norm': 1.8210413455963135, 'learning_rate': 0.0002743150684931507, 'epoch': 0.1}
{'loss': 1.5099, 'grad_norm': 0.5987659096717834, 'learning_rate': 0.0002640410958904109, 'epoch': 0.13}
{'loss': 1.4019, 'grad_norm': 1.1031361818313599, 'learning_rate': 0.0002537671232876712, 'epoch': 0.17}
{'loss': 1.1879, 'grad_norm': 0.5926764607429504, 'learning_rate': 0.0002434931506849315, 'epoch': 0.2}
{'loss': 1.198, 'grad_norm': 0.5691083073616028, 'learning_rate': 0.00023321917808219177, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0217, 'train_samples_per_second': 47.48, 'train_steps_per_second': 5.939, 'train_loss': 1.395572752565951, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_60/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.69
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)]]
proposed candidate before processing: tensor([0.0630, 0.1441, 0.1103, 0.0000, 0.0635, 0.1769, 0.1730, 0.2691, 0.0739,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8014, 0.0967])
proposed candidate after normalizing: [tensor(0.0630), tensor(0.1441), tensor(0.1103), 0, tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), 2, 1, 1, 1, 1, 1, 103, 0.0966930091381073]
iteration:  61
input_X:  [tensor(0.0630), tensor(0.1441), tensor(0.1103), 0, tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), 2, 1, 1, 1, 1, 1, 103, 0.0966930091381073]
mixing data with method:  random
arranging lora config with parameters:  103 0.0966930091381073 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 28,403,712 || all params: 8,058,664,960 || trainable%: 0.3525
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0630), tensor(0.1441), tensor(0.1103), 0, tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0630)
number of datapoints needed (ratio * total):  314
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1441)
number of datapoints needed (ratio * total):  720
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1103)
number of datapoints needed (ratio * total):  551
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0635)
number of datapoints needed (ratio * total):  317
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1769)
number of datapoints needed (ratio * total):  884
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1730)
number of datapoints needed (ratio * total):  865
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2691)
number of datapoints needed (ratio * total):  1345
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 314
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 720
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 551
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 317
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 884
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 865
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1345
})]
length of training data:  4996
training model...
trainable params: 28,403,712 || all params: 8,058,664,960 || trainable%: 0.3525
{'loss': 1.8544, 'grad_norm': 0.9064117670059204, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5837, 'grad_norm': 2.019796371459961, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.521, 'grad_norm': 0.7343384027481079, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3047, 'grad_norm': 1.162052035331726, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3066, 'grad_norm': 1.618792176246643, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.4043, 'grad_norm': 1.627367377281189, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.5351, 'grad_norm': 0.4515582025051117, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.405, 'grad_norm': 0.822055995464325, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3954, 'train_samples_per_second': 49.763, 'train_steps_per_second': 6.225, 'train_loss': 1.4930046197050106, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_61/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)]]
proposed candidate before processing: tensor([0.0951, 0.2135, 0.2071, 0.0308, 0.0792, 0.0180, 0.0493, 0.3071, 0.1264,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8541, 0.0811])
proposed candidate after normalizing: [tensor(0.0951), tensor(0.2135), tensor(0.2071), 0, tensor(0.0792), 0, 0, tensor(0.3071), 4, 1, 1, 1, 1, 1, 109, 0.08107764273881912]
iteration:  62
input_X:  [tensor(0.0951), tensor(0.2135), tensor(0.2071), 0, tensor(0.0792), 0, 0, tensor(0.3071), 4, 1, 1, 1, 1, 1, 109, 0.08107764273881912]
mixing data with method:  random
arranging lora config with parameters:  109 0.08107764273881912 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 29,913,088 || all params: 8,060,174,336 || trainable%: 0.3711
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0951), tensor(0.2135), tensor(0.2071), 0, tensor(0.0792), 0, 0, tensor(0.3071)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0951)
number of datapoints needed (ratio * total):  475
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2135)
number of datapoints needed (ratio * total):  1067
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.2071)
number of datapoints needed (ratio * total):  1035
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0792)
number of datapoints needed (ratio * total):  396
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  wikitext
ratio:  tensor(0.3071)
number of datapoints needed (ratio * total):  1535
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 475
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1067
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1035
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 396
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1535
})]
length of training data:  4508
training model...
trainable params: 29,913,088 || all params: 8,060,174,336 || trainable%: 0.3711
{'loss': 2.3135, 'grad_norm': 0.4705665111541748, 'learning_rate': 0.0002945848375451263, 'epoch': 0.04}
{'loss': 1.6846, 'grad_norm': 0.6904418468475342, 'learning_rate': 0.000283754512635379, 'epoch': 0.07}
{'loss': 1.7524, 'grad_norm': 0.8353344202041626, 'learning_rate': 0.00027292418772563177, 'epoch': 0.11}
{'loss': 1.3535, 'grad_norm': 0.9968011379241943, 'learning_rate': 0.00026209386281588447, 'epoch': 0.14}
{'loss': 1.511, 'grad_norm': 0.3872128129005432, 'learning_rate': 0.00025126353790613716, 'epoch': 0.18}
{'loss': 1.2807, 'grad_norm': 1.1072216033935547, 'learning_rate': 0.00024043321299638986, 'epoch': 0.21}
{'loss': 1.4244, 'grad_norm': 0.43401551246643066, 'learning_rate': 0.00022960288808664258, 'epoch': 0.25}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4497, 'train_samples_per_second': 44.878, 'train_steps_per_second': 5.615, 'train_loss': 1.6201078958914314, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_62/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)]]
proposed candidate before processing: tensor([7.6490e-02, 8.2902e-02, 8.3565e-02, 5.4022e-02, 1.5220e-01, 1.4908e-18,
        2.3668e-01, 3.1414e-01, 1.2477e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.4608e-01, 8.6509e-02])
proposed candidate after normalizing: [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), 0, tensor(0.2367), tensor(0.3141), 4, 1, 1, 1, 1, 1, 108, 0.08650881797075272]
iteration:  63
input_X:  [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), 0, tensor(0.2367), tensor(0.3141), 4, 1, 1, 1, 1, 1, 108, 0.08650881797075272]
mixing data with method:  random
arranging lora config with parameters:  108 0.08650881797075272 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 29,638,656 || all params: 8,059,899,904 || trainable%: 0.3677
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), 0, tensor(0.2367), tensor(0.3141)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0765)
number of datapoints needed (ratio * total):  382
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0829)
number of datapoints needed (ratio * total):  414
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0836)
number of datapoints needed (ratio * total):  417
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0540)
number of datapoints needed (ratio * total):  270
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1522)
number of datapoints needed (ratio * total):  761
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2367)
number of datapoints needed (ratio * total):  1183
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3141)
number of datapoints needed (ratio * total):  1570
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 382
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 414
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 417
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 270
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 761
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1183
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1570
})]
length of training data:  4997
training model...
trainable params: 29,638,656 || all params: 8,059,899,904 || trainable%: 0.3677
{'loss': 2.5028, 'grad_norm': 1.1275649070739746, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.8804, 'grad_norm': 0.3944668471813202, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.6386, 'grad_norm': 1.472208857536316, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.4564, 'grad_norm': 0.8113958239555359, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3229, 'grad_norm': 1.3003891706466675, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.6639, 'grad_norm': 0.8131126165390015, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.4005, 'grad_norm': 0.5362800359725952, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.1228, 'grad_norm': 0.4214331805706024, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0919, 'train_samples_per_second': 49.924, 'train_steps_per_second': 6.244, 'train_loss': 1.6227756871117487, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_63/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.61
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)]]
proposed candidate before processing: tensor([1.2905e-01, 3.1753e-01, 1.0285e-01, 0.0000e+00, 2.8872e-06, 4.8332e-02,
        1.5934e-01, 2.4290e-01, 1.2612e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.7130e-01, 8.4603e-02])
proposed candidate after normalizing: [tensor(0.1290), tensor(0.3175), tensor(0.1028), 0, 0, 0, tensor(0.1593), tensor(0.2429), 4, 1, 1, 1, 1, 1, 99, 0.0846027061343193]
iteration:  64
input_X:  [tensor(0.1290), tensor(0.3175), tensor(0.1028), 0, 0, 0, tensor(0.1593), tensor(0.2429), 4, 1, 1, 1, 1, 1, 99, 0.0846027061343193]
mixing data with method:  random
arranging lora config with parameters:  99 0.0846027061343193 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 27,168,768 || all params: 8,057,430,016 || trainable%: 0.3372
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1290), tensor(0.3175), tensor(0.1028), 0, 0, 0, tensor(0.1593), tensor(0.2429)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1290)
number of datapoints needed (ratio * total):  645
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.3175)
number of datapoints needed (ratio * total):  1587
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1028)
number of datapoints needed (ratio * total):  514
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1593)
number of datapoints needed (ratio * total):  796
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2429)
number of datapoints needed (ratio * total):  1214
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 645
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1587
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 514
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 796
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1214
})]
length of training data:  4756
training model...
trainable params: 27,168,768 || all params: 8,057,430,016 || trainable%: 0.3372
{'loss': 2.0107, 'grad_norm': 1.2972888946533203, 'learning_rate': 0.00029487179487179484, 'epoch': 0.03}
{'loss': 1.4278, 'grad_norm': 0.7883468270301819, 'learning_rate': 0.00028461538461538457, 'epoch': 0.07}
{'loss': 1.2816, 'grad_norm': 1.013046383857727, 'learning_rate': 0.0002743589743589743, 'epoch': 0.1}
{'loss': 1.3465, 'grad_norm': 0.8551818132400513, 'learning_rate': 0.0002641025641025641, 'epoch': 0.13}
{'loss': 1.2168, 'grad_norm': 0.6138643026351929, 'learning_rate': 0.0002538461538461538, 'epoch': 0.17}
{'loss': 1.4638, 'grad_norm': 3.4920527935028076, 'learning_rate': 0.00024358974358974357, 'epoch': 0.2}
{'loss': 1.3148, 'grad_norm': 0.6135925054550171, 'learning_rate': 0.0002333333333333333, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1048, 'train_samples_per_second': 47.51, 'train_steps_per_second': 5.944, 'train_loss': 1.4501357956936485, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_64/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)]]
proposed candidate before processing: tensor([0.0298, 0.2238, 0.1589, 0.0258, 0.0684, 0.0966, 0.1126, 0.2842, 0.1279,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6840, 0.0836])
proposed candidate after normalizing: [0, tensor(0.2238), tensor(0.1589), 0, tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), 4, 1, 1, 1, 1, 1, 88, 0.08362815529108047]
iteration:  65
input_X:  [0, tensor(0.2238), tensor(0.1589), 0, tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), 4, 1, 1, 1, 1, 1, 88, 0.08362815529108047]
mixing data with method:  random
arranging lora config with parameters:  88 0.08362815529108047 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 24,150,016 || all params: 8,054,411,264 || trainable%: 0.2998
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.2238), tensor(0.1589), 0, tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.2238)
number of datapoints needed (ratio * total):  1118
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1589)
number of datapoints needed (ratio * total):  794
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0684)
number of datapoints needed (ratio * total):  342
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0966)
number of datapoints needed (ratio * total):  483
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1126)
number of datapoints needed (ratio * total):  563
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2842)
number of datapoints needed (ratio * total):  1420
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1118
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 794
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 342
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 483
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 563
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1420
})]
length of training data:  4720
training model...
trainable params: 24,150,016 || all params: 8,054,411,264 || trainable%: 0.2998
{'loss': 1.905, 'grad_norm': 1.0180174112319946, 'learning_rate': 0.0002948275862068965, 'epoch': 0.03}
{'loss': 1.8709, 'grad_norm': 0.6068999767303467, 'learning_rate': 0.0002844827586206896, 'epoch': 0.07}
{'loss': 1.5158, 'grad_norm': 0.6847462058067322, 'learning_rate': 0.0002741379310344827, 'epoch': 0.1}
{'loss': 1.3097, 'grad_norm': 0.7406962513923645, 'learning_rate': 0.00026379310344827584, 'epoch': 0.14}
{'loss': 1.4273, 'grad_norm': 1.7330527305603027, 'learning_rate': 0.00025344827586206895, 'epoch': 0.17}
{'loss': 1.533, 'grad_norm': 0.6451854109764099, 'learning_rate': 0.00024310344827586203, 'epoch': 0.2}
{'loss': 1.5635, 'grad_norm': 0.49743518233299255, 'learning_rate': 0.00023275862068965515, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.7322, 'train_samples_per_second': 46.857, 'train_steps_per_second': 5.857, 'train_loss': 1.5724432132388122, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_65/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)]]
proposed candidate before processing: tensor([1.8791e-01, 2.4350e-01, 1.3058e-01, 7.1155e-03, 3.5413e-17, 1.3674e-17,
        1.5885e-01, 2.7204e-01, 1.2534e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 9.6422e-01, 8.5218e-02])
proposed candidate after normalizing: [tensor(0.1879), tensor(0.2435), tensor(0.1306), 0, 0, 0, tensor(0.1588), tensor(0.2720), 4, 1, 1, 1, 1, 1, 123, 0.08521784842014313]
iteration:  66
input_X:  [tensor(0.1879), tensor(0.2435), tensor(0.1306), 0, 0, 0, tensor(0.1588), tensor(0.2720), 4, 1, 1, 1, 1, 1, 123, 0.08521784842014313]
mixing data with method:  random
arranging lora config with parameters:  123 0.08521784842014313 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 33,755,136 || all params: 8,064,016,384 || trainable%: 0.4186
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1879), tensor(0.2435), tensor(0.1306), 0, 0, 0, tensor(0.1588), tensor(0.2720)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1879)
number of datapoints needed (ratio * total):  939
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2435)
number of datapoints needed (ratio * total):  1217
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1306)
number of datapoints needed (ratio * total):  652
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1588)
number of datapoints needed (ratio * total):  794
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2720)
number of datapoints needed (ratio * total):  1360
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 939
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1217
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 652
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 794
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1360
})]
length of training data:  4962
training model...
trainable params: 33,755,136 || all params: 8,064,016,384 || trainable%: 0.4186
{'loss': 2.0778, 'grad_norm': 1.6292225122451782, 'learning_rate': 0.0002950900163666121, 'epoch': 0.03}
{'loss': 1.5332, 'grad_norm': 0.4249052107334137, 'learning_rate': 0.0002852700490998363, 'epoch': 0.06}
{'loss': 1.7682, 'grad_norm': 1.00424063205719, 'learning_rate': 0.00027545008183306056, 'epoch': 0.1}
{'loss': 1.316, 'grad_norm': 0.5835168957710266, 'learning_rate': 0.0002656301145662848, 'epoch': 0.13}
{'loss': 1.2425, 'grad_norm': 0.6125882267951965, 'learning_rate': 0.000255810147299509, 'epoch': 0.16}
{'loss': 1.2704, 'grad_norm': 0.5406882166862488, 'learning_rate': 0.0002459901800327332, 'epoch': 0.19}
{'loss': 1.2075, 'grad_norm': 0.2964548170566559, 'learning_rate': 0.00023617021276595742, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0338, 'train_samples_per_second': 49.603, 'train_steps_per_second': 6.208, 'train_loss': 1.4874885114904952, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_66/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.64
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)]]
proposed candidate before processing: tensor([1.4842e-01, 2.8265e-01, 6.7654e-02, 0.0000e+00, 1.4737e-01, 7.3980e-18,
        7.4587e-02, 2.7932e-01, 1.2550e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.2647e-01, 8.5651e-02])
proposed candidate after normalizing: [tensor(0.1484), tensor(0.2826), tensor(0.0677), 0, tensor(0.1474), 0, tensor(0.0746), tensor(0.2793), 4, 1, 1, 1, 1, 1, 106, 0.08565083146095276]
iteration:  67
input_X:  [tensor(0.1484), tensor(0.2826), tensor(0.0677), 0, tensor(0.1474), 0, tensor(0.0746), tensor(0.2793), 4, 1, 1, 1, 1, 1, 106, 0.08565083146095276]
mixing data with method:  random
arranging lora config with parameters:  106 0.08565083146095276 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 29,089,792 || all params: 8,059,351,040 || trainable%: 0.3609
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1484), tensor(0.2826), tensor(0.0677), 0, tensor(0.1474), 0, tensor(0.0746), tensor(0.2793)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1484)
number of datapoints needed (ratio * total):  742
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2826)
number of datapoints needed (ratio * total):  1413
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0677)
number of datapoints needed (ratio * total):  338
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1474)
number of datapoints needed (ratio * total):  736
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0746)
number of datapoints needed (ratio * total):  372
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2793)
number of datapoints needed (ratio * total):  1396
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 742
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1413
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 338
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 736
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 372
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1396
})]
length of training data:  4997
training model...
trainable params: 29,089,792 || all params: 8,059,351,040 || trainable%: 0.3609
{'loss': 2.4097, 'grad_norm': 0.5822862386703491, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.4832, 'grad_norm': 0.877570629119873, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.6556, 'grad_norm': 0.4908260703086853, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.4356, 'grad_norm': 0.37614190578460693, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.2497, 'grad_norm': 0.5066422820091248, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.4524, 'grad_norm': 1.5218480825424194, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.2787, 'grad_norm': 0.6330680251121521, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4699, 'train_samples_per_second': 49.736, 'train_steps_per_second': 6.221, 'train_loss': 1.5406394574627187, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_67/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.6
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)]]
proposed candidate before processing: tensor([6.6036e-02, 1.4493e-01, 2.0989e-01, 1.4471e-01, 1.1797e-17, 1.9732e-17,
        1.4956e-01, 2.8489e-01, 1.2593e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.3788e-01, 8.5945e-02])
proposed candidate after normalizing: [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), 0, 0, tensor(0.1496), tensor(0.2849), 4, 1, 1, 1, 1, 1, 107, 0.08594469726085663]
iteration:  68
input_X:  [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), 0, 0, tensor(0.1496), tensor(0.2849), 4, 1, 1, 1, 1, 1, 107, 0.08594469726085663]
mixing data with method:  random
arranging lora config with parameters:  107 0.08594469726085663 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 29,364,224 || all params: 8,059,625,472 || trainable%: 0.3643
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), 0, 0, tensor(0.1496), tensor(0.2849)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0660)
number of datapoints needed (ratio * total):  330
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1449)
number of datapoints needed (ratio * total):  724
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.2099)
number of datapoints needed (ratio * total):  1049
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1447)
number of datapoints needed (ratio * total):  723
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1496)
number of datapoints needed (ratio * total):  747
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2849)
number of datapoints needed (ratio * total):  1424
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 330
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 724
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1049
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 723
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 747
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1424
})]
length of training data:  4997
training model...
trainable params: 29,364,224 || all params: 8,059,625,472 || trainable%: 0.3643
{'loss': 2.3081, 'grad_norm': 2.2567341327667236, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.775, 'grad_norm': 0.8290594816207886, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.904, 'grad_norm': 0.6842867732048035, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3936, 'grad_norm': 0.5175418257713318, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.288, 'grad_norm': 1.2065467834472656, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.6348, 'grad_norm': 0.4812445342540741, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0304, 'train_samples_per_second': 49.955, 'train_steps_per_second': 6.248, 'train_loss': 1.6588793967267592, 'epoch': 0.22}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_68/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.6
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)]]
proposed candidate before processing: tensor([0.1085, 0.1622, 0.0489, 0.0584, 0.1512, 0.0572, 0.1625, 0.2511, 0.1479,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7765, 0.0603])
proposed candidate after normalizing: [tensor(0.1085), tensor(0.1622), 0, tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), 5, 1, 1, 1, 1, 1, 99, 0.06032257899641991]
iteration:  69
input_X:  [tensor(0.1085), tensor(0.1622), 0, tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), 5, 1, 1, 1, 1, 1, 99, 0.06032257899641991]
mixing data with method:  random
arranging lora config with parameters:  99 0.06032257899641991 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 33,960,960 || all params: 8,064,222,208 || trainable%: 0.4211
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1085), tensor(0.1622), 0, tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1085)
number of datapoints needed (ratio * total):  542
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1622)
number of datapoints needed (ratio * total):  811
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0584)
number of datapoints needed (ratio * total):  291
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1512)
number of datapoints needed (ratio * total):  756
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0572)
number of datapoints needed (ratio * total):  285
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1625)
number of datapoints needed (ratio * total):  812
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2511)
number of datapoints needed (ratio * total):  1255
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 542
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 811
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 291
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 756
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 285
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 812
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1255
})]
length of training data:  4752
training model...
trainable params: 33,960,960 || all params: 8,064,222,208 || trainable%: 0.4211
{'loss': 2.469, 'grad_norm': 1.835658311843872, 'learning_rate': 0.00029486301369863015, 'epoch': 0.03}
{'loss': 1.5349, 'grad_norm': 1.0361932516098022, 'learning_rate': 0.0002845890410958904, 'epoch': 0.07}
{'loss': 1.5326, 'grad_norm': 1.4060251712799072, 'learning_rate': 0.0002743150684931507, 'epoch': 0.1}
{'loss': 1.7867, 'grad_norm': 0.7153809666633606, 'learning_rate': 0.0002640410958904109, 'epoch': 0.13}
{'loss': 1.5201, 'grad_norm': 0.7648478746414185, 'learning_rate': 0.0002537671232876712, 'epoch': 0.17}
{'loss': 1.418, 'grad_norm': 0.8529577255249023, 'learning_rate': 0.0002434931506849315, 'epoch': 0.2}
{'loss': 1.3639, 'grad_norm': 0.6019429564476013, 'learning_rate': 0.00023321917808219177, 'epoch': 0.24}
{'loss': 1.2961, 'grad_norm': 0.8581856489181519, 'learning_rate': 0.00022294520547945203, 'epoch': 0.27}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.081, 'train_samples_per_second': 47.482, 'train_steps_per_second': 5.935, 'train_loss': 1.5938584861301242, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_69/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.52
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)]]
proposed candidate before processing: tensor([0.1801, 0.1437, 0.1682, 0.0000, 0.0000, 0.0479, 0.1068, 0.3534, 0.1227,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7777, 0.1000])
proposed candidate after normalizing: [tensor(0.1801), tensor(0.1437), tensor(0.1682), 0, 0, 0, tensor(0.1068), tensor(0.3534), 4, 1, 1, 1, 1, 1, 100, 0.10000000149011612]
iteration:  70
input_X:  [tensor(0.1801), tensor(0.1437), tensor(0.1682), 0, 0, 0, tensor(0.1068), tensor(0.3534), 4, 1, 1, 1, 1, 1, 100, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  100 0.10000000149011612 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 34,235,392 || all params: 8,064,496,640 || trainable%: 0.4245
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1801), tensor(0.1437), tensor(0.1682), 0, 0, 0, tensor(0.1068), tensor(0.3534)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1801)
number of datapoints needed (ratio * total):  900
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1437)
number of datapoints needed (ratio * total):  718
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1682)
number of datapoints needed (ratio * total):  840
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1068)
number of datapoints needed (ratio * total):  533
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3534)
number of datapoints needed (ratio * total):  1766
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 900
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 718
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 840
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 533
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1766
})]
length of training data:  4757
training model...
trainable params: 34,235,392 || all params: 8,064,496,640 || trainable%: 0.4245
{'loss': 2.0999, 'grad_norm': 1.893017292022705, 'learning_rate': 0.00029487179487179484, 'epoch': 0.03}
{'loss': 1.5327, 'grad_norm': 1.9036002159118652, 'learning_rate': 0.00028461538461538457, 'epoch': 0.07}
{'loss': 1.5232, 'grad_norm': 0.7853865027427673, 'learning_rate': 0.0002743589743589743, 'epoch': 0.1}
{'loss': 1.4855, 'grad_norm': 0.8335487842559814, 'learning_rate': 0.0002641025641025641, 'epoch': 0.13}
{'loss': 1.4863, 'grad_norm': 0.8177427649497986, 'learning_rate': 0.0002538461538461538, 'epoch': 0.17}
{'loss': 1.3926, 'grad_norm': 0.626400351524353, 'learning_rate': 0.00024358974358974357, 'epoch': 0.2}
{'loss': 1.5031, 'grad_norm': 0.5176599621772766, 'learning_rate': 0.0002333333333333333, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5496, 'train_samples_per_second': 47.31, 'train_steps_per_second': 5.917, 'train_loss': 1.5664598578648852, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_70/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)]]
proposed candidate before processing: tensor([0.0000e+00, 2.8512e-01, 1.2074e-01, 3.0785e-17, 8.4978e-03, 8.0799e-02,
        1.9139e-01, 3.1345e-01, 1.1932e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.7046e-01, 1.0000e-01])
proposed candidate after normalizing: [0, tensor(0.2851), tensor(0.1207), 0, 0, tensor(0.0808), tensor(0.1914), tensor(0.3135), 4, 1, 1, 1, 1, 1, 111, 0.10000000149011612]
iteration:  71
input_X:  [0, tensor(0.2851), tensor(0.1207), 0, 0, tensor(0.0808), tensor(0.1914), tensor(0.3135), 4, 1, 1, 1, 1, 1, 111, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  111 0.10000000149011612 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 30,461,952 || all params: 8,060,723,200 || trainable%: 0.3779
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.2851), tensor(0.1207), 0, 0, tensor(0.0808), tensor(0.1914), tensor(0.3135)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.2851)
number of datapoints needed (ratio * total):  1425
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1207)
number of datapoints needed (ratio * total):  603
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  tensor(0.0808)
number of datapoints needed (ratio * total):  403
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1914)
number of datapoints needed (ratio * total):  956
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3135)
number of datapoints needed (ratio * total):  1567
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1425
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 603
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 403
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 956
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1567
})]
length of training data:  4954
training model...
trainable params: 30,461,952 || all params: 8,060,723,200 || trainable%: 0.3779
{'loss': 2.0773, 'grad_norm': 1.204409122467041, 'learning_rate': 0.0002950819672131147, 'epoch': 0.03}
{'loss': 1.8809, 'grad_norm': 0.4098264276981354, 'learning_rate': 0.00028524590163934424, 'epoch': 0.06}
{'loss': 1.3445, 'grad_norm': 0.45131343603134155, 'learning_rate': 0.00027540983606557377, 'epoch': 0.1}
{'loss': 1.3473, 'grad_norm': 1.0297755002975464, 'learning_rate': 0.00026557377049180324, 'epoch': 0.13}
{'loss': 1.48, 'grad_norm': 0.5862011909484863, 'learning_rate': 0.00025573770491803277, 'epoch': 0.16}
{'loss': 1.4952, 'grad_norm': 0.7282005548477173, 'learning_rate': 0.0002459016393442623, 'epoch': 0.19}
{'loss': 1.1757, 'grad_norm': 0.39825743436813354, 'learning_rate': 0.00023606557377049177, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3003, 'train_samples_per_second': 49.392, 'train_steps_per_second': 6.181, 'train_loss': 1.5339170961963886, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_71/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.61
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)]]
proposed candidate before processing: tensor([3.3282e-17, 1.4940e-01, 1.2074e-01, 6.0346e-02, 1.3821e-01, 1.3130e-01,
        8.1373e-02, 3.1863e-01, 7.3647e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 9.3819e-01, 1.0000e-01])
proposed candidate after normalizing: [0, tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), 2, 1, 1, 1, 1, 1, 120, 0.10000000149011612]
iteration:  72
input_X:  [0, tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), 2, 1, 1, 1, 1, 1, 120, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  120 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 31,696,896 || all params: 8,061,958,144 || trainable%: 0.3932
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.1494)
number of datapoints needed (ratio * total):  746
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1207)
number of datapoints needed (ratio * total):  603
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0603)
number of datapoints needed (ratio * total):  301
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1382)
number of datapoints needed (ratio * total):  691
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1313)
number of datapoints needed (ratio * total):  656
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0814)
number of datapoints needed (ratio * total):  406
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3186)
number of datapoints needed (ratio * total):  1593
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 746
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 603
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 301
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 691
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 656
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 406
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1593
})]
length of training data:  4996
training model...
trainable params: 31,696,896 || all params: 8,061,958,144 || trainable%: 0.3932
{'loss': 1.7042, 'grad_norm': 1.504973292350769, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.4695, 'grad_norm': 0.8034474849700928, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.4065, 'grad_norm': 0.5592078566551208, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.5713, 'grad_norm': 0.7146298289299011, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.2883, 'grad_norm': 1.7489582300186157, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.6801, 'grad_norm': 0.4055986702442169, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.5912, 'grad_norm': 0.6200191378593445, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3239, 'train_samples_per_second': 49.799, 'train_steps_per_second': 6.23, 'train_loss': 1.5118847666559994, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_72/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)]]
proposed candidate before processing: tensor([1.1308e-01, 2.1236e-01, 1.4948e-01, 1.6910e-17, 1.1395e-01, 1.2134e-01,
        1.6706e-02, 2.7309e-01, 7.2481e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.9892e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1131), tensor(0.2124), tensor(0.1495), 0, tensor(0.1139), tensor(0.1213), 0, tensor(0.2731), 2, 1, 1, 1, 1, 1, 102, 0.10000000149011612]
iteration:  73
input_X:  [tensor(0.1131), tensor(0.2124), tensor(0.1495), 0, tensor(0.1139), tensor(0.1213), 0, tensor(0.2731), 2, 1, 1, 1, 1, 1, 102, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  102 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 13,996,032 || all params: 8,044,257,280 || trainable%: 0.1740
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1131), tensor(0.2124), tensor(0.1495), 0, tensor(0.1139), tensor(0.1213), 0, tensor(0.2731)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1131)
number of datapoints needed (ratio * total):  565
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2124)
number of datapoints needed (ratio * total):  1061
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1495)
number of datapoints needed (ratio * total):  747
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1139)
number of datapoints needed (ratio * total):  569
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1213)
number of datapoints needed (ratio * total):  606
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  wikitext
ratio:  tensor(0.2731)
number of datapoints needed (ratio * total):  1365
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 565
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1061
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 747
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 569
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 606
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1365
})]
length of training data:  4913
training model...
trainable params: 13,996,032 || all params: 8,044,257,280 || trainable%: 0.1740
{'loss': 2.0179, 'grad_norm': 0.6535540223121643, 'learning_rate': 0.0002950413223140496, 'epoch': 0.03}
{'loss': 1.3919, 'grad_norm': 1.1412941217422485, 'learning_rate': 0.0002851239669421488, 'epoch': 0.07}
{'loss': 1.7731, 'grad_norm': 0.4732494652271271, 'learning_rate': 0.0002752066115702479, 'epoch': 0.1}
{'loss': 1.5686, 'grad_norm': 0.7397388219833374, 'learning_rate': 0.0002652892561983471, 'epoch': 0.13}
{'loss': 1.3136, 'grad_norm': 0.8540211915969849, 'learning_rate': 0.00025537190082644627, 'epoch': 0.16}
{'loss': 1.3326, 'grad_norm': 1.5621470212936401, 'learning_rate': 0.00024545454545454545, 'epoch': 0.2}
{'loss': 1.3064, 'grad_norm': 0.6835329532623291, 'learning_rate': 0.00023553719008264463, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2979, 'train_samples_per_second': 48.984, 'train_steps_per_second': 6.132, 'train_loss': 1.5055236073283407, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_73/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.64
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)]]
proposed candidate before processing: tensor([5.8953e-02, 1.5661e-01, 1.6984e-01, 1.0236e-18, 6.3307e-02, 4.1242e-02,
        2.3455e-01, 2.7550e-01, 7.4903e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 9.5996e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.0590), tensor(0.1566), tensor(0.1698), 0, tensor(0.0633), 0, tensor(0.2346), tensor(0.2755), 2, 1, 1, 1, 1, 1, 123, 0.10000000149011612]
iteration:  74
input_X:  [tensor(0.0590), tensor(0.1566), tensor(0.1698), 0, tensor(0.0633), 0, tensor(0.2346), tensor(0.2755), 2, 1, 1, 1, 1, 1, 123, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  123 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 16,877,568 || all params: 8,047,138,816 || trainable%: 0.2097
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0590), tensor(0.1566), tensor(0.1698), 0, tensor(0.0633), 0, tensor(0.2346), tensor(0.2755)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0590)
number of datapoints needed (ratio * total):  294
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1566)
number of datapoints needed (ratio * total):  783
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1698)
number of datapoints needed (ratio * total):  849
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0633)
number of datapoints needed (ratio * total):  316
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2346)
number of datapoints needed (ratio * total):  1172
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2755)
number of datapoints needed (ratio * total):  1377
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 294
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 783
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 849
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 316
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1172
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1377
})]
length of training data:  4791
training model...
trainable params: 16,877,568 || all params: 8,047,138,816 || trainable%: 0.2097
{'loss': 2.3294, 'grad_norm': 5.280125617980957, 'learning_rate': 0.0002949066213921901, 'epoch': 0.03}
{'loss': 1.8104, 'grad_norm': 0.5409235954284668, 'learning_rate': 0.00028471986417657043, 'epoch': 0.07}
{'loss': 1.8202, 'grad_norm': 0.72833251953125, 'learning_rate': 0.0002745331069609507, 'epoch': 0.1}
{'loss': 1.3032, 'grad_norm': 0.6587196588516235, 'learning_rate': 0.00026434634974533105, 'epoch': 0.13}
{'loss': 1.5749, 'grad_norm': 0.8324498534202576, 'learning_rate': 0.00025415959252971134, 'epoch': 0.17}
{'loss': 1.1292, 'grad_norm': 0.9419264793395996, 'learning_rate': 0.00024397283531409167, 'epoch': 0.2}
{'loss': 1.3702, 'grad_norm': 0.6861392259597778, 'learning_rate': 0.00023378607809847199, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2046, 'train_samples_per_second': 47.812, 'train_steps_per_second': 5.978, 'train_loss': 1.602120238465148, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_74/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)]]
proposed candidate before processing: tensor([0.0124, 0.1169, 0.1679, 0.1010, 0.0257, 0.0350, 0.2175, 0.3237, 0.0739,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8708, 0.1000])
proposed candidate after normalizing: [0, tensor(0.1169), tensor(0.1679), tensor(0.1010), 0, 0, tensor(0.2175), tensor(0.3237), 2, 1, 1, 1, 1, 1, 111, 0.10000000149011612]
iteration:  75
input_X:  [0, tensor(0.1169), tensor(0.1679), tensor(0.1010), 0, 0, tensor(0.2175), tensor(0.3237), 2, 1, 1, 1, 1, 1, 111, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  111 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 15,230,976 || all params: 8,045,492,224 || trainable%: 0.1893
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.1169), tensor(0.1679), tensor(0.1010), 0, 0, tensor(0.2175), tensor(0.3237)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.1169)
number of datapoints needed (ratio * total):  584
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1679)
number of datapoints needed (ratio * total):  839
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1010)
number of datapoints needed (ratio * total):  504
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2175)
number of datapoints needed (ratio * total):  1087
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3237)
number of datapoints needed (ratio * total):  1618
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 584
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 839
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 504
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1087
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1618
})]
length of training data:  4632
training model...
trainable params: 15,230,976 || all params: 8,045,492,224 || trainable%: 0.1893
{'loss': 2.3653, 'grad_norm': 1.8072285652160645, 'learning_rate': 0.0002947275922671353, 'epoch': 0.03}
{'loss': 1.9701, 'grad_norm': 2.024611234664917, 'learning_rate': 0.000284182776801406, 'epoch': 0.07}
{'loss': 1.8636, 'grad_norm': 0.46239402890205383, 'learning_rate': 0.0002736379613356766, 'epoch': 0.1}
{'loss': 1.7408, 'grad_norm': 1.535988211631775, 'learning_rate': 0.00026309314586994725, 'epoch': 0.14}
{'loss': 1.4423, 'grad_norm': 0.5222582221031189, 'learning_rate': 0.00025254833040421794, 'epoch': 0.17}
{'loss': 1.6734, 'grad_norm': 0.5383803844451904, 'learning_rate': 0.00024200351493848854, 'epoch': 0.21}
{'loss': 1.5487, 'grad_norm': 0.7724292874336243, 'learning_rate': 0.0002314586994727592, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0754, 'train_samples_per_second': 46.285, 'train_steps_per_second': 5.786, 'train_loss': 1.7688320127026789, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_75/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.68
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)]]
proposed candidate before processing: tensor([0.2385, 0.0945, 0.1793, 0.0758, 0.0276, 0.0988, 0.0819, 0.2036, 0.1481,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9854, 0.0615])
proposed candidate after normalizing: [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), 0, tensor(0.0988), tensor(0.0819), tensor(0.2036), 5, 1, 1, 1, 1, 1, 126, 0.061545174568891525]
iteration:  76
input_X:  [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), 0, tensor(0.0988), tensor(0.0819), tensor(0.2036), 5, 1, 1, 1, 1, 1, 126, 0.061545174568891525]
mixing data with method:  random
arranging lora config with parameters:  126 0.061545174568891525 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 43,223,040 || all params: 8,073,484,288 || trainable%: 0.5354
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), 0, tensor(0.0988), tensor(0.0819), tensor(0.2036)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.2385)
number of datapoints needed (ratio * total):  1192
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0945)
number of datapoints needed (ratio * total):  472
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1793)
number of datapoints needed (ratio * total):  896
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0758)
number of datapoints needed (ratio * total):  378
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  tensor(0.0988)
number of datapoints needed (ratio * total):  493
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0819)
number of datapoints needed (ratio * total):  409
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2036)
number of datapoints needed (ratio * total):  1018
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1192
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 472
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 896
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 378
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 493
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 409
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1018
})]
length of training data:  4858
training model...
trainable params: 43,223,040 || all params: 8,073,484,288 || trainable%: 0.5354
{'loss': 2.0804, 'grad_norm': 2.129714012145996, 'learning_rate': 0.00029498327759197324, 'epoch': 0.03}
{'loss': 1.4045, 'grad_norm': 1.50798499584198, 'learning_rate': 0.00028494983277591973, 'epoch': 0.07}
{'loss': 1.2938, 'grad_norm': 0.6226224899291992, 'learning_rate': 0.00027491638795986616, 'epoch': 0.1}
{'loss': 1.3353, 'grad_norm': 0.6110203266143799, 'learning_rate': 0.0002648829431438127, 'epoch': 0.13}
{'loss': 1.3137, 'grad_norm': 0.6003245711326599, 'learning_rate': 0.0002548494983277592, 'epoch': 0.16}
{'loss': 1.4642, 'grad_norm': 0.9480282664299011, 'learning_rate': 0.00024481605351170567, 'epoch': 0.2}
{'loss': 1.3153, 'grad_norm': 0.9198246002197266, 'learning_rate': 0.00023478260869565215, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1132, 'train_samples_per_second': 48.525, 'train_steps_per_second': 6.073, 'train_loss': 1.4545691917682517, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_76/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.53
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)]]
proposed candidate before processing: tensor([0.0000, 0.0655, 0.1654, 0.0271, 0.0488, 0.0518, 0.2401, 0.4013, 0.0745,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8937, 0.1000])
proposed candidate after normalizing: [0, tensor(0.0655), tensor(0.1654), 0, 0, tensor(0.0518), tensor(0.2401), tensor(0.4013), 2, 1, 1, 1, 1, 1, 114, 0.10000000149011612]
iteration:  77
input_X:  [0, tensor(0.0655), tensor(0.1654), 0, 0, tensor(0.0518), tensor(0.2401), tensor(0.4013), 2, 1, 1, 1, 1, 1, 114, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  114 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 41,576,448 || all params: 8,071,837,696 || trainable%: 0.5151
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.0655), tensor(0.1654), 0, 0, tensor(0.0518), tensor(0.2401), tensor(0.4013)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.0655)
number of datapoints needed (ratio * total):  327
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1654)
number of datapoints needed (ratio * total):  826
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  tensor(0.0518)
number of datapoints needed (ratio * total):  258
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2401)
number of datapoints needed (ratio * total):  1200
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.4013)
number of datapoints needed (ratio * total):  2006
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 327
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 826
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 258
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1200
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 2006
})]
length of training data:  4617
training model...
trainable params: 41,576,448 || all params: 8,071,837,696 || trainable%: 0.5151
{'loss': 1.7998, 'grad_norm': 0.42830511927604675, 'learning_rate': 0.0002947183098591549, 'epoch': 0.03}
{'loss': 1.69, 'grad_norm': 0.5661044120788574, 'learning_rate': 0.00028415492957746476, 'epoch': 0.07}
{'loss': 1.6236, 'grad_norm': 0.4495142102241516, 'learning_rate': 0.0002735915492957746, 'epoch': 0.1}
{'loss': 1.3387, 'grad_norm': 0.43376854062080383, 'learning_rate': 0.0002630281690140845, 'epoch': 0.14}
{'loss': 1.6129, 'grad_norm': 0.8787857294082642, 'learning_rate': 0.0002524647887323943, 'epoch': 0.17}
{'loss': 1.5759, 'grad_norm': 0.3325386345386505, 'learning_rate': 0.0002419014084507042, 'epoch': 0.21}
{'loss': 1.6822, 'grad_norm': 0.739383339881897, 'learning_rate': 0.00023133802816901406, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3496, 'train_samples_per_second': 46.009, 'train_steps_per_second': 5.76, 'train_loss': 1.6211105030813036, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_77/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)]]
proposed candidate before processing: tensor([0.0306, 0.2087, 0.1890, 0.1272, 0.0149, 0.0506, 0.1637, 0.2152, 0.0738,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9192, 0.1000])
proposed candidate after normalizing: [0, tensor(0.2087), tensor(0.1890), tensor(0.1272), 0, tensor(0.0506), tensor(0.1637), tensor(0.2152), 2, 1, 1, 1, 1, 1, 118, 0.10000000149011612]
iteration:  78
input_X:  [0, tensor(0.2087), tensor(0.1890), tensor(0.1272), 0, tensor(0.0506), tensor(0.1637), tensor(0.2152), 2, 1, 1, 1, 1, 1, 118, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  118 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 16,191,488 || all params: 8,046,452,736 || trainable%: 0.2012
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.2087), tensor(0.1890), tensor(0.1272), 0, tensor(0.0506), tensor(0.1637), tensor(0.2152)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.2087)
number of datapoints needed (ratio * total):  1043
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1890)
number of datapoints needed (ratio * total):  945
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1272)
number of datapoints needed (ratio * total):  636
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  tensor(0.0506)
number of datapoints needed (ratio * total):  252
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1637)
number of datapoints needed (ratio * total):  818
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2152)
number of datapoints needed (ratio * total):  1076
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1043
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 945
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 636
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 252
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 818
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1076
})]
length of training data:  4770
training model...
trainable params: 16,191,488 || all params: 8,046,452,736 || trainable%: 0.2012
{'loss': 1.8864, 'grad_norm': 0.8303747177124023, 'learning_rate': 0.00029488926746166946, 'epoch': 0.03}
{'loss': 1.3855, 'grad_norm': 0.9023893475532532, 'learning_rate': 0.0002846678023850085, 'epoch': 0.07}
{'loss': 1.5376, 'grad_norm': 0.558512806892395, 'learning_rate': 0.0002744463373083475, 'epoch': 0.1}
{'loss': 1.5243, 'grad_norm': 1.8705837726593018, 'learning_rate': 0.00026422487223168653, 'epoch': 0.13}
{'loss': 1.6832, 'grad_norm': 0.4114299714565277, 'learning_rate': 0.0002540034071550255, 'epoch': 0.17}
{'loss': 1.102, 'grad_norm': 0.8576204180717468, 'learning_rate': 0.00024378194207836453, 'epoch': 0.2}
{'loss': 1.2649, 'grad_norm': 0.8488408923149109, 'learning_rate': 0.00023356047700170358, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5671, 'train_samples_per_second': 47.431, 'train_steps_per_second': 5.936, 'train_loss': 1.4674028762399334, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_78/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.6
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1440, 0.1691, 0.1376, 0.0885, 0.0724, 0.0000, 0.0462, 0.3421, 0.0354,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8837, 0.1000])
proposed candidate after normalizing: [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), 0, 0, tensor(0.3421), 1, 1, 1, 1, 1, 1, 113, 0.10000000149011612]
iteration:  79
input_X:  [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), 0, 0, tensor(0.3421), 1, 1, 1, 1, 1, 1, 113, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  113 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 15,848,448 || all params: 8,046,109,696 || trainable%: 0.1970
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), 0, 0, tensor(0.3421)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1440)
number of datapoints needed (ratio * total):  720
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1691)
number of datapoints needed (ratio * total):  845
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1376)
number of datapoints needed (ratio * total):  688
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0885)
number of datapoints needed (ratio * total):  442
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0724)
number of datapoints needed (ratio * total):  362
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  wikitext
ratio:  tensor(0.3421)
number of datapoints needed (ratio * total):  1710
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 720
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 845
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 688
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 442
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 362
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1710
})]
length of training data:  4767
training model...
trainable params: 15,848,448 || all params: 8,046,109,696 || trainable%: 0.1970
{'loss': 1.5547, 'grad_norm': 0.8094722628593445, 'learning_rate': 0.0002948805460750853, 'epoch': 0.03}
{'loss': 1.6849, 'grad_norm': 0.9715394377708435, 'learning_rate': 0.00028464163822525593, 'epoch': 0.07}
{'loss': 1.7143, 'grad_norm': 1.181819200515747, 'learning_rate': 0.00027440273037542657, 'epoch': 0.1}
{'loss': 1.6186, 'grad_norm': 1.177826166152954, 'learning_rate': 0.00026416382252559726, 'epoch': 0.13}
{'loss': 1.3801, 'grad_norm': 1.172174096107483, 'learning_rate': 0.0002539249146757679, 'epoch': 0.17}
{'loss': 1.3163, 'grad_norm': 0.3587355613708496, 'learning_rate': 0.00024368600682593853, 'epoch': 0.2}
{'loss': 1.5721, 'grad_norm': 1.2915080785751343, 'learning_rate': 0.0002334470989761092, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3886, 'train_samples_per_second': 47.485, 'train_steps_per_second': 5.937, 'train_loss': 1.5355331036779616, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_79/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1407, 0.1754, 0.1248, 0.0872, 0.1649, 0.0000, 0.1242, 0.1827, 0.0359,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8333, 0.1000])
proposed candidate after normalizing: [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), 0, tensor(0.1242), tensor(0.1827), 1, 1, 1, 1, 1, 1, 107, 0.10000000149011612]
iteration:  80
input_X:  [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), 0, tensor(0.1242), tensor(0.1827), 1, 1, 1, 1, 1, 1, 107, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  107 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,341,056 || all params: 8,037,602,304 || trainable%: 0.0913
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), 0, tensor(0.1242), tensor(0.1827)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1407)
number of datapoints needed (ratio * total):  703
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1754)
number of datapoints needed (ratio * total):  877
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1248)
number of datapoints needed (ratio * total):  623
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0872)
number of datapoints needed (ratio * total):  436
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1649)
number of datapoints needed (ratio * total):  824
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1242)
number of datapoints needed (ratio * total):  621
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1827)
number of datapoints needed (ratio * total):  913
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 703
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 877
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 623
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 436
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 824
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 621
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 913
})]
length of training data:  4997
training model...
trainable params: 7,341,056 || all params: 8,037,602,304 || trainable%: 0.0913
{'loss': 2.4189, 'grad_norm': 1.6571683883666992, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5464, 'grad_norm': 1.6557894945144653, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.8491, 'grad_norm': 1.1945079565048218, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.5754, 'grad_norm': 1.5135324001312256, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.2155, 'grad_norm': 1.3810352087020874, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.3733, 'grad_norm': 0.6167991161346436, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.2097, 'grad_norm': 1.2025429010391235, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.228, 'train_samples_per_second': 49.856, 'train_steps_per_second': 6.236, 'train_loss': 1.5809778522800755, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_80/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.69
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1268, 0.1333, 0.1387, 0.0450, 0.1637, 0.0000, 0.1664, 0.2260, 0.0363,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7400, 0.1000])
proposed candidate after normalizing: [tensor(0.1268), tensor(0.1333), tensor(0.1387), 0, tensor(0.1637), 0, tensor(0.1664), tensor(0.2260), 1, 1, 1, 1, 1, 1, 95, 0.10000000149011612]
iteration:  81
input_X:  [tensor(0.1268), tensor(0.1333), tensor(0.1387), 0, tensor(0.1637), 0, tensor(0.1664), tensor(0.2260), 1, 1, 1, 1, 1, 1, 95, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  95 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 6,517,760 || all params: 8,036,779,008 || trainable%: 0.0811
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1268), tensor(0.1333), tensor(0.1387), 0, tensor(0.1637), 0, tensor(0.1664), tensor(0.2260)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1268)
number of datapoints needed (ratio * total):  634
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1333)
number of datapoints needed (ratio * total):  666
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1387)
number of datapoints needed (ratio * total):  693
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1637)
number of datapoints needed (ratio * total):  818
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1664)
number of datapoints needed (ratio * total):  832
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2260)
number of datapoints needed (ratio * total):  1129
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 634
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 666
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 693
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 818
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 832
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1129
})]
length of training data:  4772
training model...
trainable params: 6,517,760 || all params: 8,036,779,008 || trainable%: 0.0811
{'loss': 2.7228, 'grad_norm': 0.6026632189750671, 'learning_rate': 0.00029488926746166946, 'epoch': 0.03}
{'loss': 1.7469, 'grad_norm': 0.4708877503871918, 'learning_rate': 0.0002846678023850085, 'epoch': 0.07}
{'loss': 1.4722, 'grad_norm': 0.7605857253074646, 'learning_rate': 0.0002744463373083475, 'epoch': 0.1}
{'loss': 1.4384, 'grad_norm': 1.8071147203445435, 'learning_rate': 0.00026422487223168653, 'epoch': 0.13}
{'loss': 1.4238, 'grad_norm': 0.9704182147979736, 'learning_rate': 0.0002540034071550255, 'epoch': 0.17}
{'loss': 1.528, 'grad_norm': 1.090644359588623, 'learning_rate': 0.00024378194207836453, 'epoch': 0.2}
{'loss': 1.4909, 'grad_norm': 0.5371227264404297, 'learning_rate': 0.00023356047700170358, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1553, 'train_samples_per_second': 47.646, 'train_steps_per_second': 5.961, 'train_loss': 1.6646436217484202, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_81/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)]]
proposed candidate before processing: tensor([1.6488e-01, 2.1853e-01, 1.0392e-01, 1.2060e-01, 1.6328e-01, 8.2407e-04,
        8.0619e-02, 1.4736e-01, 3.5857e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 9.7267e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), 0, tensor(0.0806), tensor(0.1474), 1, 1, 1, 1, 1, 1, 125, 0.10000000149011612]
iteration:  82
input_X:  [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), 0, tensor(0.0806), tensor(0.1474), 1, 1, 1, 1, 1, 1, 125, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  125 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 8,576,000 || all params: 8,038,837,248 || trainable%: 0.1067
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), 0, tensor(0.0806), tensor(0.1474)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1649)
number of datapoints needed (ratio * total):  824
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2185)
number of datapoints needed (ratio * total):  1092
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1039)
number of datapoints needed (ratio * total):  519
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1206)
number of datapoints needed (ratio * total):  602
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1633)
number of datapoints needed (ratio * total):  816
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0806)
number of datapoints needed (ratio * total):  403
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1474)
number of datapoints needed (ratio * total):  736
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 824
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1092
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 519
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 602
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 816
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 403
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 736
})]
length of training data:  4992
training model...
trainable params: 8,576,000 || all params: 8,038,837,248 || trainable%: 0.1067
{'loss': 2.3098, 'grad_norm': 0.4715820550918579, 'learning_rate': 0.000295114006514658, 'epoch': 0.03}
{'loss': 1.5236, 'grad_norm': 0.8346819281578064, 'learning_rate': 0.0002853420195439739, 'epoch': 0.06}
{'loss': 1.4642, 'grad_norm': 0.5913767218589783, 'learning_rate': 0.00027557003257328987, 'epoch': 0.1}
{'loss': 1.2339, 'grad_norm': 1.0043957233428955, 'learning_rate': 0.0002657980456026058, 'epoch': 0.13}
{'loss': 1.5387, 'grad_norm': 0.3169649541378021, 'learning_rate': 0.0002560260586319218, 'epoch': 0.16}
{'loss': 1.0939, 'grad_norm': 0.41445350646972656, 'learning_rate': 0.00024625407166123777, 'epoch': 0.19}
{'loss': 1.2573, 'grad_norm': 0.25439971685409546, 'learning_rate': 0.00023648208469055374, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1707, 'train_samples_per_second': 49.835, 'train_steps_per_second': 6.229, 'train_loss': 1.5010550518163899, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_82/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1229, 0.1835, 0.1172, 0.1359, 0.0986, 0.1279, 0.0671, 0.1468, 0.0360,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8733, 0.1000])
proposed candidate after normalizing: [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), 1, 1, 1, 1, 1, 1, 112, 0.10000000149011612]
iteration:  83
input_X:  [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), 1, 1, 1, 1, 1, 1, 112, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  112 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,684,096 || all params: 8,037,945,344 || trainable%: 0.0956
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1229)
number of datapoints needed (ratio * total):  614
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1835)
number of datapoints needed (ratio * total):  917
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1172)
number of datapoints needed (ratio * total):  586
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1359)
number of datapoints needed (ratio * total):  679
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0986)
number of datapoints needed (ratio * total):  493
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1279)
number of datapoints needed (ratio * total):  639
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0671)
number of datapoints needed (ratio * total):  335
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1468)
number of datapoints needed (ratio * total):  734
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 614
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 917
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 586
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 679
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 493
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 639
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 335
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 734
})]
length of training data:  4997
training model...
trainable params: 7,684,096 || all params: 8,037,945,344 || trainable%: 0.0956
{'loss': 2.2655, 'grad_norm': 2.6743991374969482, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.446, 'grad_norm': 0.6651303172111511, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.4606, 'grad_norm': 0.8327223658561707, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3041, 'grad_norm': 1.748521089553833, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.1556, 'grad_norm': 0.5312691926956177, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.5251, 'grad_norm': 0.3877340853214264, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.0775, 'grad_norm': 1.146972417831421, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4159, 'train_samples_per_second': 49.763, 'train_steps_per_second': 6.224, 'train_loss': 1.4530974537336907, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_83/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)]]
proposed candidate before processing: tensor([1.0482e-01, 2.5415e-01, 1.3992e-01, 1.3913e-03, 1.9585e-01, 8.9785e-18,
        6.5580e-02, 2.3828e-01, 3.5675e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 9.2707e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1048), tensor(0.2542), tensor(0.1399), 0, tensor(0.1959), 0, tensor(0.0656), tensor(0.2383), 1, 1, 1, 1, 1, 1, 119, 0.10000000149011612]
iteration:  84
input_X:  [tensor(0.1048), tensor(0.2542), tensor(0.1399), 0, tensor(0.1959), 0, tensor(0.0656), tensor(0.2383), 1, 1, 1, 1, 1, 1, 119, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  119 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 8,164,352 || all params: 8,038,425,600 || trainable%: 0.1016
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1048), tensor(0.2542), tensor(0.1399), 0, tensor(0.1959), 0, tensor(0.0656), tensor(0.2383)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1048)
number of datapoints needed (ratio * total):  524
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2542)
number of datapoints needed (ratio * total):  1270
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1399)
number of datapoints needed (ratio * total):  699
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1959)
number of datapoints needed (ratio * total):  979
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0656)
number of datapoints needed (ratio * total):  327
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2383)
number of datapoints needed (ratio * total):  1191
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 524
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1270
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 699
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 979
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 327
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1191
})]
length of training data:  4990
training model...
trainable params: 8,164,352 || all params: 8,038,425,600 || trainable%: 0.1016
{'loss': 2.1803, 'grad_norm': 1.3677116632461548, 'learning_rate': 0.000295114006514658, 'epoch': 0.03}
{'loss': 1.5575, 'grad_norm': 0.7886404395103455, 'learning_rate': 0.0002853420195439739, 'epoch': 0.06}
{'loss': 1.3821, 'grad_norm': 0.4558742642402649, 'learning_rate': 0.00027557003257328987, 'epoch': 0.1}
{'loss': 1.283, 'grad_norm': 0.9364035129547119, 'learning_rate': 0.0002657980456026058, 'epoch': 0.13}
{'loss': 1.269, 'grad_norm': 0.7280853986740112, 'learning_rate': 0.0002560260586319218, 'epoch': 0.16}
{'loss': 1.3919, 'grad_norm': 0.6095680594444275, 'learning_rate': 0.00024625407166123777, 'epoch': 0.19}
{'loss': 1.0195, 'grad_norm': 0.4639376699924469, 'learning_rate': 0.00023648208469055374, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1817, 'train_samples_per_second': 49.809, 'train_steps_per_second': 6.229, 'train_loss': 1.431344727546938, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_84/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)], [tensor(0.1048), tensor(0.2542), tensor(0.1399), tensor(0.0014), tensor(0.1959), tensor(8.9785e-18), tensor(0.0656), tensor(0.2383), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9271), tensor(0.1000)]]
proposed candidate before processing: tensor([7.4998e-02, 1.3593e-01, 9.7280e-02, 7.2963e-18, 1.5082e-01, 0.0000e+00,
        1.9664e-01, 3.4433e-01, 7.3464e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.8675e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.0750), tensor(0.1359), tensor(0.0973), 0, tensor(0.1508), 0, tensor(0.1966), tensor(0.3443), 2, 1, 1, 1, 1, 1, 114, 0.10000000149011612]
iteration:  85
input_X:  [tensor(0.0750), tensor(0.1359), tensor(0.0973), 0, tensor(0.1508), 0, tensor(0.1966), tensor(0.3443), 2, 1, 1, 1, 1, 1, 114, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  114 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 15,642,624 || all params: 8,045,903,872 || trainable%: 0.1944
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0750), tensor(0.1359), tensor(0.0973), 0, tensor(0.1508), 0, tensor(0.1966), tensor(0.3443)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0750)
number of datapoints needed (ratio * total):  374
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1359)
number of datapoints needed (ratio * total):  679
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0973)
number of datapoints needed (ratio * total):  486
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1508)
number of datapoints needed (ratio * total):  754
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1966)
number of datapoints needed (ratio * total):  983
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3443)
number of datapoints needed (ratio * total):  1721
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 374
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 679
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 486
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 754
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 983
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1721
})]
length of training data:  4997
training model...
trainable params: 15,642,624 || all params: 8,045,903,872 || trainable%: 0.1944
{'loss': 2.4519, 'grad_norm': 1.3312605619430542, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.6249, 'grad_norm': 0.863152801990509, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.948, 'grad_norm': 0.7240366339683533, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.708, 'grad_norm': 1.6675207614898682, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.715, 'grad_norm': 0.5964372158050537, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.6973, 'grad_norm': 1.6130009889602661, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.4092, 'grad_norm': 1.4826775789260864, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.2947, 'grad_norm': 1.004468321800232, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5329, 'train_samples_per_second': 49.705, 'train_steps_per_second': 6.217, 'train_loss': 1.7378392362309074, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_85/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.61
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)], [tensor(0.1048), tensor(0.2542), tensor(0.1399), tensor(0.0014), tensor(0.1959), tensor(8.9785e-18), tensor(0.0656), tensor(0.2383), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9271), tensor(0.1000)], [tensor(0.0750), tensor(0.1359), tensor(0.0973), tensor(7.2963e-18), tensor(0.1508), tensor(0.), tensor(0.1966), tensor(0.3443), tensor(0.0735), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8868), tensor(0.1000)]]
proposed candidate before processing: tensor([1.0756e-01, 9.0077e-02, 1.8758e-01, 1.2146e-17, 9.2893e-03, 1.5137e-01,
        1.0506e-01, 3.4906e-01, 7.2921e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.5693e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1076), tensor(0.0901), tensor(0.1876), 0, 0, tensor(0.1514), tensor(0.1051), tensor(0.3491), 2, 1, 1, 1, 1, 1, 110, 0.10000000149011612]
iteration:  86
input_X:  [tensor(0.1076), tensor(0.0901), tensor(0.1876), 0, 0, tensor(0.1514), tensor(0.1051), tensor(0.3491), 2, 1, 1, 1, 1, 1, 110, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  110 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 15,093,760 || all params: 8,045,355,008 || trainable%: 0.1876
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1076), tensor(0.0901), tensor(0.1876), 0, 0, tensor(0.1514), tensor(0.1051), tensor(0.3491)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1076)
number of datapoints needed (ratio * total):  537
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0901)
number of datapoints needed (ratio * total):  450
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1876)
number of datapoints needed (ratio * total):  937
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  tensor(0.1514)
number of datapoints needed (ratio * total):  756
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1051)
number of datapoints needed (ratio * total):  525
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3491)
number of datapoints needed (ratio * total):  1745
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 537
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 450
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 937
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 756
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 525
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1745
})]
length of training data:  4950
training model...
trainable params: 15,093,760 || all params: 8,045,355,008 || trainable%: 0.1876
{'loss': 2.4737, 'grad_norm': 2.0744054317474365, 'learning_rate': 0.00029507389162561576, 'epoch': 0.03}
{'loss': 1.6649, 'grad_norm': 1.1269179582595825, 'learning_rate': 0.0002852216748768473, 'epoch': 0.06}
{'loss': 1.8099, 'grad_norm': 0.8501185178756714, 'learning_rate': 0.0002753694581280788, 'epoch': 0.1}
{'loss': 1.9753, 'grad_norm': 2.2684578895568848, 'learning_rate': 0.0002655172413793103, 'epoch': 0.13}
{'loss': 1.4528, 'grad_norm': 0.695198118686676, 'learning_rate': 0.00025566502463054183, 'epoch': 0.16}
{'loss': 1.6822, 'grad_norm': 1.4954020977020264, 'learning_rate': 0.0002458128078817734, 'epoch': 0.19}
{'loss': 1.3126, 'grad_norm': 1.135628342628479, 'learning_rate': 0.00023596059113300492, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2097, 'train_samples_per_second': 49.396, 'train_steps_per_second': 6.177, 'train_loss': 1.733151357878679, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_86/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)], [tensor(0.1048), tensor(0.2542), tensor(0.1399), tensor(0.0014), tensor(0.1959), tensor(8.9785e-18), tensor(0.0656), tensor(0.2383), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9271), tensor(0.1000)], [tensor(0.0750), tensor(0.1359), tensor(0.0973), tensor(7.2963e-18), tensor(0.1508), tensor(0.), tensor(0.1966), tensor(0.3443), tensor(0.0735), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8868), tensor(0.1000)], [tensor(0.1076), tensor(0.0901), tensor(0.1876), tensor(1.2146e-17), tensor(0.0093), tensor(0.1514), tensor(0.1051), tensor(0.3491), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8569), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1243, 0.1822, 0.1613, 0.0000, 0.0710, 0.0000, 0.0471, 0.4140, 0.1262,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8719, 0.1000])
proposed candidate after normalizing: [tensor(0.1243), tensor(0.1822), tensor(0.1613), 0, tensor(0.0710), 0, 0, tensor(0.4140), 4, 1, 1, 1, 1, 1, 112, 0.10000000149011612]
iteration:  87
input_X:  [tensor(0.1243), tensor(0.1822), tensor(0.1613), 0, tensor(0.0710), 0, 0, tensor(0.4140), 4, 1, 1, 1, 1, 1, 112, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  112 0.10000000149011612 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 30,736,384 || all params: 8,060,997,632 || trainable%: 0.3813
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1243), tensor(0.1822), tensor(0.1613), 0, tensor(0.0710), 0, 0, tensor(0.4140)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1243)
number of datapoints needed (ratio * total):  621
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1822)
number of datapoints needed (ratio * total):  911
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1613)
number of datapoints needed (ratio * total):  806
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0710)
number of datapoints needed (ratio * total):  355
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  wikitext
ratio:  tensor(0.4140)
number of datapoints needed (ratio * total):  2069
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 621
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 911
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 806
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 355
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 2069
})]
length of training data:  4762
training model...
trainable params: 30,736,384 || all params: 8,060,997,632 || trainable%: 0.3813
{'loss': 2.3195, 'grad_norm': 1.9440890550613403, 'learning_rate': 0.0002948805460750853, 'epoch': 0.03}
{'loss': 1.8104, 'grad_norm': 1.2094377279281616, 'learning_rate': 0.00028464163822525593, 'epoch': 0.07}
{'loss': 1.8656, 'grad_norm': 0.9939681887626648, 'learning_rate': 0.00027440273037542657, 'epoch': 0.1}
{'loss': 1.5642, 'grad_norm': 0.5881935954093933, 'learning_rate': 0.00026416382252559726, 'epoch': 0.13}
{'loss': 1.4218, 'grad_norm': 0.5589117407798767, 'learning_rate': 0.0002539249146757679, 'epoch': 0.17}
{'loss': 1.7, 'grad_norm': 0.5051409602165222, 'learning_rate': 0.00024368600682593853, 'epoch': 0.2}
{'loss': 1.413, 'grad_norm': 0.7177951335906982, 'learning_rate': 0.0002334470989761092, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4953, 'train_samples_per_second': 47.385, 'train_steps_per_second': 5.931, 'train_loss': 1.7228471470527909, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_87/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)], [tensor(0.1048), tensor(0.2542), tensor(0.1399), tensor(0.0014), tensor(0.1959), tensor(8.9785e-18), tensor(0.0656), tensor(0.2383), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9271), tensor(0.1000)], [tensor(0.0750), tensor(0.1359), tensor(0.0973), tensor(7.2963e-18), tensor(0.1508), tensor(0.), tensor(0.1966), tensor(0.3443), tensor(0.0735), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8868), tensor(0.1000)], [tensor(0.1076), tensor(0.0901), tensor(0.1876), tensor(1.2146e-17), tensor(0.0093), tensor(0.1514), tensor(0.1051), tensor(0.3491), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8569), tensor(0.1000)], [tensor(0.1243), tensor(0.1822), tensor(0.1613), tensor(0.), tensor(0.0710), tensor(0.), tensor(0.0471), tensor(0.4140), tensor(0.1262), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8719), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1240, 0.1111, 0.0808, 0.0000, 0.0382, 0.0000, 0.2045, 0.4415, 0.1048,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.1000])
proposed candidate after normalizing: [tensor(0.1240), tensor(0.1111), tensor(0.0808), 0, 0, 0, tensor(0.2045), tensor(0.4415), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
iteration:  88
input_X:  [tensor(0.1240), tensor(0.1111), tensor(0.0808), 0, 0, 0, tensor(0.2045), tensor(0.4415), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  128 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 34,029,568 || all params: 8,064,290,816 || trainable%: 0.4220
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1240), tensor(0.1111), tensor(0.0808), 0, 0, 0, tensor(0.2045), tensor(0.4415)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1240)
number of datapoints needed (ratio * total):  619
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1111)
number of datapoints needed (ratio * total):  555
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0808)
number of datapoints needed (ratio * total):  404
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2045)
number of datapoints needed (ratio * total):  1022
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.4415)
number of datapoints needed (ratio * total):  2207
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 619
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 555
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 404
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1022
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 2207
})]
length of training data:  4807
training model...
trainable params: 34,029,568 || all params: 8,064,290,816 || trainable%: 0.4220
{'loss': 2.248, 'grad_norm': 0.5172852873802185, 'learning_rate': 0.00029492385786802027, 'epoch': 0.03}
{'loss': 1.7575, 'grad_norm': 0.43733832240104675, 'learning_rate': 0.00028477157360406086, 'epoch': 0.07}
{'loss': 1.7487, 'grad_norm': 0.5518730878829956, 'learning_rate': 0.0002746192893401015, 'epoch': 0.1}
{'loss': 1.773, 'grad_norm': 0.49200502038002014, 'learning_rate': 0.0002644670050761421, 'epoch': 0.13}
{'loss': 1.9037, 'grad_norm': 0.6648725867271423, 'learning_rate': 0.00025431472081218273, 'epoch': 0.17}
{'loss': 1.6719, 'grad_norm': 0.8290554285049438, 'learning_rate': 0.0002441624365482233, 'epoch': 0.2}
{'loss': 1.5162, 'grad_norm': 0.6636790633201599, 'learning_rate': 0.00023401015228426394, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.7222, 'train_samples_per_second': 47.725, 'train_steps_per_second': 5.967, 'train_loss': 1.800084024105432, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_88/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)], [tensor(0.1048), tensor(0.2542), tensor(0.1399), tensor(0.0014), tensor(0.1959), tensor(8.9785e-18), tensor(0.0656), tensor(0.2383), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9271), tensor(0.1000)], [tensor(0.0750), tensor(0.1359), tensor(0.0973), tensor(7.2963e-18), tensor(0.1508), tensor(0.), tensor(0.1966), tensor(0.3443), tensor(0.0735), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8868), tensor(0.1000)], [tensor(0.1076), tensor(0.0901), tensor(0.1876), tensor(1.2146e-17), tensor(0.0093), tensor(0.1514), tensor(0.1051), tensor(0.3491), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8569), tensor(0.1000)], [tensor(0.1243), tensor(0.1822), tensor(0.1613), tensor(0.), tensor(0.0710), tensor(0.), tensor(0.0471), tensor(0.4140), tensor(0.1262), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8719), tensor(0.1000)], [tensor(0.1240), tensor(0.1111), tensor(0.0808), tensor(0.), tensor(0.0382), tensor(0.), tensor(0.2045), tensor(0.4415), tensor(0.1048), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)]]
proposed candidate before processing: tensor([0.2641, 0.1593, 0.1723, 0.0250, 0.1459, 0.0000, 0.0543, 0.1792, 0.0357,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9208, 0.1000])
proposed candidate after normalizing: [tensor(0.2641), tensor(0.1593), tensor(0.1723), 0, tensor(0.1459), 0, tensor(0.0543), tensor(0.1792), 1, 1, 1, 1, 1, 1, 118, 0.10000000149011612]
iteration:  89
input_X:  [tensor(0.2641), tensor(0.1593), tensor(0.1723), 0, tensor(0.1459), 0, tensor(0.0543), tensor(0.1792), 1, 1, 1, 1, 1, 1, 118, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  118 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 25,659,392 || all params: 8,055,920,640 || trainable%: 0.3185
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.2641), tensor(0.1593), tensor(0.1723), 0, tensor(0.1459), 0, tensor(0.0543), tensor(0.1792)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.2641)
number of datapoints needed (ratio * total):  1320
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1593)
number of datapoints needed (ratio * total):  796
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1723)
number of datapoints needed (ratio * total):  861
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1459)
number of datapoints needed (ratio * total):  729
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0543)
number of datapoints needed (ratio * total):  271
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1792)
number of datapoints needed (ratio * total):  895
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1320
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 796
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 861
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 729
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 271
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 895
})]
length of training data:  4872
training model...
trainable params: 25,659,392 || all params: 8,055,920,640 || trainable%: 0.3185
{'loss': 1.3286, 'grad_norm': 0.9490896463394165, 'learning_rate': 0.0002949916527545909, 'epoch': 0.03}
{'loss': 1.0885, 'grad_norm': 0.6288686394691467, 'learning_rate': 0.0002849749582637729, 'epoch': 0.07}
{'loss': 1.0002, 'grad_norm': 0.3146875202655792, 'learning_rate': 0.00027495826377295493, 'epoch': 0.1}
{'loss': 1.1758, 'grad_norm': 0.712253749370575, 'learning_rate': 0.0002649415692821369, 'epoch': 0.13}
{'loss': 1.034, 'grad_norm': 0.7455271482467651, 'learning_rate': 0.0002549248747913189, 'epoch': 0.16}
{'loss': 1.1153, 'grad_norm': 0.8685178160667419, 'learning_rate': 0.0002449081803005008, 'epoch': 0.2}
{'loss': 1.4761, 'grad_norm': 0.35798123478889465, 'learning_rate': 0.00023489148580968278, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5326, 'train_samples_per_second': 48.462, 'train_steps_per_second': 6.058, 'train_loss': 1.1294279283092867, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_89/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.64
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)], [tensor(0.1048), tensor(0.2542), tensor(0.1399), tensor(0.0014), tensor(0.1959), tensor(8.9785e-18), tensor(0.0656), tensor(0.2383), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9271), tensor(0.1000)], [tensor(0.0750), tensor(0.1359), tensor(0.0973), tensor(7.2963e-18), tensor(0.1508), tensor(0.), tensor(0.1966), tensor(0.3443), tensor(0.0735), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8868), tensor(0.1000)], [tensor(0.1076), tensor(0.0901), tensor(0.1876), tensor(1.2146e-17), tensor(0.0093), tensor(0.1514), tensor(0.1051), tensor(0.3491), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8569), tensor(0.1000)], [tensor(0.1243), tensor(0.1822), tensor(0.1613), tensor(0.), tensor(0.0710), tensor(0.), tensor(0.0471), tensor(0.4140), tensor(0.1262), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8719), tensor(0.1000)], [tensor(0.1240), tensor(0.1111), tensor(0.0808), tensor(0.), tensor(0.0382), tensor(0.), tensor(0.2045), tensor(0.4415), tensor(0.1048), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.2641), tensor(0.1593), tensor(0.1723), tensor(0.0250), tensor(0.1459), tensor(0.), tensor(0.0543), tensor(0.1792), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9208), tensor(0.1000)]]
proposed candidate before processing: tensor([1.2517e-02, 1.0344e-01, 1.5863e-01, 2.6523e-17, 4.7560e-02, 0.0000e+00,
        1.9751e-01, 4.8034e-01, 1.0282e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e-01])
proposed candidate after normalizing: [0, tensor(0.1034), tensor(0.1586), 0, 0, 0, tensor(0.1975), tensor(0.4803), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
iteration:  90
input_X:  [0, tensor(0.1034), tensor(0.1586), 0, 0, 0, tensor(0.1975), tensor(0.4803), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  128 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.1034), tensor(0.1586), 0, 0, 0, tensor(0.1975), tensor(0.4803)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.1034)
number of datapoints needed (ratio * total):  517
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1586)
number of datapoints needed (ratio * total):  793
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1975)
number of datapoints needed (ratio * total):  987
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.4803)
number of datapoints needed (ratio * total):  2401
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 517
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 793
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 987
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 2401
})]
length of training data:  4698
training model...
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
{'loss': 2.4997, 'grad_norm': 0.31846994161605835, 'learning_rate': 0.00029480968858131484, 'epoch': 0.03}
{'loss': 2.1766, 'grad_norm': 0.4284036457538605, 'learning_rate': 0.00028442906574394463, 'epoch': 0.07}
{'loss': 1.9793, 'grad_norm': 0.609356164932251, 'learning_rate': 0.00027404844290657436, 'epoch': 0.1}
{'loss': 1.768, 'grad_norm': 0.37413474917411804, 'learning_rate': 0.00026366782006920415, 'epoch': 0.14}
{'loss': 1.7021, 'grad_norm': 0.8401056528091431, 'learning_rate': 0.0002532871972318339, 'epoch': 0.17}
{'loss': 1.9486, 'grad_norm': 0.5754188299179077, 'learning_rate': 0.00024290657439446364, 'epoch': 0.2}
{'loss': 1.9359, 'grad_norm': 0.7018945813179016, 'learning_rate': 0.0002325259515570934, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4421, 'train_samples_per_second': 46.773, 'train_steps_per_second': 5.854, 'train_loss': 1.9990543971788015, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_90/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)], [tensor(0.1048), tensor(0.2542), tensor(0.1399), tensor(0.0014), tensor(0.1959), tensor(8.9785e-18), tensor(0.0656), tensor(0.2383), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9271), tensor(0.1000)], [tensor(0.0750), tensor(0.1359), tensor(0.0973), tensor(7.2963e-18), tensor(0.1508), tensor(0.), tensor(0.1966), tensor(0.3443), tensor(0.0735), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8868), tensor(0.1000)], [tensor(0.1076), tensor(0.0901), tensor(0.1876), tensor(1.2146e-17), tensor(0.0093), tensor(0.1514), tensor(0.1051), tensor(0.3491), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8569), tensor(0.1000)], [tensor(0.1243), tensor(0.1822), tensor(0.1613), tensor(0.), tensor(0.0710), tensor(0.), tensor(0.0471), tensor(0.4140), tensor(0.1262), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8719), tensor(0.1000)], [tensor(0.1240), tensor(0.1111), tensor(0.0808), tensor(0.), tensor(0.0382), tensor(0.), tensor(0.2045), tensor(0.4415), tensor(0.1048), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.2641), tensor(0.1593), tensor(0.1723), tensor(0.0250), tensor(0.1459), tensor(0.), tensor(0.0543), tensor(0.1792), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9208), tensor(0.1000)], [tensor(0.0125), tensor(0.1034), tensor(0.1586), tensor(2.6523e-17), tensor(0.0476), tensor(0.), tensor(0.1975), tensor(0.4803), tensor(0.1028), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)]]
proposed candidate before processing: tensor([0.0479, 0.1753, 0.0516, 0.0000, 0.0158, 0.0000, 0.1699, 0.5395, 0.1016,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.1000])
proposed candidate after normalizing: [0, tensor(0.1753), tensor(0.0516), 0, 0, 0, tensor(0.1699), tensor(0.5395), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
iteration:  91
input_X:  [0, tensor(0.1753), tensor(0.0516), 0, 0, 0, tensor(0.1699), tensor(0.5395), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  128 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.1753), tensor(0.0516), 0, 0, 0, tensor(0.1699), tensor(0.5395)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.1753)
number of datapoints needed (ratio * total):  876
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0516)
number of datapoints needed (ratio * total):  258
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1699)
number of datapoints needed (ratio * total):  849
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.5395)
number of datapoints needed (ratio * total):  2697
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 876
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 258
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 849
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 2697
})]
length of training data:  4680
training model...
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
{'loss': 2.5068, 'grad_norm': 0.4191136360168457, 'learning_rate': 0.00029478260869565215, 'epoch': 0.03}
{'loss': 2.3518, 'grad_norm': 0.9451742768287659, 'learning_rate': 0.0002843478260869565, 'epoch': 0.07}
{'loss': 2.1824, 'grad_norm': 0.5515143275260925, 'learning_rate': 0.00027391304347826085, 'epoch': 0.1}
{'loss': 1.9928, 'grad_norm': 0.4240068793296814, 'learning_rate': 0.0002634782608695652, 'epoch': 0.14}
{'loss': 1.9508, 'grad_norm': 0.7425082325935364, 'learning_rate': 0.00025304347826086954, 'epoch': 0.17}
{'loss': 2.0056, 'grad_norm': 0.6019505262374878, 'learning_rate': 0.0002426086956521739, 'epoch': 0.21}
{'loss': 1.9662, 'grad_norm': 0.6184395551681519, 'learning_rate': 0.00023217391304347824, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5574, 'train_samples_per_second': 46.541, 'train_steps_per_second': 5.818, 'train_loss': 2.0823351944549175, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_91/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)], [tensor(0.1048), tensor(0.2542), tensor(0.1399), tensor(0.0014), tensor(0.1959), tensor(8.9785e-18), tensor(0.0656), tensor(0.2383), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9271), tensor(0.1000)], [tensor(0.0750), tensor(0.1359), tensor(0.0973), tensor(7.2963e-18), tensor(0.1508), tensor(0.), tensor(0.1966), tensor(0.3443), tensor(0.0735), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8868), tensor(0.1000)], [tensor(0.1076), tensor(0.0901), tensor(0.1876), tensor(1.2146e-17), tensor(0.0093), tensor(0.1514), tensor(0.1051), tensor(0.3491), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8569), tensor(0.1000)], [tensor(0.1243), tensor(0.1822), tensor(0.1613), tensor(0.), tensor(0.0710), tensor(0.), tensor(0.0471), tensor(0.4140), tensor(0.1262), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8719), tensor(0.1000)], [tensor(0.1240), tensor(0.1111), tensor(0.0808), tensor(0.), tensor(0.0382), tensor(0.), tensor(0.2045), tensor(0.4415), tensor(0.1048), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.2641), tensor(0.1593), tensor(0.1723), tensor(0.0250), tensor(0.1459), tensor(0.), tensor(0.0543), tensor(0.1792), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9208), tensor(0.1000)], [tensor(0.0125), tensor(0.1034), tensor(0.1586), tensor(2.6523e-17), tensor(0.0476), tensor(0.), tensor(0.1975), tensor(0.4803), tensor(0.1028), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0479), tensor(0.1753), tensor(0.0516), tensor(0.), tensor(0.0158), tensor(0.), tensor(0.1699), tensor(0.5395), tensor(0.1016), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)]]
proposed candidate before processing: tensor([9.0094e-02, 1.8225e-02, 2.0791e-01, 0.0000e+00, 7.4636e-02, 3.3078e-17,
        2.4776e-01, 3.6137e-01, 1.0545e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.0901), 0, tensor(0.2079), 0, tensor(0.0746), 0, tensor(0.2478), tensor(0.3614), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
iteration:  92
input_X:  [tensor(0.0901), 0, tensor(0.2079), 0, tensor(0.0746), 0, tensor(0.2478), tensor(0.3614), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  128 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0901), 0, tensor(0.2079), 0, tensor(0.0746), 0, tensor(0.2478), tensor(0.3614)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0901)
number of datapoints needed (ratio * total):  450
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  hellaswag
ratio:  tensor(0.2079)
number of datapoints needed (ratio * total):  1039
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0746)
number of datapoints needed (ratio * total):  373
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2478)
number of datapoints needed (ratio * total):  1238
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3614)
number of datapoints needed (ratio * total):  1806
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 450
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1039
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 373
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1238
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1806
})]
length of training data:  4906
training model...
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
{'loss': 2.4624, 'grad_norm': 1.3434520959854126, 'learning_rate': 0.0002950331125827814, 'epoch': 0.03}
{'loss': 1.938, 'grad_norm': 0.41172221302986145, 'learning_rate': 0.00028509933774834435, 'epoch': 0.07}
{'loss': 1.8743, 'grad_norm': 0.8675805926322937, 'learning_rate': 0.0002751655629139073, 'epoch': 0.1}
{'loss': 1.5486, 'grad_norm': 0.3949260413646698, 'learning_rate': 0.00026523178807947017, 'epoch': 0.13}
{'loss': 1.8358, 'grad_norm': 0.5119273066520691, 'learning_rate': 0.0002552980132450331, 'epoch': 0.16}
{'loss': 1.6715, 'grad_norm': 0.34222152829170227, 'learning_rate': 0.000245364238410596, 'epoch': 0.2}
{'loss': 1.7629, 'grad_norm': 0.8022587299346924, 'learning_rate': 0.00023543046357615892, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1837, 'train_samples_per_second': 48.97, 'train_steps_per_second': 6.129, 'train_loss': 1.8478073409841032, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_92/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)], [tensor(0.1048), tensor(0.2542), tensor(0.1399), tensor(0.0014), tensor(0.1959), tensor(8.9785e-18), tensor(0.0656), tensor(0.2383), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9271), tensor(0.1000)], [tensor(0.0750), tensor(0.1359), tensor(0.0973), tensor(7.2963e-18), tensor(0.1508), tensor(0.), tensor(0.1966), tensor(0.3443), tensor(0.0735), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8868), tensor(0.1000)], [tensor(0.1076), tensor(0.0901), tensor(0.1876), tensor(1.2146e-17), tensor(0.0093), tensor(0.1514), tensor(0.1051), tensor(0.3491), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8569), tensor(0.1000)], [tensor(0.1243), tensor(0.1822), tensor(0.1613), tensor(0.), tensor(0.0710), tensor(0.), tensor(0.0471), tensor(0.4140), tensor(0.1262), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8719), tensor(0.1000)], [tensor(0.1240), tensor(0.1111), tensor(0.0808), tensor(0.), tensor(0.0382), tensor(0.), tensor(0.2045), tensor(0.4415), tensor(0.1048), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.2641), tensor(0.1593), tensor(0.1723), tensor(0.0250), tensor(0.1459), tensor(0.), tensor(0.0543), tensor(0.1792), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9208), tensor(0.1000)], [tensor(0.0125), tensor(0.1034), tensor(0.1586), tensor(2.6523e-17), tensor(0.0476), tensor(0.), tensor(0.1975), tensor(0.4803), tensor(0.1028), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0479), tensor(0.1753), tensor(0.0516), tensor(0.), tensor(0.0158), tensor(0.), tensor(0.1699), tensor(0.5395), tensor(0.1016), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0901), tensor(0.0182), tensor(0.2079), tensor(0.), tensor(0.0746), tensor(3.3078e-17), tensor(0.2478), tensor(0.3614), tensor(0.1055), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)]]
proposed candidate before processing: tensor([1.4820e-01, 3.6421e-02, 1.7562e-01, 0.0000e+00, 8.3826e-02, 7.6911e-18,
        8.8826e-02, 4.6710e-01, 1.0568e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1482), 0, tensor(0.1756), 0, tensor(0.0838), 0, tensor(0.0888), tensor(0.4671), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
iteration:  93
input_X:  [tensor(0.1482), 0, tensor(0.1756), 0, tensor(0.0838), 0, tensor(0.0888), tensor(0.4671), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  128 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1482), 0, tensor(0.1756), 0, tensor(0.0838), 0, tensor(0.0888), tensor(0.4671)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1482)
number of datapoints needed (ratio * total):  740
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  hellaswag
ratio:  tensor(0.1756)
number of datapoints needed (ratio * total):  878
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0838)
number of datapoints needed (ratio * total):  419
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0888)
number of datapoints needed (ratio * total):  444
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.4671)
number of datapoints needed (ratio * total):  2335
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 740
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 878
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 419
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 444
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 2335
})]
length of training data:  4816
training model...
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
{'loss': 2.5754, 'grad_norm': 0.7930033802986145, 'learning_rate': 0.0002949324324324324, 'epoch': 0.03}
{'loss': 2.2756, 'grad_norm': 1.6044682264328003, 'learning_rate': 0.0002847972972972973, 'epoch': 0.07}
{'loss': 1.7176, 'grad_norm': 0.973338782787323, 'learning_rate': 0.0002746621621621621, 'epoch': 0.1}
{'loss': 1.9259, 'grad_norm': 0.4419568181037903, 'learning_rate': 0.000264527027027027, 'epoch': 0.13}
{'loss': 2.3356, 'grad_norm': 0.7142415046691895, 'learning_rate': 0.0002543918918918919, 'epoch': 0.17}
{'loss': 1.9436, 'grad_norm': 0.44459986686706543, 'learning_rate': 0.00024425675675675675, 'epoch': 0.2}
{'loss': 1.7115, 'grad_norm': 0.3430735468864441, 'learning_rate': 0.00023412162162162159, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'loss': 2.0096, 'grad_norm': 0.47474029660224915, 'learning_rate': 0.00022398648648648645, 'epoch': 0.27}
{'train_runtime': 100.2872, 'train_samples_per_second': 48.022, 'train_steps_per_second': 6.003, 'train_loss': 2.0618701457977293, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_93/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)], [tensor(0.1048), tensor(0.2542), tensor(0.1399), tensor(0.0014), tensor(0.1959), tensor(8.9785e-18), tensor(0.0656), tensor(0.2383), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9271), tensor(0.1000)], [tensor(0.0750), tensor(0.1359), tensor(0.0973), tensor(7.2963e-18), tensor(0.1508), tensor(0.), tensor(0.1966), tensor(0.3443), tensor(0.0735), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8868), tensor(0.1000)], [tensor(0.1076), tensor(0.0901), tensor(0.1876), tensor(1.2146e-17), tensor(0.0093), tensor(0.1514), tensor(0.1051), tensor(0.3491), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8569), tensor(0.1000)], [tensor(0.1243), tensor(0.1822), tensor(0.1613), tensor(0.), tensor(0.0710), tensor(0.), tensor(0.0471), tensor(0.4140), tensor(0.1262), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8719), tensor(0.1000)], [tensor(0.1240), tensor(0.1111), tensor(0.0808), tensor(0.), tensor(0.0382), tensor(0.), tensor(0.2045), tensor(0.4415), tensor(0.1048), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.2641), tensor(0.1593), tensor(0.1723), tensor(0.0250), tensor(0.1459), tensor(0.), tensor(0.0543), tensor(0.1792), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9208), tensor(0.1000)], [tensor(0.0125), tensor(0.1034), tensor(0.1586), tensor(2.6523e-17), tensor(0.0476), tensor(0.), tensor(0.1975), tensor(0.4803), tensor(0.1028), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0479), tensor(0.1753), tensor(0.0516), tensor(0.), tensor(0.0158), tensor(0.), tensor(0.1699), tensor(0.5395), tensor(0.1016), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0901), tensor(0.0182), tensor(0.2079), tensor(0.), tensor(0.0746), tensor(3.3078e-17), tensor(0.2478), tensor(0.3614), tensor(0.1055), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.1482), tensor(0.0364), tensor(0.1756), tensor(0.), tensor(0.0838), tensor(7.6911e-18), tensor(0.0888), tensor(0.4671), tensor(0.1057), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)]]
proposed candidate before processing: tensor([3.2976e-02, 1.6020e-01, 1.2345e-01, 4.7689e-17, 6.7889e-02, 0.0000e+00,
        3.1000e-01, 3.0549e-01, 1.0451e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e-01])
proposed candidate after normalizing: [0, tensor(0.1602), tensor(0.1234), 0, tensor(0.0679), 0, tensor(0.3100), tensor(0.3055), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
iteration:  94
input_X:  [0, tensor(0.1602), tensor(0.1234), 0, tensor(0.0679), 0, tensor(0.3100), tensor(0.3055), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  128 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.1602), tensor(0.1234), 0, tensor(0.0679), 0, tensor(0.3100), tensor(0.3055)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.1602)
number of datapoints needed (ratio * total):  800
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1234)
number of datapoints needed (ratio * total):  617
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0679)
number of datapoints needed (ratio * total):  339
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.3100)
number of datapoints needed (ratio * total):  1550
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3055)
number of datapoints needed (ratio * total):  1527
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 800
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 617
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 339
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1550
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1527
})]
length of training data:  4833
training model...
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
{'loss': 2.51, 'grad_norm': 0.47099170088768005, 'learning_rate': 0.00029495798319327727, 'epoch': 0.03}
{'loss': 1.6754, 'grad_norm': 1.3061153888702393, 'learning_rate': 0.0002848739495798319, 'epoch': 0.07}
{'loss': 1.7462, 'grad_norm': 0.6512384414672852, 'learning_rate': 0.0002747899159663865, 'epoch': 0.1}
{'loss': 1.6239, 'grad_norm': 2.5405020713806152, 'learning_rate': 0.00026470588235294115, 'epoch': 0.13}
{'loss': 1.4074, 'grad_norm': 0.6627545952796936, 'learning_rate': 0.00025462184873949575, 'epoch': 0.17}
{'loss': 1.5829, 'grad_norm': 1.106305718421936, 'learning_rate': 0.0002445378151260504, 'epoch': 0.2}
{'loss': 1.5091, 'grad_norm': 0.4487500488758087, 'learning_rate': 0.000234453781512605, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3381, 'train_samples_per_second': 48.167, 'train_steps_per_second': 6.03, 'train_loss': 1.6957825758518317, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_94/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)], [tensor(0.1048), tensor(0.2542), tensor(0.1399), tensor(0.0014), tensor(0.1959), tensor(8.9785e-18), tensor(0.0656), tensor(0.2383), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9271), tensor(0.1000)], [tensor(0.0750), tensor(0.1359), tensor(0.0973), tensor(7.2963e-18), tensor(0.1508), tensor(0.), tensor(0.1966), tensor(0.3443), tensor(0.0735), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8868), tensor(0.1000)], [tensor(0.1076), tensor(0.0901), tensor(0.1876), tensor(1.2146e-17), tensor(0.0093), tensor(0.1514), tensor(0.1051), tensor(0.3491), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8569), tensor(0.1000)], [tensor(0.1243), tensor(0.1822), tensor(0.1613), tensor(0.), tensor(0.0710), tensor(0.), tensor(0.0471), tensor(0.4140), tensor(0.1262), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8719), tensor(0.1000)], [tensor(0.1240), tensor(0.1111), tensor(0.0808), tensor(0.), tensor(0.0382), tensor(0.), tensor(0.2045), tensor(0.4415), tensor(0.1048), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.2641), tensor(0.1593), tensor(0.1723), tensor(0.0250), tensor(0.1459), tensor(0.), tensor(0.0543), tensor(0.1792), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9208), tensor(0.1000)], [tensor(0.0125), tensor(0.1034), tensor(0.1586), tensor(2.6523e-17), tensor(0.0476), tensor(0.), tensor(0.1975), tensor(0.4803), tensor(0.1028), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0479), tensor(0.1753), tensor(0.0516), tensor(0.), tensor(0.0158), tensor(0.), tensor(0.1699), tensor(0.5395), tensor(0.1016), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0901), tensor(0.0182), tensor(0.2079), tensor(0.), tensor(0.0746), tensor(3.3078e-17), tensor(0.2478), tensor(0.3614), tensor(0.1055), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.1482), tensor(0.0364), tensor(0.1756), tensor(0.), tensor(0.0838), tensor(7.6911e-18), tensor(0.0888), tensor(0.4671), tensor(0.1057), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0330), tensor(0.1602), tensor(0.1234), tensor(4.7689e-17), tensor(0.0679), tensor(0.), tensor(0.3100), tensor(0.3055), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)]]
proposed candidate before processing: tensor([5.4224e-02, 8.5388e-02, 9.2484e-02, 9.0802e-18, 6.3754e-02, 1.1289e-17,
        3.2082e-01, 3.8333e-01, 1.0523e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.6518e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.0542), tensor(0.0854), tensor(0.0925), 0, tensor(0.0638), 0, tensor(0.3208), tensor(0.3833), 3, 1, 1, 1, 1, 1, 111, 0.10000000149011612]
iteration:  95
input_X:  [tensor(0.0542), tensor(0.0854), tensor(0.0925), 0, tensor(0.0638), 0, tensor(0.3208), tensor(0.3833), 3, 1, 1, 1, 1, 1, 111, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  111 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 22,846,464 || all params: 8,053,107,712 || trainable%: 0.2837
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0542), tensor(0.0854), tensor(0.0925), 0, tensor(0.0638), 0, tensor(0.3208), tensor(0.3833)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0542)
number of datapoints needed (ratio * total):  271
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0854)
number of datapoints needed (ratio * total):  426
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0925)
number of datapoints needed (ratio * total):  462
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0638)
number of datapoints needed (ratio * total):  318
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.3208)
number of datapoints needed (ratio * total):  1604
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3833)
number of datapoints needed (ratio * total):  1916
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 271
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 426
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 462
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 318
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1604
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1916
})]
length of training data:  4997
training model...
trainable params: 22,846,464 || all params: 8,053,107,712 || trainable%: 0.2837
{'loss': 2.5527, 'grad_norm': 2.0259644985198975, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5673, 'grad_norm': 0.6551182866096497, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.7432, 'grad_norm': 0.8409940600395203, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.6993, 'grad_norm': 0.6496692299842834, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.7557, 'grad_norm': 0.8290674686431885, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.6605, 'grad_norm': 0.6313338279724121, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.554, 'grad_norm': 0.37109142541885376, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.731, 'grad_norm': 0.4556860029697418, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6287, 'train_samples_per_second': 49.658, 'train_steps_per_second': 6.211, 'train_loss': 1.7778237441490436, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_95/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.61
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)], [tensor(0.1048), tensor(0.2542), tensor(0.1399), tensor(0.0014), tensor(0.1959), tensor(8.9785e-18), tensor(0.0656), tensor(0.2383), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9271), tensor(0.1000)], [tensor(0.0750), tensor(0.1359), tensor(0.0973), tensor(7.2963e-18), tensor(0.1508), tensor(0.), tensor(0.1966), tensor(0.3443), tensor(0.0735), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8868), tensor(0.1000)], [tensor(0.1076), tensor(0.0901), tensor(0.1876), tensor(1.2146e-17), tensor(0.0093), tensor(0.1514), tensor(0.1051), tensor(0.3491), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8569), tensor(0.1000)], [tensor(0.1243), tensor(0.1822), tensor(0.1613), tensor(0.), tensor(0.0710), tensor(0.), tensor(0.0471), tensor(0.4140), tensor(0.1262), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8719), tensor(0.1000)], [tensor(0.1240), tensor(0.1111), tensor(0.0808), tensor(0.), tensor(0.0382), tensor(0.), tensor(0.2045), tensor(0.4415), tensor(0.1048), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.2641), tensor(0.1593), tensor(0.1723), tensor(0.0250), tensor(0.1459), tensor(0.), tensor(0.0543), tensor(0.1792), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9208), tensor(0.1000)], [tensor(0.0125), tensor(0.1034), tensor(0.1586), tensor(2.6523e-17), tensor(0.0476), tensor(0.), tensor(0.1975), tensor(0.4803), tensor(0.1028), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0479), tensor(0.1753), tensor(0.0516), tensor(0.), tensor(0.0158), tensor(0.), tensor(0.1699), tensor(0.5395), tensor(0.1016), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0901), tensor(0.0182), tensor(0.2079), tensor(0.), tensor(0.0746), tensor(3.3078e-17), tensor(0.2478), tensor(0.3614), tensor(0.1055), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.1482), tensor(0.0364), tensor(0.1756), tensor(0.), tensor(0.0838), tensor(7.6911e-18), tensor(0.0888), tensor(0.4671), tensor(0.1057), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0330), tensor(0.1602), tensor(0.1234), tensor(4.7689e-17), tensor(0.0679), tensor(0.), tensor(0.3100), tensor(0.3055), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0542), tensor(0.0854), tensor(0.0925), tensor(9.0802e-18), tensor(0.0638), tensor(1.1289e-17), tensor(0.3208), tensor(0.3833), tensor(0.1052), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8652), tensor(0.1000)]]
proposed candidate before processing: tensor([0.0199, 0.2055, 0.0434, 0.1316, 0.1563, 0.0000, 0.1739, 0.2694, 0.0355,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9391, 0.1000])
proposed candidate after normalizing: [0, tensor(0.2055), 0, tensor(0.1316), tensor(0.1563), 0, tensor(0.1739), tensor(0.2694), 1, 1, 1, 1, 1, 1, 120, 0.10000000149011612]
iteration:  96
input_X:  [0, tensor(0.2055), 0, tensor(0.1316), tensor(0.1563), 0, tensor(0.1739), tensor(0.2694), 1, 1, 1, 1, 1, 1, 120, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  120 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 23,463,936 || all params: 8,053,725,184 || trainable%: 0.2913
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.2055), 0, tensor(0.1316), tensor(0.1563), 0, tensor(0.1739), tensor(0.2694)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.2055)
number of datapoints needed (ratio * total):  1027
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1316)
number of datapoints needed (ratio * total):  657
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1563)
number of datapoints needed (ratio * total):  781
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1739)
number of datapoints needed (ratio * total):  869
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2694)
number of datapoints needed (ratio * total):  1347
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1027
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 657
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 781
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 869
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1347
})]
length of training data:  4681
training model...
trainable params: 23,463,936 || all params: 8,053,725,184 || trainable%: 0.2913
{'loss': 1.4897, 'grad_norm': 0.8184114098548889, 'learning_rate': 0.0002947916666666666, 'epoch': 0.03}
{'loss': 1.6071, 'grad_norm': 1.3377822637557983, 'learning_rate': 0.00028437499999999996, 'epoch': 0.07}
{'loss': 1.5235, 'grad_norm': 1.4475880861282349, 'learning_rate': 0.0002739583333333333, 'epoch': 0.1}
{'loss': 1.3073, 'grad_norm': 1.2222572565078735, 'learning_rate': 0.00026354166666666664, 'epoch': 0.14}
{'loss': 1.3965, 'grad_norm': 0.8931728005409241, 'learning_rate': 0.000253125, 'epoch': 0.17}
{'loss': 1.3985, 'grad_norm': 0.47379109263420105, 'learning_rate': 0.00024270833333333333, 'epoch': 0.2}
{'loss': 1.244, 'grad_norm': 0.7278712391853333, 'learning_rate': 0.00023229166666666664, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0604, 'train_samples_per_second': 46.782, 'train_steps_per_second': 5.856, 'train_loss': 1.4154441231175472, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_96/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)], [tensor(0.1048), tensor(0.2542), tensor(0.1399), tensor(0.0014), tensor(0.1959), tensor(8.9785e-18), tensor(0.0656), tensor(0.2383), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9271), tensor(0.1000)], [tensor(0.0750), tensor(0.1359), tensor(0.0973), tensor(7.2963e-18), tensor(0.1508), tensor(0.), tensor(0.1966), tensor(0.3443), tensor(0.0735), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8868), tensor(0.1000)], [tensor(0.1076), tensor(0.0901), tensor(0.1876), tensor(1.2146e-17), tensor(0.0093), tensor(0.1514), tensor(0.1051), tensor(0.3491), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8569), tensor(0.1000)], [tensor(0.1243), tensor(0.1822), tensor(0.1613), tensor(0.), tensor(0.0710), tensor(0.), tensor(0.0471), tensor(0.4140), tensor(0.1262), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8719), tensor(0.1000)], [tensor(0.1240), tensor(0.1111), tensor(0.0808), tensor(0.), tensor(0.0382), tensor(0.), tensor(0.2045), tensor(0.4415), tensor(0.1048), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.2641), tensor(0.1593), tensor(0.1723), tensor(0.0250), tensor(0.1459), tensor(0.), tensor(0.0543), tensor(0.1792), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9208), tensor(0.1000)], [tensor(0.0125), tensor(0.1034), tensor(0.1586), tensor(2.6523e-17), tensor(0.0476), tensor(0.), tensor(0.1975), tensor(0.4803), tensor(0.1028), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0479), tensor(0.1753), tensor(0.0516), tensor(0.), tensor(0.0158), tensor(0.), tensor(0.1699), tensor(0.5395), tensor(0.1016), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0901), tensor(0.0182), tensor(0.2079), tensor(0.), tensor(0.0746), tensor(3.3078e-17), tensor(0.2478), tensor(0.3614), tensor(0.1055), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.1482), tensor(0.0364), tensor(0.1756), tensor(0.), tensor(0.0838), tensor(7.6911e-18), tensor(0.0888), tensor(0.4671), tensor(0.1057), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0330), tensor(0.1602), tensor(0.1234), tensor(4.7689e-17), tensor(0.0679), tensor(0.), tensor(0.3100), tensor(0.3055), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0542), tensor(0.0854), tensor(0.0925), tensor(9.0802e-18), tensor(0.0638), tensor(1.1289e-17), tensor(0.3208), tensor(0.3833), tensor(0.1052), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8652), tensor(0.1000)], [tensor(0.0199), tensor(0.2055), tensor(0.0434), tensor(0.1316), tensor(0.1563), tensor(0.), tensor(0.1739), tensor(0.2694), tensor(0.0355), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9391), tensor(0.1000)]]
proposed candidate before processing: tensor([0.0472, 0.2129, 0.2490, 0.0000, 0.0259, 0.0000, 0.1503, 0.3146, 0.1030,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.1000])
proposed candidate after normalizing: [0, tensor(0.2129), tensor(0.2490), 0, 0, 0, tensor(0.1503), tensor(0.3146), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
iteration:  97
input_X:  [0, tensor(0.2129), tensor(0.2490), 0, 0, 0, tensor(0.1503), tensor(0.3146), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  128 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.2129), tensor(0.2490), 0, 0, 0, tensor(0.1503), tensor(0.3146)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.2129)
number of datapoints needed (ratio * total):  1064
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.2490)
number of datapoints needed (ratio * total):  1245
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1503)
number of datapoints needed (ratio * total):  751
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3146)
number of datapoints needed (ratio * total):  1573
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1064
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1245
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 751
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1573
})]
length of training data:  4633
training model...
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
{'loss': 1.9805, 'grad_norm': 1.6885889768600464, 'learning_rate': 0.0002947368421052631, 'epoch': 0.03}
{'loss': 1.777, 'grad_norm': 1.0192186832427979, 'learning_rate': 0.0002842105263157894, 'epoch': 0.07}
{'loss': 1.7057, 'grad_norm': 0.9142763614654541, 'learning_rate': 0.00027368421052631573, 'epoch': 0.1}
{'loss': 1.4834, 'grad_norm': 0.7459768652915955, 'learning_rate': 0.00026315789473684205, 'epoch': 0.14}
{'loss': 1.5372, 'grad_norm': 0.37898555397987366, 'learning_rate': 0.00025263157894736836, 'epoch': 0.17}
{'loss': 1.7634, 'grad_norm': 0.7073065638542175, 'learning_rate': 0.0002421052631578947, 'epoch': 0.21}
{'loss': 1.388, 'grad_norm': 0.4442771375179291, 'learning_rate': 0.00023157894736842101, 'epoch': 0.24}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3798, 'train_samples_per_second': 46.155, 'train_steps_per_second': 5.778, 'train_loss': 1.6528371175130208, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_97/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)], [tensor(0.1048), tensor(0.2542), tensor(0.1399), tensor(0.0014), tensor(0.1959), tensor(8.9785e-18), tensor(0.0656), tensor(0.2383), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9271), tensor(0.1000)], [tensor(0.0750), tensor(0.1359), tensor(0.0973), tensor(7.2963e-18), tensor(0.1508), tensor(0.), tensor(0.1966), tensor(0.3443), tensor(0.0735), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8868), tensor(0.1000)], [tensor(0.1076), tensor(0.0901), tensor(0.1876), tensor(1.2146e-17), tensor(0.0093), tensor(0.1514), tensor(0.1051), tensor(0.3491), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8569), tensor(0.1000)], [tensor(0.1243), tensor(0.1822), tensor(0.1613), tensor(0.), tensor(0.0710), tensor(0.), tensor(0.0471), tensor(0.4140), tensor(0.1262), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8719), tensor(0.1000)], [tensor(0.1240), tensor(0.1111), tensor(0.0808), tensor(0.), tensor(0.0382), tensor(0.), tensor(0.2045), tensor(0.4415), tensor(0.1048), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.2641), tensor(0.1593), tensor(0.1723), tensor(0.0250), tensor(0.1459), tensor(0.), tensor(0.0543), tensor(0.1792), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9208), tensor(0.1000)], [tensor(0.0125), tensor(0.1034), tensor(0.1586), tensor(2.6523e-17), tensor(0.0476), tensor(0.), tensor(0.1975), tensor(0.4803), tensor(0.1028), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0479), tensor(0.1753), tensor(0.0516), tensor(0.), tensor(0.0158), tensor(0.), tensor(0.1699), tensor(0.5395), tensor(0.1016), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0901), tensor(0.0182), tensor(0.2079), tensor(0.), tensor(0.0746), tensor(3.3078e-17), tensor(0.2478), tensor(0.3614), tensor(0.1055), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.1482), tensor(0.0364), tensor(0.1756), tensor(0.), tensor(0.0838), tensor(7.6911e-18), tensor(0.0888), tensor(0.4671), tensor(0.1057), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0330), tensor(0.1602), tensor(0.1234), tensor(4.7689e-17), tensor(0.0679), tensor(0.), tensor(0.3100), tensor(0.3055), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0542), tensor(0.0854), tensor(0.0925), tensor(9.0802e-18), tensor(0.0638), tensor(1.1289e-17), tensor(0.3208), tensor(0.3833), tensor(0.1052), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8652), tensor(0.1000)], [tensor(0.0199), tensor(0.2055), tensor(0.0434), tensor(0.1316), tensor(0.1563), tensor(0.), tensor(0.1739), tensor(0.2694), tensor(0.0355), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9391), tensor(0.1000)], [tensor(0.0472), tensor(0.2129), tensor(0.2490), tensor(0.), tensor(0.0259), tensor(0.), tensor(0.1503), tensor(0.3146), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1780, 0.1970, 0.0868, 0.0000, 0.0610, 0.0077, 0.2266, 0.2429, 0.1260,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8501, 0.0833])
proposed candidate after normalizing: [tensor(0.1780), tensor(0.1970), tensor(0.0868), 0, tensor(0.0610), 0, tensor(0.2266), tensor(0.2429), 4, 1, 1, 1, 1, 1, 109, 0.08332516998052597]
iteration:  98
input_X:  [tensor(0.1780), tensor(0.1970), tensor(0.0868), 0, tensor(0.0610), 0, tensor(0.2266), tensor(0.2429), 4, 1, 1, 1, 1, 1, 109, 0.08332516998052597]
mixing data with method:  random
arranging lora config with parameters:  109 0.08332516998052597 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 29,913,088 || all params: 8,060,174,336 || trainable%: 0.3711
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1780), tensor(0.1970), tensor(0.0868), 0, tensor(0.0610), 0, tensor(0.2266), tensor(0.2429)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1780)
number of datapoints needed (ratio * total):  889
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1970)
number of datapoints needed (ratio * total):  984
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0868)
number of datapoints needed (ratio * total):  433
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0610)
number of datapoints needed (ratio * total):  304
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2266)
number of datapoints needed (ratio * total):  1133
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2429)
number of datapoints needed (ratio * total):  1214
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 889
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 984
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 433
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 304
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1133
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1214
})]
length of training data:  4957
training model...
trainable params: 29,913,088 || all params: 8,060,174,336 || trainable%: 0.3711
{'loss': 1.924, 'grad_norm': 0.6692259907722473, 'learning_rate': 0.0002950819672131147, 'epoch': 0.03}
{'loss': 1.7952, 'grad_norm': 1.7202717065811157, 'learning_rate': 0.00028524590163934424, 'epoch': 0.06}
{'loss': 1.5058, 'grad_norm': 1.4165183305740356, 'learning_rate': 0.00027540983606557377, 'epoch': 0.1}
{'loss': 1.1012, 'grad_norm': 0.44761383533477783, 'learning_rate': 0.00026557377049180324, 'epoch': 0.13}
{'loss': 1.3298, 'grad_norm': 0.7232494354248047, 'learning_rate': 0.00025573770491803277, 'epoch': 0.16}
{'loss': 1.4381, 'grad_norm': 0.552378237247467, 'learning_rate': 0.0002459016393442623, 'epoch': 0.19}
{'loss': 1.2103, 'grad_norm': 0.6480434536933899, 'learning_rate': 0.00023606557377049177, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2926, 'train_samples_per_second': 49.425, 'train_steps_per_second': 6.182, 'train_loss': 1.4527502479793142, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_98/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.57
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)], [tensor(0.1048), tensor(0.2542), tensor(0.1399), tensor(0.0014), tensor(0.1959), tensor(8.9785e-18), tensor(0.0656), tensor(0.2383), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9271), tensor(0.1000)], [tensor(0.0750), tensor(0.1359), tensor(0.0973), tensor(7.2963e-18), tensor(0.1508), tensor(0.), tensor(0.1966), tensor(0.3443), tensor(0.0735), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8868), tensor(0.1000)], [tensor(0.1076), tensor(0.0901), tensor(0.1876), tensor(1.2146e-17), tensor(0.0093), tensor(0.1514), tensor(0.1051), tensor(0.3491), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8569), tensor(0.1000)], [tensor(0.1243), tensor(0.1822), tensor(0.1613), tensor(0.), tensor(0.0710), tensor(0.), tensor(0.0471), tensor(0.4140), tensor(0.1262), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8719), tensor(0.1000)], [tensor(0.1240), tensor(0.1111), tensor(0.0808), tensor(0.), tensor(0.0382), tensor(0.), tensor(0.2045), tensor(0.4415), tensor(0.1048), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.2641), tensor(0.1593), tensor(0.1723), tensor(0.0250), tensor(0.1459), tensor(0.), tensor(0.0543), tensor(0.1792), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9208), tensor(0.1000)], [tensor(0.0125), tensor(0.1034), tensor(0.1586), tensor(2.6523e-17), tensor(0.0476), tensor(0.), tensor(0.1975), tensor(0.4803), tensor(0.1028), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0479), tensor(0.1753), tensor(0.0516), tensor(0.), tensor(0.0158), tensor(0.), tensor(0.1699), tensor(0.5395), tensor(0.1016), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0901), tensor(0.0182), tensor(0.2079), tensor(0.), tensor(0.0746), tensor(3.3078e-17), tensor(0.2478), tensor(0.3614), tensor(0.1055), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.1482), tensor(0.0364), tensor(0.1756), tensor(0.), tensor(0.0838), tensor(7.6911e-18), tensor(0.0888), tensor(0.4671), tensor(0.1057), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0330), tensor(0.1602), tensor(0.1234), tensor(4.7689e-17), tensor(0.0679), tensor(0.), tensor(0.3100), tensor(0.3055), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0542), tensor(0.0854), tensor(0.0925), tensor(9.0802e-18), tensor(0.0638), tensor(1.1289e-17), tensor(0.3208), tensor(0.3833), tensor(0.1052), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8652), tensor(0.1000)], [tensor(0.0199), tensor(0.2055), tensor(0.0434), tensor(0.1316), tensor(0.1563), tensor(0.), tensor(0.1739), tensor(0.2694), tensor(0.0355), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9391), tensor(0.1000)], [tensor(0.0472), tensor(0.2129), tensor(0.2490), tensor(0.), tensor(0.0259), tensor(0.), tensor(0.1503), tensor(0.3146), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.1780), tensor(0.1970), tensor(0.0868), tensor(0.), tensor(0.0610), tensor(0.0077), tensor(0.2266), tensor(0.2429), tensor(0.1260), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8501), tensor(0.0833)]]
proposed candidate before processing: tensor([0.0000, 0.1953, 0.2068, 0.0000, 0.1217, 0.0000, 0.1335, 0.3427, 0.1042,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.1000])
proposed candidate after normalizing: [0, tensor(0.1953), tensor(0.2068), 0, tensor(0.1217), 0, tensor(0.1335), tensor(0.3427), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
iteration:  99
input_X:  [0, tensor(0.1953), tensor(0.2068), 0, tensor(0.1217), 0, tensor(0.1335), tensor(0.3427), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  128 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 33,823,744 || all params: 8,064,084,992 || trainable%: 0.4194
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.1953), tensor(0.2068), 0, tensor(0.1217), 0, tensor(0.1335), tensor(0.3427)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.1953)
number of datapoints needed (ratio * total):  976
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.2068)
number of datapoints needed (ratio * total):  1033
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1217)
number of datapoints needed (ratio * total):  608
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1335)
number of datapoints needed (ratio * total):  667
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.3427)
number of datapoints needed (ratio * total):  1713
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 976
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1033
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 608
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 667
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1713
})]
length of training data:  4997
training model...
trainable params: 33,823,744 || all params: 8,064,084,992 || trainable%: 0.4194
{'loss': 1.9873, 'grad_norm': 0.7928017377853394, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.4898, 'grad_norm': 0.5060979723930359, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.8367, 'grad_norm': 0.748659074306488, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.2824, 'grad_norm': 0.636349618434906, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.442, 'grad_norm': 0.7717525959014893, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.3199, 'grad_norm': 0.5271770358085632, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.4168, 'grad_norm': 1.3863462209701538, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6421, 'train_samples_per_second': 49.651, 'train_steps_per_second': 6.21, 'train_loss': 1.5405253915978758, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_99/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.69
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0673), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1437), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8697), tensor(0.0580)], [tensor(0.1236), tensor(0.1265), tensor(0.0615), tensor(0.0262), tensor(0.1257), tensor(0.0530), tensor(0.2146), tensor(0.2689), tensor(0.0804), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7345), tensor(0.0626)], [tensor(0.1184), tensor(0.1150), tensor(0.0552), tensor(0.1162), tensor(0.1483), tensor(0.1610), tensor(0.1499), tensor(0.1361), tensor(0.1344), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9051), tensor(0.0483)], [tensor(0.1426), tensor(0.1402), tensor(0.0818), tensor(0.0477), tensor(0.0966), tensor(0.0223), tensor(0.1804), tensor(0.2885), tensor(0.2629), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9048), tensor(0.0657)], [tensor(0.1524), tensor(0.1278), tensor(0.1060), tensor(0.0820), tensor(0.0976), tensor(0.0684), tensor(0.1300), tensor(0.2357), tensor(0.1491), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8001), tensor(0.0642)], [tensor(0.1714), tensor(0.1508), tensor(0.1295), tensor(0.0595), tensor(0.1071), tensor(0.0481), tensor(0.1317), tensor(0.2019), tensor(0.1442), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8237), tensor(0.0738)], [tensor(0.1308), tensor(0.0843), tensor(0.1003), tensor(0.1129), tensor(0.0862), tensor(0.1152), tensor(0.1169), tensor(0.2534), tensor(0.1606), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8117), tensor(0.0434)], [tensor(0.1502), tensor(0.1867), tensor(0.0382), tensor(0.0917), tensor(0.1296), tensor(0.0486), tensor(0.1531), tensor(0.2019), tensor(0.1613), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7945), tensor(0.0896)], [tensor(0.1504), tensor(0.1063), tensor(0.1651), tensor(0.0036), tensor(0.1202), tensor(0.0673), tensor(0.1858), tensor(0.2013), tensor(0.1618), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8196), tensor(0.0476)], [tensor(0.1063), tensor(0.1544), tensor(0.1179), tensor(0.0731), tensor(0.0975), tensor(0.0498), tensor(0.1291), tensor(0.2719), tensor(0.1284), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8178), tensor(0.0684)], [tensor(0.0691), tensor(0.1361), tensor(0.1156), tensor(0.0982), tensor(0.1183), tensor(0.0701), tensor(0.1345), tensor(0.2580), tensor(0.1343), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8183), tensor(0.0485)], [tensor(0.1405), tensor(0.0846), tensor(0.1176), tensor(0.0612), tensor(0.0654), tensor(0.1495), tensor(0.1247), tensor(0.2567), tensor(0.1356), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7731), tensor(0.0564)], [tensor(0.1611), tensor(0.0948), tensor(0.1200), tensor(0.0661), tensor(0.1155), tensor(0.0453), tensor(0.1637), tensor(0.2335), tensor(0.1349), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.0433)], [tensor(0.1403), tensor(0.1855), tensor(0.0932), tensor(0.0171), tensor(0.0385), tensor(0.1274), tensor(0.1123), tensor(0.2857), tensor(0.1450), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8198), tensor(0.1000)], [tensor(0.1239), tensor(0.1674), tensor(0.1063), tensor(0.0759), tensor(0.0393), tensor(0.0679), tensor(0.1631), tensor(0.2562), tensor(0.1483), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8154), tensor(0.1000)], [tensor(0.1460), tensor(0.1556), tensor(0.0772), tensor(0.0201), tensor(0.1508), tensor(0.0872), tensor(0.0730), tensor(0.2900), tensor(0.1579), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8165), tensor(0.0801)], [tensor(0.1483), tensor(0.1783), tensor(0.0753), tensor(0.0357), tensor(0.1003), tensor(0.0848), tensor(0.1101), tensor(0.2670), tensor(0.1559), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0224)], [tensor(0.1111), tensor(0.1107), tensor(0.0937), tensor(0.0530), tensor(0.1394), tensor(0.0814), tensor(0.1363), tensor(0.2744), tensor(0.1649), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.1000)], [tensor(0.1664), tensor(0.1386), tensor(0.1198), tensor(0.0840), tensor(0.0973), tensor(0.0472), tensor(0.0966), tensor(0.2502), tensor(0.1286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8257), tensor(0.1000)], [tensor(0.1814), tensor(0.1379), tensor(0.1238), tensor(0.0916), tensor(0.0934), tensor(0.0395), tensor(0.0796), tensor(0.2527), tensor(0.1175), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8294), tensor(0.1000)], [tensor(0.1884), tensor(0.1311), tensor(0.0665), tensor(0.0672), tensor(0.0518), tensor(0.1329), tensor(0.1164), tensor(0.2457), tensor(0.1429), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.1000)], [tensor(0.1573), tensor(0.1516), tensor(0.1515), tensor(0.0945), tensor(0.1109), tensor(0.0125), tensor(0.0671), tensor(0.2546), tensor(0.1100), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8457), tensor(0.1000)], [tensor(0.1552), tensor(0.1515), tensor(0.1560), tensor(0.0940), tensor(0.1112), tensor(0.0147), tensor(0.0644), tensor(0.2531), tensor(0.1111), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8429), tensor(0.1000)], [tensor(0.1448), tensor(0.1390), tensor(0.1303), tensor(0.0771), tensor(0.0968), tensor(0.0702), tensor(0.1080), tensor(0.2338), tensor(0.1475), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8023), tensor(0.1000)], [tensor(0.1774), tensor(0.1357), tensor(0.0925), tensor(0.0865), tensor(0.1041), tensor(9.0700e-19), tensor(0.1101), tensor(0.2937), tensor(0.0771), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8809), tensor(0.1000)], [tensor(0.1794), tensor(0.1346), tensor(0.0901), tensor(0.0846), tensor(0.1019), tensor(0.), tensor(0.1100), tensor(0.2994), tensor(0.0634), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8879), tensor(0.1000)], [tensor(0.1799), tensor(0.1322), tensor(0.0935), tensor(0.1019), tensor(0.1108), tensor(1.3688e-18), tensor(0.0961), tensor(0.2855), tensor(0.1321), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8500), tensor(0.1000)], [tensor(0.1911), tensor(0.1235), tensor(0.0808), tensor(0.1159), tensor(0.1106), tensor(0.), tensor(0.0816), tensor(0.2964), tensor(0.1470), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8453), tensor(0.1000)], [tensor(0.1882), tensor(0.1303), tensor(0.0872), tensor(0.1039), tensor(0.1111), tensor(1.7312e-17), tensor(0.0971), tensor(0.2823), tensor(0.1408), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8533), tensor(0.1000)], [tensor(0.1349), tensor(0.1529), tensor(0.1240), tensor(0.0708), tensor(0.1087), tensor(0.0151), tensor(0.1072), tensor(0.2864), tensor(0.0561), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8582), tensor(0.1000)], [tensor(0.1454), tensor(0.1493), tensor(0.1212), tensor(0.0653), tensor(0.1083), tensor(0.0264), tensor(0.1102), tensor(0.2739), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8735), tensor(0.1000)], [tensor(0.1441), tensor(0.1522), tensor(0.1255), tensor(0.0546), tensor(0.1092), tensor(0.0327), tensor(0.1100), tensor(0.2716), tensor(0.0434), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1452), tensor(0.1533), tensor(0.1282), tensor(0.0473), tensor(0.1093), tensor(0.0387), tensor(0.1086), tensor(0.2694), tensor(0.0234), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9088), tensor(0.1000)], [tensor(0.1710), tensor(0.1430), tensor(0.1161), tensor(0.0919), tensor(0.0427), tensor(0.0740), tensor(0.0856), tensor(0.2756), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8720), tensor(0.0760)], [tensor(0.1634), tensor(0.1384), tensor(0.1138), tensor(0.0812), tensor(0.0611), tensor(0.0701), tensor(0.1033), tensor(0.2686), tensor(0.0831), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8635), tensor(0.0758)], [tensor(0.1435), tensor(0.1619), tensor(0.1365), tensor(0.0674), tensor(0.1030), tensor(0.0275), tensor(0.0791), tensor(0.2812), tensor(0.0102), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8968), tensor(0.1000)], [tensor(0.1520), tensor(0.0939), tensor(0.0981), tensor(0.0907), tensor(0.1682), tensor(0.0746), tensor(0.1090), tensor(0.2134), tensor(0.2707), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.1398), tensor(0.1519), tensor(0.1009), tensor(3.4559e-19), tensor(0.1199), tensor(0.0296), tensor(0.1631), tensor(0.2948), tensor(0.0960), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9544), tensor(0.1000)], [tensor(0.1442), tensor(0.1544), tensor(0.1008), tensor(0.), tensor(0.1232), tensor(0.0335), tensor(0.1531), tensor(0.2908), tensor(0.0957), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9405), tensor(0.1000)], [tensor(0.1566), tensor(0.1835), tensor(0.1151), tensor(0.0324), tensor(0.0795), tensor(0.0017), tensor(0.1695), tensor(0.2616), tensor(0.0942), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8967), tensor(0.1000)], [tensor(0.1367), tensor(0.1075), tensor(0.0941), tensor(0.0168), tensor(0.1496), tensor(0.0673), tensor(0.1055), tensor(0.3226), tensor(0.0955), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8817), tensor(0.1000)], [tensor(0.1098), tensor(0.1353), tensor(0.1138), tensor(0.0922), tensor(0.0741), tensor(0.0180), tensor(0.1636), tensor(0.2932), tensor(0.0944), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9202), tensor(0.1000)], [tensor(0.1848), tensor(0.1958), tensor(0.1062), tensor(2.3785e-18), tensor(0.1426), tensor(0.0418), tensor(0.1060), tensor(0.2227), tensor(0.0880), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8696), tensor(0.1000)], [tensor(0.1723), tensor(0.0949), tensor(0.1357), tensor(0.0448), tensor(0.0597), tensor(0.0211), tensor(0.1395), tensor(0.3320), tensor(0.0964), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8722), tensor(0.1000)], [tensor(0.1166), tensor(0.1184), tensor(0.1480), tensor(1.0273e-17), tensor(0.0829), tensor(0.0853), tensor(0.1902), tensor(0.2587), tensor(0.0597), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9874), tensor(0.1000)], [tensor(0.0862), tensor(0.1868), tensor(0.1546), tensor(0.0134), tensor(0.0776), tensor(0.0160), tensor(0.1040), tensor(0.3615), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8892), tensor(0.1000)], [tensor(0.0568), tensor(0.2166), tensor(0.1670), tensor(3.8625e-19), tensor(0.0613), tensor(0.0031), tensor(0.0854), tensor(0.4098), tensor(0.0791), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8842), tensor(0.1000)], [tensor(0.0721), tensor(0.2182), tensor(0.1098), tensor(0.), tensor(0.0654), tensor(0.0364), tensor(0.1061), tensor(0.3920), tensor(0.0717), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9068), tensor(0.1000)], [tensor(0.0782), tensor(0.1567), tensor(0.2298), tensor(0.), tensor(0.0870), tensor(8.3463e-18), tensor(0.0929), tensor(0.3554), tensor(0.0724), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8784), tensor(0.1000)], [tensor(0.0940), tensor(0.2135), tensor(0.1208), tensor(0.0081), tensor(0.0627), tensor(0.), tensor(0.0676), tensor(0.4333), tensor(0.1098), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8847), tensor(0.1000)], [tensor(0.1431), tensor(0.2067), tensor(0.1298), tensor(1.2880e-17), tensor(0.0073), tensor(1.3375e-18), tensor(0.1024), tensor(0.4107), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8948), tensor(0.1000)], [tensor(0.1629), tensor(0.1934), tensor(0.1223), tensor(0.), tensor(0.), tensor(1.4908e-19), tensor(0.0800), tensor(0.4414), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9144), tensor(0.1000)], [tensor(0.0543), tensor(0.2659), tensor(0.1477), tensor(0.0216), tensor(0.0467), tensor(0.), tensor(0.1420), tensor(0.3218), tensor(0.0699), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8692), tensor(0.1000)], [tensor(0.1038), tensor(0.1750), tensor(0.1366), tensor(5.2956e-18), tensor(0.0901), tensor(3.9827e-18), tensor(0.0919), tensor(0.4026), tensor(0.1200), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9259), tensor(0.1000)], [tensor(0.1430), tensor(0.1418), tensor(0.1205), tensor(0.0245), tensor(0.1227), tensor(0.0605), tensor(0.1410), tensor(0.2460), tensor(0.0322), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9113), tensor(0.0992)], [tensor(0.1186), tensor(0.1725), tensor(0.0117), tensor(0.0730), tensor(0.0150), tensor(0.0671), tensor(0.0844), tensor(0.4577), tensor(0.1276), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7956), tensor(0.0813)], [tensor(0.1649), tensor(0.0615), tensor(0.0436), tensor(0.0565), tensor(0.0807), tensor(0.1065), tensor(0.0967), tensor(0.3895), tensor(0.1477), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8986), tensor(0.0642)], [tensor(0.0475), tensor(0.2079), tensor(0.1635), tensor(0.0024), tensor(0.0900), tensor(3.3881e-21), tensor(0.1924), tensor(0.2963), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8092), tensor(0.0839)], [tensor(0.0630), tensor(0.1441), tensor(0.1103), tensor(0.), tensor(0.0635), tensor(0.1769), tensor(0.1730), tensor(0.2691), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8014), tensor(0.0967)], [tensor(0.0951), tensor(0.2135), tensor(0.2071), tensor(0.0308), tensor(0.0792), tensor(0.0180), tensor(0.0493), tensor(0.3071), tensor(0.1264), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8541), tensor(0.0811)], [tensor(0.0765), tensor(0.0829), tensor(0.0836), tensor(0.0540), tensor(0.1522), tensor(1.4908e-18), tensor(0.2367), tensor(0.3141), tensor(0.1248), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8461), tensor(0.0865)], [tensor(0.1290), tensor(0.3175), tensor(0.1028), tensor(0.), tensor(2.8872e-06), tensor(0.0483), tensor(0.1593), tensor(0.2429), tensor(0.1261), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7713), tensor(0.0846)], [tensor(0.0298), tensor(0.2238), tensor(0.1589), tensor(0.0258), tensor(0.0684), tensor(0.0966), tensor(0.1126), tensor(0.2842), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.6840), tensor(0.0836)], [tensor(0.1879), tensor(0.2435), tensor(0.1306), tensor(0.0071), tensor(3.5413e-17), tensor(1.3674e-17), tensor(0.1588), tensor(0.2720), tensor(0.1253), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9642), tensor(0.0852)], [tensor(0.1484), tensor(0.2826), tensor(0.0677), tensor(0.), tensor(0.1474), tensor(7.3980e-18), tensor(0.0746), tensor(0.2793), tensor(0.1255), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8265), tensor(0.0857)], [tensor(0.0660), tensor(0.1449), tensor(0.2099), tensor(0.1447), tensor(1.1797e-17), tensor(1.9732e-17), tensor(0.1496), tensor(0.2849), tensor(0.1259), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8379), tensor(0.0859)], [tensor(0.1085), tensor(0.1622), tensor(0.0489), tensor(0.0584), tensor(0.1512), tensor(0.0572), tensor(0.1625), tensor(0.2511), tensor(0.1479), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7765), tensor(0.0603)], [tensor(0.1801), tensor(0.1437), tensor(0.1682), tensor(0.), tensor(0.), tensor(0.0479), tensor(0.1068), tensor(0.3534), tensor(0.1227), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7777), tensor(0.1000)], [tensor(0.), tensor(0.2851), tensor(0.1207), tensor(3.0785e-17), tensor(0.0085), tensor(0.0808), tensor(0.1914), tensor(0.3135), tensor(0.1193), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8705), tensor(0.1000)], [tensor(3.3282e-17), tensor(0.1494), tensor(0.1207), tensor(0.0603), tensor(0.1382), tensor(0.1313), tensor(0.0814), tensor(0.3186), tensor(0.0736), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9382), tensor(0.1000)], [tensor(0.1131), tensor(0.2124), tensor(0.1495), tensor(1.6910e-17), tensor(0.1139), tensor(0.1213), tensor(0.0167), tensor(0.2731), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7989), tensor(0.1000)], [tensor(0.0590), tensor(0.1566), tensor(0.1698), tensor(1.0236e-18), tensor(0.0633), tensor(0.0412), tensor(0.2346), tensor(0.2755), tensor(0.0749), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9600), tensor(0.1000)], [tensor(0.0124), tensor(0.1169), tensor(0.1679), tensor(0.1010), tensor(0.0257), tensor(0.0350), tensor(0.2175), tensor(0.3237), tensor(0.0739), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8708), tensor(0.1000)], [tensor(0.2385), tensor(0.0945), tensor(0.1793), tensor(0.0758), tensor(0.0276), tensor(0.0988), tensor(0.0819), tensor(0.2036), tensor(0.1481), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9854), tensor(0.0615)], [tensor(0.), tensor(0.0655), tensor(0.1654), tensor(0.0271), tensor(0.0488), tensor(0.0518), tensor(0.2401), tensor(0.4013), tensor(0.0745), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.0306), tensor(0.2087), tensor(0.1890), tensor(0.1272), tensor(0.0149), tensor(0.0506), tensor(0.1637), tensor(0.2152), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9192), tensor(0.1000)], [tensor(0.1440), tensor(0.1691), tensor(0.1376), tensor(0.0885), tensor(0.0724), tensor(0.), tensor(0.0462), tensor(0.3421), tensor(0.0354), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8837), tensor(0.1000)], [tensor(0.1407), tensor(0.1754), tensor(0.1248), tensor(0.0872), tensor(0.1649), tensor(0.), tensor(0.1242), tensor(0.1827), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8333), tensor(0.1000)], [tensor(0.1268), tensor(0.1333), tensor(0.1387), tensor(0.0450), tensor(0.1637), tensor(0.), tensor(0.1664), tensor(0.2260), tensor(0.0363), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7400), tensor(0.1000)], [tensor(0.1649), tensor(0.2185), tensor(0.1039), tensor(0.1206), tensor(0.1633), tensor(0.0008), tensor(0.0806), tensor(0.1474), tensor(0.0359), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9727), tensor(0.1000)], [tensor(0.1229), tensor(0.1835), tensor(0.1172), tensor(0.1359), tensor(0.0986), tensor(0.1279), tensor(0.0671), tensor(0.1468), tensor(0.0360), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8733), tensor(0.1000)], [tensor(0.1048), tensor(0.2542), tensor(0.1399), tensor(0.0014), tensor(0.1959), tensor(8.9785e-18), tensor(0.0656), tensor(0.2383), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9271), tensor(0.1000)], [tensor(0.0750), tensor(0.1359), tensor(0.0973), tensor(7.2963e-18), tensor(0.1508), tensor(0.), tensor(0.1966), tensor(0.3443), tensor(0.0735), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8868), tensor(0.1000)], [tensor(0.1076), tensor(0.0901), tensor(0.1876), tensor(1.2146e-17), tensor(0.0093), tensor(0.1514), tensor(0.1051), tensor(0.3491), tensor(0.0729), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8569), tensor(0.1000)], [tensor(0.1243), tensor(0.1822), tensor(0.1613), tensor(0.), tensor(0.0710), tensor(0.), tensor(0.0471), tensor(0.4140), tensor(0.1262), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8719), tensor(0.1000)], [tensor(0.1240), tensor(0.1111), tensor(0.0808), tensor(0.), tensor(0.0382), tensor(0.), tensor(0.2045), tensor(0.4415), tensor(0.1048), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.2641), tensor(0.1593), tensor(0.1723), tensor(0.0250), tensor(0.1459), tensor(0.), tensor(0.0543), tensor(0.1792), tensor(0.0357), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9208), tensor(0.1000)], [tensor(0.0125), tensor(0.1034), tensor(0.1586), tensor(2.6523e-17), tensor(0.0476), tensor(0.), tensor(0.1975), tensor(0.4803), tensor(0.1028), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0479), tensor(0.1753), tensor(0.0516), tensor(0.), tensor(0.0158), tensor(0.), tensor(0.1699), tensor(0.5395), tensor(0.1016), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0901), tensor(0.0182), tensor(0.2079), tensor(0.), tensor(0.0746), tensor(3.3078e-17), tensor(0.2478), tensor(0.3614), tensor(0.1055), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.1482), tensor(0.0364), tensor(0.1756), tensor(0.), tensor(0.0838), tensor(7.6911e-18), tensor(0.0888), tensor(0.4671), tensor(0.1057), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0330), tensor(0.1602), tensor(0.1234), tensor(4.7689e-17), tensor(0.0679), tensor(0.), tensor(0.3100), tensor(0.3055), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.0542), tensor(0.0854), tensor(0.0925), tensor(9.0802e-18), tensor(0.0638), tensor(1.1289e-17), tensor(0.3208), tensor(0.3833), tensor(0.1052), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8652), tensor(0.1000)], [tensor(0.0199), tensor(0.2055), tensor(0.0434), tensor(0.1316), tensor(0.1563), tensor(0.), tensor(0.1739), tensor(0.2694), tensor(0.0355), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9391), tensor(0.1000)], [tensor(0.0472), tensor(0.2129), tensor(0.2490), tensor(0.), tensor(0.0259), tensor(0.), tensor(0.1503), tensor(0.3146), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)], [tensor(0.1780), tensor(0.1970), tensor(0.0868), tensor(0.), tensor(0.0610), tensor(0.0077), tensor(0.2266), tensor(0.2429), tensor(0.1260), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8501), tensor(0.0833)], [tensor(0.), tensor(0.1953), tensor(0.2068), tensor(0.), tensor(0.1217), tensor(0.), tensor(0.1335), tensor(0.3427), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.1000)]]
proposed candidate before processing: tensor([0.0000e+00, 2.0115e-01, 2.0046e-01, 0.0000e+00, 1.6691e-01, 2.6726e-17,
        1.0380e-01, 3.2768e-01, 1.0459e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e-01])
proposed candidate after normalizing: [0, tensor(0.2012), tensor(0.2005), 0, tensor(0.1669), 0, tensor(0.1038), tensor(0.3277), 3, 1, 1, 1, 1, 1, 128, 0.10000000149011612]
Best at every step: [0.37, 0.37, 0.63, 0.68, 0.68, 0.68, 0.68, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7]
running BO on both data and model with fixed feature list
commonsense_qa
headqa_en
hellaswag
pubmedqa
sciq
triviaqa
truthfulqa_gen
wikitext
fixed feature list generated:
{9: 0, 10: 0, 11: 0, 12: 0, 13: 0}
{9: 0, 10: 0, 11: 0, 12: 0, 13: 1}
{9: 0, 10: 0, 11: 0, 12: 1, 13: 0}
{9: 0, 10: 0, 11: 0, 12: 1, 13: 1}
{9: 0, 10: 0, 11: 1, 12: 0, 13: 0}
{9: 0, 10: 0, 11: 1, 12: 0, 13: 1}
{9: 0, 10: 0, 11: 1, 12: 1, 13: 0}
{9: 0, 10: 0, 11: 1, 12: 1, 13: 1}
{9: 0, 10: 1, 11: 0, 12: 0, 13: 0}
{9: 0, 10: 1, 11: 0, 12: 0, 13: 1}
{9: 0, 10: 1, 11: 0, 12: 1, 13: 0}
{9: 0, 10: 1, 11: 0, 12: 1, 13: 1}
{9: 0, 10: 1, 11: 1, 12: 0, 13: 0}
{9: 0, 10: 1, 11: 1, 12: 0, 13: 1}
{9: 0, 10: 1, 11: 1, 12: 1, 13: 0}
{9: 0, 10: 1, 11: 1, 12: 1, 13: 1}
{9: 1, 10: 0, 11: 0, 12: 0, 13: 0}
{9: 1, 10: 0, 11: 0, 12: 0, 13: 1}
{9: 1, 10: 0, 11: 0, 12: 1, 13: 0}
{9: 1, 10: 0, 11: 0, 12: 1, 13: 1}
{9: 1, 10: 0, 11: 1, 12: 0, 13: 0}
{9: 1, 10: 0, 11: 1, 12: 0, 13: 1}
{9: 1, 10: 0, 11: 1, 12: 1, 13: 0}
{9: 1, 10: 0, 11: 1, 12: 1, 13: 1}
{9: 1, 10: 1, 11: 0, 12: 0, 13: 0}
{9: 1, 10: 1, 11: 0, 12: 0, 13: 1}
{9: 1, 10: 1, 11: 0, 12: 1, 13: 0}
{9: 1, 10: 1, 11: 0, 12: 1, 13: 1}
{9: 1, 10: 1, 11: 1, 12: 0, 13: 0}
{9: 1, 10: 1, 11: 1, 12: 0, 13: 1}
{9: 1, 10: 1, 11: 1, 12: 1, 13: 0}
{9: 1, 10: 1, 11: 1, 12: 1, 13: 1}
iteration:  0
input_X:  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 16, 1, 1, 1, 1, 1, 72, 0.05]
mixing data with method:  random
arranging lora config with parameters:  72 0.05 16 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 79,036,416 || all params: 8,109,297,664 || trainable%: 0.9746
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
})]
length of training data:  5000
training model...
trainable params: 79,036,416 || all params: 8,109,297,664 || trainable%: 0.9746
{'loss': 1.8494, 'grad_norm': 0.8403694033622742, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.2571, 'grad_norm': 0.6450926661491394, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 0.8298, 'grad_norm': 0.45925360918045044, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 0.9204, 'grad_norm': 0.6809854507446289, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.0549, 'grad_norm': 0.6768918037414551, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 0.6906, 'grad_norm': 0.7942788004875183, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3215, 'train_samples_per_second': 49.84, 'train_steps_per_second': 6.23, 'train_loss': 1.0695528628221198, 'epoch': 0.21}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_0/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.35
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05]]
proposed candidate before processing: tensor([0.1320, 0.0412, 0.0666, 0.3804, 0.0453, 0.0872, 0.0189, 0.2285, 0.7649,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0406, 0.0897])
proposed candidate after normalizing: [tensor(0.1320), 0, tensor(0.0666), tensor(0.3804), 0, tensor(0.0872), 0, tensor(0.2285), 24, 0, 0, 0, 0, 0, 5, 0.08970030397176743]
iteration:  1
input_X:  [tensor(0.1320), 0, tensor(0.0666), tensor(0.3804), 0, tensor(0.0872), 0, tensor(0.2285), 24, 0, 0, 0, 0, 0, 5, 0.08970030397176743]
mixing data with method:  random
arranging lora config with parameters:  5 0.08970030397176743 24 [0, 0, 0, 0, 0]
[]
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)]]
proposed candidate before processing: tensor([0.1265, 0.1256, 0.0871, 0.0900, 0.1257, 0.1015, 0.1582, 0.1853, 0.2839,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7487, 0.0549])
proposed candidate after normalizing: [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), 9, 1, 1, 1, 1, 1, 96, 0.05487089604139328]
iteration:  2
input_X:  [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), 9, 1, 1, 1, 1, 1, 96, 0.05487089604139328]
mixing data with method:  random
arranging lora config with parameters:  96 0.05487089604139328 9 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 93,855,744 || all params: 8,124,116,992 || trainable%: 1.1553
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1265)
number of datapoints needed (ratio * total):  632
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1256)
number of datapoints needed (ratio * total):  628
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0871)
number of datapoints needed (ratio * total):  435
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0900)
number of datapoints needed (ratio * total):  450
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1257)
number of datapoints needed (ratio * total):  628
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1015)
number of datapoints needed (ratio * total):  507
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1582)
number of datapoints needed (ratio * total):  791
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1853)
number of datapoints needed (ratio * total):  926
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 632
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 628
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 435
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 450
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 628
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 507
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 791
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 926
})]
length of training data:  4997
training model...
trainable params: 93,855,744 || all params: 8,124,116,992 || trainable%: 1.1553
{'loss': 1.4094, 'grad_norm': 0.5916219353675842, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 0.9931, 'grad_norm': 0.8628707528114319, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.4704, 'grad_norm': 0.6487346291542053, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.257, 'grad_norm': 0.7965716123580933, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3444, 'grad_norm': 0.4403916299343109, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1034, 'grad_norm': 0.7152658104896545, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
⏰ Max training time of 100 seconds reached. Stopping.
{'loss': 1.2846, 'grad_norm': 0.5173695087432861, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'train_runtime': 100.7279, 'train_samples_per_second': 49.609, 'train_steps_per_second': 6.205, 'train_loss': 1.266051823752267, 'epoch': 0.22}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_2/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.6
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)]]
proposed candidate before processing: tensor([0.1274, 0.1260, 0.0626, 0.0674, 0.1261, 0.0863, 0.1798, 0.2245, 0.1441,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8695, 0.0580])
proposed candidate after normalizing: [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), 5, 1, 1, 1, 1, 1, 111, 0.05801589787006378]
iteration:  3
input_X:  [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), 5, 1, 1, 1, 1, 1, 111, 0.05801589787006378]
mixing data with method:  random
arranging lora config with parameters:  111 0.05801589787006378 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 64,422,912 || all params: 8,094,684,160 || trainable%: 0.7959
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1274)
number of datapoints needed (ratio * total):  636
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1260)
number of datapoints needed (ratio * total):  629
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0626)
number of datapoints needed (ratio * total):  312
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0674)
number of datapoints needed (ratio * total):  336
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1261)
number of datapoints needed (ratio * total):  630
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0863)
number of datapoints needed (ratio * total):  431
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1798)
number of datapoints needed (ratio * total):  898
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2245)
number of datapoints needed (ratio * total):  1122
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 636
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 629
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 312
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 336
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 630
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 431
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 898
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1122
})]
length of training data:  4994
training model...
trainable params: 64,422,912 || all params: 8,094,684,160 || trainable%: 0.7959
{'loss': 2.1017, 'grad_norm': 1.369253396987915, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.267, 'grad_norm': 0.4059525728225708, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.2639, 'grad_norm': 0.502894937992096, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.1624, 'grad_norm': 1.6658000946044922, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.0158, 'grad_norm': 0.5095681548118591, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.2946, 'grad_norm': 0.522628903388977, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.2392, 'grad_norm': 0.628652036190033, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3575, 'train_samples_per_second': 49.762, 'train_steps_per_second': 6.228, 'train_loss': 1.3381229972839355, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_3/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)]]
proposed candidate before processing: tensor([0.0820, 0.0924, 0.0352, 0.0142, 0.1860, 0.1108, 0.2588, 0.2205, 0.0900,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8135, 0.0557])
proposed candidate after normalizing: [tensor(0.0820), tensor(0.0924), 0, 0, tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), 3, 1, 1, 1, 1, 1, 104, 0.055730246007442474]
iteration:  4
input_X:  [tensor(0.0820), tensor(0.0924), 0, 0, tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), 3, 1, 1, 1, 1, 1, 104, 0.055730246007442474]
mixing data with method:  random
arranging lora config with parameters:  104 0.055730246007442474 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 36,636,672 || all params: 8,066,897,920 || trainable%: 0.4542
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0820), tensor(0.0924), 0, 0, tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0820)
number of datapoints needed (ratio * total):  409
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0924)
number of datapoints needed (ratio * total):  462
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1860)
number of datapoints needed (ratio * total):  930
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1108)
number of datapoints needed (ratio * total):  554
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2588)
number of datapoints needed (ratio * total):  1294
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2205)
number of datapoints needed (ratio * total):  1102
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 409
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 462
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 930
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 554
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1294
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1102
})]
length of training data:  4751
training model...
trainable params: 36,636,672 || all params: 8,066,897,920 || trainable%: 0.4542
{'loss': 1.487, 'grad_norm': 0.48382729291915894, 'learning_rate': 0.00029486301369863015, 'epoch': 0.03}
{'loss': 1.4568, 'grad_norm': 0.8500563502311707, 'learning_rate': 0.0002845890410958904, 'epoch': 0.07}
{'loss': 1.3908, 'grad_norm': 0.6682724952697754, 'learning_rate': 0.0002743150684931507, 'epoch': 0.1}
{'loss': 1.4612, 'grad_norm': 0.56362384557724, 'learning_rate': 0.0002640410958904109, 'epoch': 0.13}
{'loss': 1.3487, 'grad_norm': 0.41825583577156067, 'learning_rate': 0.0002537671232876712, 'epoch': 0.17}
{'loss': 1.2849, 'grad_norm': 0.5628860592842102, 'learning_rate': 0.0002434931506849315, 'epoch': 0.2}
{'loss': 1.3433, 'grad_norm': 1.2648950815200806, 'learning_rate': 0.00023321917808219177, 'epoch': 0.24}
{'loss': 1.2805, 'grad_norm': 0.535114049911499, 'learning_rate': 0.00022294520547945203, 'epoch': 0.27}
{'loss': 1.1155, 'grad_norm': 0.7479642629623413, 'learning_rate': 0.0002126712328767123, 'epoch': 0.3}
{'loss': 1.3817, 'grad_norm': 0.9398815035820007, 'learning_rate': 0.0002023972602739726, 'epoch': 0.34}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3694, 'train_samples_per_second': 47.335, 'train_steps_per_second': 5.918, 'train_loss': 1.3345451984765395, 'epoch': 0.36}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_4/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)]]
proposed candidate before processing: tensor([0.1516, 0.1312, 0.0480, 0.0335, 0.1182, 0.0711, 0.1538, 0.2926, 0.0667,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7729, 0.0654])
proposed candidate after normalizing: [tensor(0.1516), tensor(0.1312), 0, 0, tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), 2, 1, 1, 1, 1, 1, 99, 0.0654030293226242]
iteration:  5
input_X:  [tensor(0.1516), tensor(0.1312), 0, 0, tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), 2, 1, 1, 1, 1, 1, 99, 0.0654030293226242]
mixing data with method:  random
arranging lora config with parameters:  99 0.0654030293226242 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 20,719,616 || all params: 8,050,980,864 || trainable%: 0.2574
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1516), tensor(0.1312), 0, 0, tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1516)
number of datapoints needed (ratio * total):  758
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1312)
number of datapoints needed (ratio * total):  656
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1182)
number of datapoints needed (ratio * total):  590
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0711)
number of datapoints needed (ratio * total):  355
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1538)
number of datapoints needed (ratio * total):  768
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2926)
number of datapoints needed (ratio * total):  1462
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 758
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 656
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 590
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 355
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 768
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1462
})]
length of training data:  4589
training model...
trainable params: 20,719,616 || all params: 8,050,980,864 || trainable%: 0.2574
{'loss': 2.0406, 'grad_norm': 0.7481011152267456, 'learning_rate': 0.0002946808510638298, 'epoch': 0.03}
{'loss': 1.9875, 'grad_norm': 0.46688079833984375, 'learning_rate': 0.00028404255319148934, 'epoch': 0.07}
{'loss': 1.9253, 'grad_norm': 1.6693943738937378, 'learning_rate': 0.00027340425531914895, 'epoch': 0.1}
{'loss': 1.793, 'grad_norm': 0.9835809469223022, 'learning_rate': 0.0002627659574468085, 'epoch': 0.14}
{'loss': 1.8737, 'grad_norm': 0.45898228883743286, 'learning_rate': 0.00025212765957446806, 'epoch': 0.17}
{'loss': 1.3956, 'grad_norm': 0.5599034428596497, 'learning_rate': 0.00024148936170212765, 'epoch': 0.21}
{'loss': 1.5166, 'grad_norm': 0.5005685687065125, 'learning_rate': 0.0002308510638297872, 'epoch': 0.24}
{'loss': 1.3667, 'grad_norm': 0.542273998260498, 'learning_rate': 0.00022021276595744679, 'epoch': 0.28}
{'loss': 1.6212, 'grad_norm': 1.5613223314285278, 'learning_rate': 0.00020957446808510634, 'epoch': 0.31}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4289, 'train_samples_per_second': 45.694, 'train_steps_per_second': 5.715, 'train_loss': 1.7241084759051983, 'epoch': 0.32}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_5/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.59
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)]]
proposed candidate before processing: tensor([0.0830, 0.1132, 0.0681, 0.1023, 0.1554, 0.1093, 0.2417, 0.1268, 0.1365,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9347, 0.0473])
proposed candidate after normalizing: [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), 4, 1, 1, 1, 1, 1, 120, 0.0472651943564415]
iteration:  6
input_X:  [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), 4, 1, 1, 1, 1, 1, 120, 0.0472651943564415]
mixing data with method:  random
arranging lora config with parameters:  120 0.0472651943564415 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 32,931,840 || all params: 8,063,193,088 || trainable%: 0.4084
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0830)
number of datapoints needed (ratio * total):  415
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1132)
number of datapoints needed (ratio * total):  566
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0681)
number of datapoints needed (ratio * total):  340
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1023)
number of datapoints needed (ratio * total):  511
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1554)
number of datapoints needed (ratio * total):  777
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1093)
number of datapoints needed (ratio * total):  546
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2417)
number of datapoints needed (ratio * total):  1208
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1268)
number of datapoints needed (ratio * total):  634
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 415
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 566
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 340
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 511
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 777
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 546
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1208
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 634
})]
length of training data:  4997
training model...
trainable params: 32,931,840 || all params: 8,063,193,088 || trainable%: 0.4084
{'loss': 1.8656, 'grad_norm': 2.043602466583252, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.247, 'grad_norm': 1.086503267288208, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.4215, 'grad_norm': 0.7528549432754517, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.016, 'grad_norm': 0.6576977968215942, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3231, 'grad_norm': 0.5678002834320068, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1952, 'grad_norm': 0.48172125220298767, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 0.8234, 'grad_norm': 0.6045849919319153, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1933, 'train_samples_per_second': 49.874, 'train_steps_per_second': 6.238, 'train_loss': 1.2541437088304264, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_6/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.58
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)]]
proposed candidate before processing: tensor([1.0581e-01, 9.7421e-02, 4.3349e-02, 1.4934e-19, 1.6115e-01, 1.0144e-01,
        2.1682e-01, 2.7401e-01, 2.2827e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.9426e-01, 6.1186e-02])
proposed candidate after normalizing: [tensor(0.1058), tensor(0.0974), 0, 0, tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), 7, 1, 1, 1, 1, 1, 114, 0.06118599325418472]
iteration:  7
input_X:  [tensor(0.1058), tensor(0.0974), 0, 0, tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), 7, 1, 1, 1, 1, 1, 114, 0.06118599325418472]
mixing data with method:  random
arranging lora config with parameters:  114 0.06118599325418472 7 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 54,749,184 || all params: 8,085,010,432 || trainable%: 0.6772
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1058), tensor(0.0974), 0, 0, tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1058)
number of datapoints needed (ratio * total):  529
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0974)
number of datapoints needed (ratio * total):  487
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1611)
number of datapoints needed (ratio * total):  805
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1014)
number of datapoints needed (ratio * total):  507
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2168)
number of datapoints needed (ratio * total):  1084
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2740)
number of datapoints needed (ratio * total):  1370
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 529
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 487
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 805
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 507
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1084
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1370
})]
length of training data:  4782
training model...
trainable params: 54,749,184 || all params: 8,085,010,432 || trainable%: 0.6772
{'loss': 2.2393, 'grad_norm': 0.5434313416481018, 'learning_rate': 0.0002948979591836734, 'epoch': 0.03}
{'loss': 1.6526, 'grad_norm': 0.7708089351654053, 'learning_rate': 0.0002846938775510204, 'epoch': 0.07}
{'loss': 1.6918, 'grad_norm': 0.5139864683151245, 'learning_rate': 0.00027448979591836734, 'epoch': 0.1}
{'loss': 1.7672, 'grad_norm': 1.197798490524292, 'learning_rate': 0.00026428571428571424, 'epoch': 0.13}
{'loss': 1.6882, 'grad_norm': 1.200602650642395, 'learning_rate': 0.0002540816326530612, 'epoch': 0.17}
{'loss': 1.2973, 'grad_norm': 0.7683371901512146, 'learning_rate': 0.00024387755102040816, 'epoch': 0.2}
{'loss': 1.2026, 'grad_norm': 0.4756166338920593, 'learning_rate': 0.00023367346938775506, 'epoch': 0.23}
{'loss': 0.9933, 'grad_norm': 0.5018165111541748, 'learning_rate': 0.00022346938775510205, 'epoch': 0.27}
{'loss': 1.3277, 'grad_norm': 0.6154284477233887, 'learning_rate': 0.00021326530612244898, 'epoch': 0.3}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2224, 'train_samples_per_second': 47.714, 'train_steps_per_second': 5.967, 'train_loss': 1.5410238115411057, 'epoch': 0.32}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_7/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.53
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)]]
proposed candidate before processing: tensor([0.1176, 0.1303, 0.0702, 0.0945, 0.1347, 0.0942, 0.1923, 0.1663, 0.0738,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7685, 0.0528])
proposed candidate after normalizing: [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), 2, 1, 1, 1, 1, 1, 98, 0.05275414511561394]
iteration:  8
input_X:  [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), 2, 1, 1, 1, 1, 1, 98, 0.05275414511561394]
mixing data with method:  random
arranging lora config with parameters:  98 0.05275414511561394 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 52,553,728 || all params: 8,082,814,976 || trainable%: 0.6502
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1176)
number of datapoints needed (ratio * total):  587
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1303)
number of datapoints needed (ratio * total):  651
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0702)
number of datapoints needed (ratio * total):  351
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0945)
number of datapoints needed (ratio * total):  472
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1347)
number of datapoints needed (ratio * total):  673
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0942)
number of datapoints needed (ratio * total):  471
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1923)
number of datapoints needed (ratio * total):  961
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1663)
number of datapoints needed (ratio * total):  831
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 587
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 651
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 351
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 472
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 673
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 471
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 961
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 831
})]
length of training data:  4997
training model...
trainable params: 52,553,728 || all params: 8,082,814,976 || trainable%: 0.6502
{'loss': 1.4062, 'grad_norm': 0.5818895697593689, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 0.956, 'grad_norm': 0.6384141445159912, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3196, 'grad_norm': 0.641528844833374, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.1415, 'grad_norm': 0.5109559297561646, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3335, 'grad_norm': 0.8277748823165894, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1923, 'grad_norm': 0.9321521520614624, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.1628, 'grad_norm': 0.40919628739356995, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1546, 'train_samples_per_second': 49.893, 'train_steps_per_second': 6.24, 'train_loss': 1.2009213383992512, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_8/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)]]
proposed candidate before processing: tensor([0.0978, 0.1001, 0.0329, 0.1305, 0.1403, 0.0828, 0.1987, 0.2169, 0.0953,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8060, 0.0575])
proposed candidate after normalizing: [tensor(0.0978), tensor(0.1001), 0, tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), 3, 1, 1, 1, 1, 1, 103, 0.05751367285847664]
iteration:  9
input_X:  [tensor(0.0978), tensor(0.1001), 0, tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), 3, 1, 1, 1, 1, 1, 103, 0.05751367285847664]
mixing data with method:  random
arranging lora config with parameters:  103 0.05751367285847664 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 21,199,872 || all params: 8,051,461,120 || trainable%: 0.2633
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0978), tensor(0.1001), 0, tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0978)
number of datapoints needed (ratio * total):  489
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1001)
number of datapoints needed (ratio * total):  500
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1305)
number of datapoints needed (ratio * total):  652
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1403)
number of datapoints needed (ratio * total):  701
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0828)
number of datapoints needed (ratio * total):  413
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1987)
number of datapoints needed (ratio * total):  993
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2169)
number of datapoints needed (ratio * total):  1084
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 489
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 500
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 652
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 701
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 413
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 993
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1084
})]
length of training data:  4832
training model...
trainable params: 21,199,872 || all params: 8,051,461,120 || trainable%: 0.2633
{'loss': 2.2346, 'grad_norm': 0.8690738081932068, 'learning_rate': 0.00029494949494949493, 'epoch': 0.03}
{'loss': 1.3884, 'grad_norm': 0.9477090835571289, 'learning_rate': 0.0002848484848484848, 'epoch': 0.07}
{'loss': 1.4469, 'grad_norm': 0.9283853769302368, 'learning_rate': 0.0002747474747474747, 'epoch': 0.1}
{'loss': 1.5834, 'grad_norm': 1.223717212677002, 'learning_rate': 0.00026464646464646464, 'epoch': 0.13}
{'loss': 1.6149, 'grad_norm': 0.7340396642684937, 'learning_rate': 0.0002545454545454545, 'epoch': 0.17}
{'loss': 1.2369, 'grad_norm': 1.2124871015548706, 'learning_rate': 0.00024444444444444443, 'epoch': 0.2}
{'loss': 1.6457, 'grad_norm': 0.5947529077529907, 'learning_rate': 0.00023434343434343432, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1571, 'train_samples_per_second': 48.244, 'train_steps_per_second': 6.031, 'train_loss': 1.5617457974341609, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_9/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)]]
proposed candidate before processing: tensor([0.1351, 0.1722, 0.0984, 0.0117, 0.1208, 0.0804, 0.2026, 0.1788, 0.1014,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8095, 0.0504])
proposed candidate after normalizing: [tensor(0.1351), tensor(0.1722), tensor(0.0984), 0, tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), 3, 1, 1, 1, 1, 1, 104, 0.05036899074912071]
iteration:  10
input_X:  [tensor(0.1351), tensor(0.1722), tensor(0.0984), 0, tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), 3, 1, 1, 1, 1, 1, 104, 0.05036899074912071]
mixing data with method:  random
arranging lora config with parameters:  104 0.05036899074912071 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 21,405,696 || all params: 8,051,666,944 || trainable%: 0.2659
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1351), tensor(0.1722), tensor(0.0984), 0, tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1351)
number of datapoints needed (ratio * total):  675
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1722)
number of datapoints needed (ratio * total):  861
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0984)
number of datapoints needed (ratio * total):  491
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1208)
number of datapoints needed (ratio * total):  604
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0804)
number of datapoints needed (ratio * total):  401
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2026)
number of datapoints needed (ratio * total):  1013
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1788)
number of datapoints needed (ratio * total):  894
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 675
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 861
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 491
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 604
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 401
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1013
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 894
})]
length of training data:  4939
training model...
trainable params: 21,405,696 || all params: 8,051,666,944 || trainable%: 0.2659
{'loss': 2.0591, 'grad_norm': 3.473008394241333, 'learning_rate': 0.0002950657894736842, 'epoch': 0.03}
{'loss': 1.4763, 'grad_norm': 1.4267245531082153, 'learning_rate': 0.0002851973684210526, 'epoch': 0.06}
{'loss': 1.1639, 'grad_norm': 0.6324899792671204, 'learning_rate': 0.00027532894736842105, 'epoch': 0.1}
{'loss': 1.3067, 'grad_norm': 1.269781231880188, 'learning_rate': 0.0002654605263157894, 'epoch': 0.13}
{'loss': 1.1468, 'grad_norm': 2.251828908920288, 'learning_rate': 0.00025559210526315785, 'epoch': 0.16}
{'loss': 1.2238, 'grad_norm': 1.1206932067871094, 'learning_rate': 0.0002457236842105263, 'epoch': 0.19}
{'loss': 1.2985, 'grad_norm': 0.667393147945404, 'learning_rate': 0.00023585526315789474, 'epoch': 0.23}
{'loss': 1.0106, 'grad_norm': 0.3921144902706146, 'learning_rate': 0.00022598684210526314, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4052, 'train_samples_per_second': 49.191, 'train_steps_per_second': 6.155, 'train_loss': 1.3280384109680912, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_10/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)]]
proposed candidate before processing: tensor([0.1458, 0.0882, 0.0725, 0.0596, 0.1594, 0.1507, 0.1488, 0.1749, 0.1005,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8024, 0.0609])
proposed candidate after normalizing: [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), 3, 1, 1, 1, 1, 1, 103, 0.06088319793343544]
iteration:  11
input_X:  [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), 3, 1, 1, 1, 1, 1, 103, 0.06088319793343544]
mixing data with method:  random
arranging lora config with parameters:  103 0.06088319793343544 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 21,199,872 || all params: 8,051,461,120 || trainable%: 0.2633
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1458)
number of datapoints needed (ratio * total):  729
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0882)
number of datapoints needed (ratio * total):  440
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0725)
number of datapoints needed (ratio * total):  362
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0596)
number of datapoints needed (ratio * total):  297
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1594)
number of datapoints needed (ratio * total):  797
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1507)
number of datapoints needed (ratio * total):  753
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1488)
number of datapoints needed (ratio * total):  744
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1749)
number of datapoints needed (ratio * total):  874
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 729
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 440
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 362
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 297
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 797
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 753
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 744
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 874
})]
length of training data:  4996
training model...
trainable params: 21,199,872 || all params: 8,051,461,120 || trainable%: 0.2633
{'loss': 2.2988, 'grad_norm': 0.8363724946975708, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.2378, 'grad_norm': 0.7082029581069946, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3204, 'grad_norm': 2.251565933227539, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.4603, 'grad_norm': 0.8264622688293457, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.2332, 'grad_norm': 0.7639258503913879, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.2565, 'grad_norm': 0.6697733402252197, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.6686, 'grad_norm': 0.5389425158500671, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2539, 'train_samples_per_second': 49.833, 'train_steps_per_second': 6.234, 'train_loss': 1.4797943091090722, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_11/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.64
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)]]
proposed candidate before processing: tensor([0.0721, 0.1330, 0.0959, 0.0723, 0.1627, 0.0948, 0.1693, 0.1999, 0.1094,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7948, 0.0291])
proposed candidate after normalizing: [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), 4, 1, 1, 1, 1, 1, 102, 0.029062621295452118]
iteration:  12
input_X:  [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), 4, 1, 1, 1, 1, 1, 102, 0.029062621295452118]
mixing data with method:  random
arranging lora config with parameters:  102 0.029062621295452118 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 27,992,064 || all params: 8,058,253,312 || trainable%: 0.3474
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0721)
number of datapoints needed (ratio * total):  360
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1330)
number of datapoints needed (ratio * total):  664
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0959)
number of datapoints needed (ratio * total):  479
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0723)
number of datapoints needed (ratio * total):  361
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1627)
number of datapoints needed (ratio * total):  813
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0948)
number of datapoints needed (ratio * total):  474
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1693)
number of datapoints needed (ratio * total):  846
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1999)
number of datapoints needed (ratio * total):  999
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 360
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 664
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 479
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 361
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 813
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 474
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 846
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 999
})]
length of training data:  4996
training model...
trainable params: 27,992,064 || all params: 8,058,253,312 || trainable%: 0.3474
{'loss': 1.9411, 'grad_norm': 1.849475622177124, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.3572, 'grad_norm': 0.510510265827179, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3372, 'grad_norm': 2.3366920948028564, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.363, 'grad_norm': 0.6656532287597656, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.1905, 'grad_norm': 1.0020794868469238, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.523, 'grad_norm': 0.7094491720199585, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.505, 'grad_norm': 0.549084484577179, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0434, 'train_samples_per_second': 49.938, 'train_steps_per_second': 6.247, 'train_loss': 1.4433231480916342, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_12/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.56
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)]]
proposed candidate before processing: tensor([0.1876, 0.1061, 0.0077, 0.0757, 0.1040, 0.0999, 0.2660, 0.1530, 0.1035,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7959, 0.1000])
proposed candidate after normalizing: [tensor(0.1876), tensor(0.1061), 0, tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), 3, 1, 1, 1, 1, 1, 102, 0.10000000149011612]
iteration:  13
input_X:  [tensor(0.1876), tensor(0.1061), 0, tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), 3, 1, 1, 1, 1, 1, 102, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  102 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 27,992,064 || all params: 8,058,253,312 || trainable%: 0.3474
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1876), tensor(0.1061), 0, tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1876)
number of datapoints needed (ratio * total):  937
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1061)
number of datapoints needed (ratio * total):  530
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0757)
number of datapoints needed (ratio * total):  378
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1040)
number of datapoints needed (ratio * total):  519
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0999)
number of datapoints needed (ratio * total):  499
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2660)
number of datapoints needed (ratio * total):  1330
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1530)
number of datapoints needed (ratio * total):  765
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 937
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 530
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 378
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 519
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 499
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1330
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 765
})]
length of training data:  4958
training model...
trainable params: 27,992,064 || all params: 8,058,253,312 || trainable%: 0.3474
{'loss': 1.3369, 'grad_norm': 7.42296028137207, 'learning_rate': 0.0002950819672131147, 'epoch': 0.03}
{'loss': 1.3271, 'grad_norm': 0.5676109194755554, 'learning_rate': 0.00028524590163934424, 'epoch': 0.06}
{'loss': 0.9525, 'grad_norm': 0.45954859256744385, 'learning_rate': 0.00027540983606557377, 'epoch': 0.1}
{'loss': 1.0797, 'grad_norm': 0.7158648371696472, 'learning_rate': 0.00026557377049180324, 'epoch': 0.13}
{'loss': 1.3094, 'grad_norm': 0.6098048090934753, 'learning_rate': 0.00025573770491803277, 'epoch': 0.16}
{'loss': 1.1332, 'grad_norm': 1.1027103662490845, 'learning_rate': 0.0002459016393442623, 'epoch': 0.19}
{'loss': 0.9757, 'grad_norm': 0.7493060231208801, 'learning_rate': 0.00023606557377049177, 'epoch': 0.23}
{'loss': 1.1315, 'grad_norm': 0.5990028977394104, 'learning_rate': 0.00022622950819672127, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.123, 'train_samples_per_second': 49.519, 'train_steps_per_second': 6.192, 'train_loss': 1.1518499283563524, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_13/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1451, 0.1227, 0.0323, 0.0753, 0.0743, 0.1437, 0.2303, 0.1763, 0.0990,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8082, 0.1000])
proposed candidate after normalizing: [tensor(0.1451), tensor(0.1227), 0, tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), 3, 1, 1, 1, 1, 1, 103, 0.10000000149011612]
iteration:  14
input_X:  [tensor(0.1451), tensor(0.1227), 0, tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), 3, 1, 1, 1, 1, 1, 103, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  103 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 21,199,872 || all params: 8,051,461,120 || trainable%: 0.2633
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1451), tensor(0.1227), 0, tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1451)
number of datapoints needed (ratio * total):  725
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1227)
number of datapoints needed (ratio * total):  613
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0753)
number of datapoints needed (ratio * total):  376
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0743)
number of datapoints needed (ratio * total):  371
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1437)
number of datapoints needed (ratio * total):  718
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2303)
number of datapoints needed (ratio * total):  1151
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1763)
number of datapoints needed (ratio * total):  881
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 725
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 613
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 376
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 371
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 718
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1151
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 881
})]
length of training data:  4835
training model...
trainable params: 21,199,872 || all params: 8,051,461,120 || trainable%: 0.2633
{'loss': 2.5585, 'grad_norm': 0.8484604358673096, 'learning_rate': 0.00029495798319327727, 'epoch': 0.03}
{'loss': 1.2195, 'grad_norm': 1.308982014656067, 'learning_rate': 0.0002848739495798319, 'epoch': 0.07}
{'loss': 1.4171, 'grad_norm': 1.3549973964691162, 'learning_rate': 0.0002747899159663865, 'epoch': 0.1}
{'loss': 1.534, 'grad_norm': 0.8277664184570312, 'learning_rate': 0.00026470588235294115, 'epoch': 0.13}
{'loss': 1.1213, 'grad_norm': 1.427904725074768, 'learning_rate': 0.00025462184873949575, 'epoch': 0.17}
{'loss': 1.1425, 'grad_norm': 0.8073980212211609, 'learning_rate': 0.0002445378151260504, 'epoch': 0.2}
{'loss': 0.9554, 'grad_norm': 0.7678837776184082, 'learning_rate': 0.000234453781512605, 'epoch': 0.23}
{'loss': 1.2193, 'grad_norm': 0.9053695797920227, 'learning_rate': 0.00022436974789915966, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1948, 'train_samples_per_second': 48.256, 'train_steps_per_second': 6.038, 'train_loss': 1.3799896754190593, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_14/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)]]
proposed candidate before processing: tensor([0.2004, 0.0922, 0.0291, 0.0781, 0.1802, 0.0391, 0.2277, 0.1532, 0.1034,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8047, 0.0597])
proposed candidate after normalizing: [tensor(0.2004), tensor(0.0922), 0, tensor(0.0781), tensor(0.1802), 0, tensor(0.2277), tensor(0.1532), 3, 1, 1, 1, 1, 1, 103, 0.0597008541226387]
iteration:  15
input_X:  [tensor(0.2004), tensor(0.0922), 0, tensor(0.0781), tensor(0.1802), 0, tensor(0.2277), tensor(0.1532), 3, 1, 1, 1, 1, 1, 103, 0.0597008541226387]
mixing data with method:  random
arranging lora config with parameters:  103 0.0597008541226387 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 21,199,872 || all params: 8,051,461,120 || trainable%: 0.2633
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.2004), tensor(0.0922), 0, tensor(0.0781), tensor(0.1802), 0, tensor(0.2277), tensor(0.1532)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.2004)
number of datapoints needed (ratio * total):  1002
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0922)
number of datapoints needed (ratio * total):  460
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0781)
number of datapoints needed (ratio * total):  390
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1802)
number of datapoints needed (ratio * total):  900
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2277)
number of datapoints needed (ratio * total):  1138
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1532)
number of datapoints needed (ratio * total):  765
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1002
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 460
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 390
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 900
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1138
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 765
})]
length of training data:  4655
training model...
trainable params: 21,199,872 || all params: 8,051,461,120 || trainable%: 0.2633
{'loss': 1.8536, 'grad_norm': 0.6967607140541077, 'learning_rate': 0.0002947552447552447, 'epoch': 0.03}
{'loss': 1.7397, 'grad_norm': 0.6862656474113464, 'learning_rate': 0.0002842657342657343, 'epoch': 0.07}
{'loss': 1.3618, 'grad_norm': 0.9968535304069519, 'learning_rate': 0.0002737762237762238, 'epoch': 0.1}
{'loss': 1.169, 'grad_norm': 0.5879744291305542, 'learning_rate': 0.0002632867132867133, 'epoch': 0.14}
{'loss': 1.1719, 'grad_norm': 0.9483725428581238, 'learning_rate': 0.0002527972027972028, 'epoch': 0.17}
{'loss': 0.9592, 'grad_norm': 1.1446489095687866, 'learning_rate': 0.0002423076923076923, 'epoch': 0.21}
{'loss': 1.2101, 'grad_norm': 1.0577073097229004, 'learning_rate': 0.0002318181818181818, 'epoch': 0.24}
{'loss': 1.5119, 'grad_norm': 0.48843246698379517, 'learning_rate': 0.00022132867132867133, 'epoch': 0.27}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4399, 'train_samples_per_second': 46.346, 'train_steps_per_second': 5.795, 'train_loss': 1.3624425077152824, 'epoch': 0.29}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_15/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.58
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)]]
proposed candidate before processing: tensor([0.1496, 0.1100, 0.0163, 0.0627, 0.0997, 0.1371, 0.2511, 0.1735, 0.0989,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7854, 0.0317])
proposed candidate after normalizing: [tensor(0.1496), tensor(0.1100), 0, tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), 3, 1, 1, 1, 1, 1, 101, 0.03174708038568497]
iteration:  16
input_X:  [tensor(0.1496), tensor(0.1100), 0, tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), 3, 1, 1, 1, 1, 1, 101, 0.03174708038568497]
mixing data with method:  random
arranging lora config with parameters:  101 0.03174708038568497 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 20,788,224 || all params: 8,051,049,472 || trainable%: 0.2582
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1496), tensor(0.1100), 0, tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1496)
number of datapoints needed (ratio * total):  747
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1100)
number of datapoints needed (ratio * total):  549
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0627)
number of datapoints needed (ratio * total):  313
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0997)
number of datapoints needed (ratio * total):  498
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1371)
number of datapoints needed (ratio * total):  685
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2511)
number of datapoints needed (ratio * total):  1255
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1735)
number of datapoints needed (ratio * total):  867
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 747
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 549
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 313
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 498
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 685
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1255
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 867
})]
length of training data:  4914
training model...
trainable params: 20,788,224 || all params: 8,051,049,472 || trainable%: 0.2582
{'loss': 2.231, 'grad_norm': 1.158280849456787, 'learning_rate': 0.0002950413223140496, 'epoch': 0.03}
{'loss': 1.4719, 'grad_norm': 0.8494389057159424, 'learning_rate': 0.0002851239669421488, 'epoch': 0.07}
{'loss': 1.4243, 'grad_norm': 1.1627088785171509, 'learning_rate': 0.0002752066115702479, 'epoch': 0.1}
{'loss': 1.1341, 'grad_norm': 1.0928584337234497, 'learning_rate': 0.0002652892561983471, 'epoch': 0.13}
{'loss': 1.4446, 'grad_norm': 2.159005641937256, 'learning_rate': 0.00025537190082644627, 'epoch': 0.16}
{'loss': 1.3361, 'grad_norm': 0.6622677445411682, 'learning_rate': 0.00024545454545454545, 'epoch': 0.2}
{'loss': 1.3157, 'grad_norm': 0.39357125759124756, 'learning_rate': 0.00023553719008264463, 'epoch': 0.23}
{'loss': 1.1536, 'grad_norm': 0.6964998841285706, 'learning_rate': 0.00022561983471074378, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.406, 'train_samples_per_second': 48.941, 'train_steps_per_second': 6.125, 'train_loss': 1.4485269097720876, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_16/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)]]
proposed candidate before processing: tensor([0.1392, 0.0760, 0.0917, 0.0733, 0.1025, 0.1002, 0.2389, 0.1783, 0.0969,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7991, 0.1000])
proposed candidate after normalizing: [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), 3, 1, 1, 1, 1, 1, 102, 0.10000000149011612]
iteration:  17
input_X:  [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), 3, 1, 1, 1, 1, 1, 102, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  102 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 20,994,048 || all params: 8,051,255,296 || trainable%: 0.2608
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1392)
number of datapoints needed (ratio * total):  695
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0760)
number of datapoints needed (ratio * total):  379
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0917)
number of datapoints needed (ratio * total):  458
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0733)
number of datapoints needed (ratio * total):  366
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1025)
number of datapoints needed (ratio * total):  512
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1002)
number of datapoints needed (ratio * total):  500
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2389)
number of datapoints needed (ratio * total):  1194
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1783)
number of datapoints needed (ratio * total):  891
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 695
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 379
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 458
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 366
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 512
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 500
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1194
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 891
})]
length of training data:  4995
training model...
trainable params: 20,994,048 || all params: 8,051,255,296 || trainable%: 0.2608
{'loss': 2.195, 'grad_norm': 1.2530879974365234, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.4602, 'grad_norm': 2.7827911376953125, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.4163, 'grad_norm': 0.7438151836395264, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.4255, 'grad_norm': 0.9105061292648315, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3838, 'grad_norm': 0.7981447577476501, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 0.9729, 'grad_norm': 0.7719075083732605, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.0877, 'grad_norm': 0.7052332758903503, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2214, 'train_samples_per_second': 49.84, 'train_steps_per_second': 6.236, 'train_loss': 1.4400145735921739, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_17/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.57
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)]]
proposed candidate before processing: tensor([1.4121e-01, 1.9698e-01, 2.2441e-18, 7.0164e-02, 1.4519e-01, 1.3035e-01,
        1.6073e-01, 1.5536e-01, 1.0577e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.1121e-01, 9.9326e-02])
proposed candidate after normalizing: [tensor(0.1412), tensor(0.1970), 0, tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), 3, 1, 1, 1, 1, 1, 104, 0.09932610392570496]
iteration:  18
input_X:  [tensor(0.1412), tensor(0.1970), 0, tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), 3, 1, 1, 1, 1, 1, 104, 0.09932610392570496]
mixing data with method:  random
arranging lora config with parameters:  104 0.09932610392570496 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 21,405,696 || all params: 8,051,666,944 || trainable%: 0.2659
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1412), tensor(0.1970), 0, tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1412)
number of datapoints needed (ratio * total):  706
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1970)
number of datapoints needed (ratio * total):  984
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0702)
number of datapoints needed (ratio * total):  350
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1452)
number of datapoints needed (ratio * total):  725
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1303)
number of datapoints needed (ratio * total):  651
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1607)
number of datapoints needed (ratio * total):  803
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1554)
number of datapoints needed (ratio * total):  776
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 706
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 984
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 350
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 725
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 651
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 803
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 776
})]
length of training data:  4995
training model...
trainable params: 21,405,696 || all params: 8,051,666,944 || trainable%: 0.2659
{'loss': 2.0593, 'grad_norm': 2.334740400314331, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.1945, 'grad_norm': 0.5480514764785767, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.183, 'grad_norm': 0.732822835445404, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.4577, 'grad_norm': 0.8227301836013794, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.2889, 'grad_norm': 0.5050548315048218, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 0.8055, 'grad_norm': 0.4028719365596771, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.0521, 'grad_norm': 0.7116594910621643, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 0.9564, 'grad_norm': 1.1488181352615356, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1726, 'train_samples_per_second': 49.864, 'train_steps_per_second': 6.239, 'train_loss': 1.2489770004548222, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_18/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.64
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)]]
proposed candidate before processing: tensor([1.6044e-01, 1.7827e-01, 1.0066e-17, 9.0651e-02, 1.1110e-01, 1.2326e-01,
        1.6453e-01, 1.7176e-01, 1.1323e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.2350e-01, 4.5970e-02])
proposed candidate after normalizing: [tensor(0.1604), tensor(0.1783), 0, tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), 4, 1, 1, 1, 1, 1, 105, 0.04597048833966255]
iteration:  19
input_X:  [tensor(0.1604), tensor(0.1783), 0, tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), 4, 1, 1, 1, 1, 1, 105, 0.04597048833966255]
mixing data with method:  random
arranging lora config with parameters:  105 0.04597048833966255 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 28,815,360 || all params: 8,059,076,608 || trainable%: 0.3576
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1604), tensor(0.1783), 0, tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1604)
number of datapoints needed (ratio * total):  802
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1783)
number of datapoints needed (ratio * total):  891
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0907)
number of datapoints needed (ratio * total):  453
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1111)
number of datapoints needed (ratio * total):  555
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1233)
number of datapoints needed (ratio * total):  616
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1645)
number of datapoints needed (ratio * total):  822
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1718)
number of datapoints needed (ratio * total):  858
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 802
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 891
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 453
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 555
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 616
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 822
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 858
})]
length of training data:  4997
training model...
trainable params: 28,815,360 || all params: 8,059,076,608 || trainable%: 0.3576
{'loss': 2.0976, 'grad_norm': 0.9360716342926025, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.325, 'grad_norm': 1.3418105840682983, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.2315, 'grad_norm': 1.4699761867523193, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.2258, 'grad_norm': 1.2039213180541992, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.303, 'grad_norm': 0.42179620265960693, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.4007, 'grad_norm': 0.635329008102417, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.4188, 'grad_norm': 0.7978773713111877, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3035, 'train_samples_per_second': 49.819, 'train_steps_per_second': 6.231, 'train_loss': 1.4111263946917072, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_19/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)]]
proposed candidate before processing: tensor([0.1154, 0.1607, 0.0000, 0.0462, 0.1617, 0.1266, 0.2249, 0.1645, 0.0974,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7774, 0.1000])
proposed candidate after normalizing: [tensor(0.1154), tensor(0.1607), 0, 0, tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), 3, 1, 1, 1, 1, 1, 100, 0.10000000149011612]
iteration:  20
input_X:  [tensor(0.1154), tensor(0.1607), 0, 0, tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), 3, 1, 1, 1, 1, 1, 100, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  100 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 27,786,240 || all params: 8,058,047,488 || trainable%: 0.3448
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1154), tensor(0.1607), 0, 0, tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1154)
number of datapoints needed (ratio * total):  576
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1607)
number of datapoints needed (ratio * total):  803
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1617)
number of datapoints needed (ratio * total):  808
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1266)
number of datapoints needed (ratio * total):  633
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2249)
number of datapoints needed (ratio * total):  1124
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1645)
number of datapoints needed (ratio * total):  822
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 576
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 803
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 808
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 633
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1124
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 822
})]
length of training data:  4766
training model...
trainable params: 27,786,240 || all params: 8,058,047,488 || trainable%: 0.3448
{'loss': 1.4876, 'grad_norm': 0.7528959512710571, 'learning_rate': 0.0002948805460750853, 'epoch': 0.03}
{'loss': 1.4559, 'grad_norm': 1.6923428773880005, 'learning_rate': 0.00028464163822525593, 'epoch': 0.07}
{'loss': 1.4154, 'grad_norm': 0.5705515742301941, 'learning_rate': 0.00027440273037542657, 'epoch': 0.1}
{'loss': 1.3776, 'grad_norm': 0.5804527997970581, 'learning_rate': 0.00026416382252559726, 'epoch': 0.13}
{'loss': 1.1107, 'grad_norm': 1.1396458148956299, 'learning_rate': 0.0002539249146757679, 'epoch': 0.17}
{'loss': 1.0048, 'grad_norm': 0.726462185382843, 'learning_rate': 0.00024368600682593853, 'epoch': 0.2}
{'loss': 1.1012, 'grad_norm': 0.4311661720275879, 'learning_rate': 0.0002334470989761092, 'epoch': 0.23}
{'loss': 0.9871, 'grad_norm': 0.5062767267227173, 'learning_rate': 0.00022320819112627984, 'epoch': 0.27}
{'loss': 1.1031, 'grad_norm': 0.6908373236656189, 'learning_rate': 0.0002129692832764505, 'epoch': 0.3}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5152, 'train_samples_per_second': 47.416, 'train_steps_per_second': 5.929, 'train_loss': 1.2117508025396437, 'epoch': 0.32}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_20/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.64
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)]]
proposed candidate before processing: tensor([1.3310e-01, 1.4196e-01, 4.1505e-18, 5.3244e-02, 1.6103e-01, 1.3655e-01,
        1.9735e-01, 1.7677e-01, 7.3088e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.1555e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1331), tensor(0.1420), 0, tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), 2, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
iteration:  21
input_X:  [tensor(0.1331), tensor(0.1420), 0, tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), 2, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  104 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 21,131,264 || all params: 8,051,392,512 || trainable%: 0.2625
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1331), tensor(0.1420), 0, tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1331)
number of datapoints needed (ratio * total):  665
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1420)
number of datapoints needed (ratio * total):  709
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0532)
number of datapoints needed (ratio * total):  266
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1610)
number of datapoints needed (ratio * total):  805
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1366)
number of datapoints needed (ratio * total):  682
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1973)
number of datapoints needed (ratio * total):  986
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1768)
number of datapoints needed (ratio * total):  883
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 665
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 709
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 266
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 805
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 682
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 986
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 883
})]
length of training data:  4996
training model...
trainable params: 21,131,264 || all params: 8,051,392,512 || trainable%: 0.2625
{'loss': 2.0841, 'grad_norm': 1.253147840499878, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.3048, 'grad_norm': 0.5069054961204529, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.416, 'grad_norm': 0.9935883283615112, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.5162, 'grad_norm': 0.4412449300289154, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3437, 'grad_norm': 0.8639848232269287, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.3351, 'grad_norm': 1.0098485946655273, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.2222, 'grad_norm': 0.910209059715271, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.5164, 'grad_norm': 0.7030152678489685, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.113, 'train_samples_per_second': 49.904, 'train_steps_per_second': 6.243, 'train_loss': 1.4633174718812454, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_21/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.7
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)]]
proposed candidate before processing: tensor([1.3583e-01, 1.3437e-01, 8.6559e-18, 3.6962e-02, 1.7616e-01, 1.4965e-01,
        1.8982e-01, 1.7721e-01, 2.1666e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.3809e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1358), tensor(0.1344), 0, 0, tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), 1, 1, 1, 1, 1, 1, 107, 0.10000000149011612]
iteration:  22
input_X:  [tensor(0.1358), tensor(0.1344), 0, 0, tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), 1, 1, 1, 1, 1, 1, 107, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  107 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 14,476,288 || all params: 8,044,737,536 || trainable%: 0.1799
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1358), tensor(0.1344), 0, 0, tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1358)
number of datapoints needed (ratio * total):  679
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1344)
number of datapoints needed (ratio * total):  671
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1762)
number of datapoints needed (ratio * total):  880
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1497)
number of datapoints needed (ratio * total):  748
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1898)
number of datapoints needed (ratio * total):  949
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1772)
number of datapoints needed (ratio * total):  886
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 679
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 671
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 880
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 748
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 949
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 886
})]
length of training data:  4813
training model...
trainable params: 14,476,288 || all params: 8,044,737,536 || trainable%: 0.1799
{'loss': 1.4218, 'grad_norm': 0.8005266189575195, 'learning_rate': 0.0002949324324324324, 'epoch': 0.03}
{'loss': 1.0447, 'grad_norm': 0.44585976004600525, 'learning_rate': 0.0002847972972972973, 'epoch': 0.07}
{'loss': 1.1849, 'grad_norm': 0.636734127998352, 'learning_rate': 0.0002746621621621621, 'epoch': 0.1}
{'loss': 1.3041, 'grad_norm': 0.5989770889282227, 'learning_rate': 0.000264527027027027, 'epoch': 0.13}
{'loss': 1.0101, 'grad_norm': 1.0765364170074463, 'learning_rate': 0.0002543918918918919, 'epoch': 0.17}
{'loss': 1.3234, 'grad_norm': 1.0760538578033447, 'learning_rate': 0.00024425675675675675, 'epoch': 0.2}
{'loss': 0.927, 'grad_norm': 0.7862347364425659, 'learning_rate': 0.00023412162162162159, 'epoch': 0.23}
{'loss': 1.1789, 'grad_norm': 0.9507461190223694, 'learning_rate': 0.00022398648648648645, 'epoch': 0.27}
{'loss': 1.1448, 'grad_norm': 0.5779132843017578, 'learning_rate': 0.00021385135135135134, 'epoch': 0.3}
⏰ Max training time of 100 seconds reached. Stopping.
{'loss': 1.1711, 'grad_norm': 0.3560919165611267, 'learning_rate': 0.0002037162162162162, 'epoch': 0.33}
{'train_runtime': 100.468, 'train_samples_per_second': 47.906, 'train_steps_per_second': 5.992, 'train_loss': 1.1710716342926026, 'epoch': 0.33}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_22/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)]]
proposed candidate before processing: tensor([1.3615e-01, 1.3079e-01, 6.1812e-18, 5.3671e-02, 1.7354e-01, 1.3968e-01,
        1.8369e-01, 1.8248e-01, 4.5665e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.3009e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1361), tensor(0.1308), 0, tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), 1, 1, 1, 1, 1, 1, 106, 0.10000000149011612]
iteration:  23
input_X:  [tensor(0.1361), tensor(0.1308), 0, tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), 1, 1, 1, 1, 1, 1, 106, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  106 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,272,448 || all params: 8,037,533,696 || trainable%: 0.0905
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1361), tensor(0.1308), 0, tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1361)
number of datapoints needed (ratio * total):  680
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1308)
number of datapoints needed (ratio * total):  653
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0537)
number of datapoints needed (ratio * total):  268
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1735)
number of datapoints needed (ratio * total):  867
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1397)
number of datapoints needed (ratio * total):  698
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1837)
number of datapoints needed (ratio * total):  918
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1825)
number of datapoints needed (ratio * total):  912
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 680
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 653
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 268
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 867
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 698
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 918
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 912
})]
length of training data:  4996
training model...
trainable params: 7,272,448 || all params: 8,037,533,696 || trainable%: 0.0905
{'loss': 2.2546, 'grad_norm': 1.0150376558303833, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.8311, 'grad_norm': 2.427347183227539, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.2827, 'grad_norm': 0.8642875552177429, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3294, 'grad_norm': 0.9837995767593384, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.8338, 'grad_norm': 0.7096237540245056, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.481, 'grad_norm': 1.0668624639511108, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.6333, 'grad_norm': 0.3309192657470703, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.5111, 'grad_norm': 0.9142835736274719, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5192, 'train_samples_per_second': 49.702, 'train_steps_per_second': 6.218, 'train_loss': 1.6622691262852063, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_23/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1242, 0.1610, 0.0000, 0.0329, 0.1526, 0.1206, 0.2335, 0.1753, 0.0519,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8199, 0.1000])
proposed candidate after normalizing: [tensor(0.1242), tensor(0.1610), 0, 0, tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), 2, 1, 1, 1, 1, 1, 105, 0.10000000149011612]
iteration:  24
input_X:  [tensor(0.1242), tensor(0.1610), 0, 0, tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), 2, 1, 1, 1, 1, 1, 105, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  105 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 14,407,680 || all params: 8,044,668,928 || trainable%: 0.1791
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1242), tensor(0.1610), 0, 0, tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1242)
number of datapoints needed (ratio * total):  621
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1610)
number of datapoints needed (ratio * total):  804
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1526)
number of datapoints needed (ratio * total):  763
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1206)
number of datapoints needed (ratio * total):  602
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2335)
number of datapoints needed (ratio * total):  1167
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1753)
number of datapoints needed (ratio * total):  876
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 621
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 804
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 763
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 602
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1167
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 876
})]
length of training data:  4833
training model...
trainable params: 14,407,680 || all params: 8,044,668,928 || trainable%: 0.1791
{'loss': 2.161, 'grad_norm': 0.7973405718803406, 'learning_rate': 0.00029495798319327727, 'epoch': 0.03}
{'loss': 1.542, 'grad_norm': 0.7450478076934814, 'learning_rate': 0.0002848739495798319, 'epoch': 0.07}
{'loss': 1.2855, 'grad_norm': 0.5553962588310242, 'learning_rate': 0.0002747899159663865, 'epoch': 0.1}
{'loss': 1.6799, 'grad_norm': 0.8271452784538269, 'learning_rate': 0.00026470588235294115, 'epoch': 0.13}
{'loss': 1.1877, 'grad_norm': 0.7902557253837585, 'learning_rate': 0.00025462184873949575, 'epoch': 0.17}
{'loss': 1.4443, 'grad_norm': 0.8086525201797485, 'learning_rate': 0.0002445378151260504, 'epoch': 0.2}
{'loss': 1.1465, 'grad_norm': 0.8996834754943848, 'learning_rate': 0.000234453781512605, 'epoch': 0.23}
{'loss': 1.2576, 'grad_norm': 0.827237069606781, 'learning_rate': 0.00022436974789915966, 'epoch': 0.26}
{'loss': 1.2239, 'grad_norm': 0.5417673587799072, 'learning_rate': 0.00021428571428571427, 'epoch': 0.3}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6941, 'train_samples_per_second': 47.997, 'train_steps_per_second': 6.008, 'train_loss': 1.426267770232347, 'epoch': 0.31}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_24/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)]]
proposed candidate before processing: tensor([1.4130e-01, 1.2349e-01, 1.2636e-17, 6.4841e-02, 1.6920e-01, 1.5607e-01,
        1.7169e-01, 1.7341e-01, 1.0208e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.9834e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1413), tensor(0.1235), 0, tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), 3, 1, 1, 1, 1, 1, 102, 0.10000000149011612]
iteration:  25
input_X:  [tensor(0.1413), tensor(0.1235), 0, tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), 3, 1, 1, 1, 1, 1, 102, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  102 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 20,994,048 || all params: 8,051,255,296 || trainable%: 0.2608
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1413), tensor(0.1235), 0, tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1413)
number of datapoints needed (ratio * total):  706
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1235)
number of datapoints needed (ratio * total):  617
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0648)
number of datapoints needed (ratio * total):  324
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1692)
number of datapoints needed (ratio * total):  845
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1561)
number of datapoints needed (ratio * total):  780
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1717)
number of datapoints needed (ratio * total):  858
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1734)
number of datapoints needed (ratio * total):  867
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 706
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 617
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 324
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 845
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 780
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 858
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 867
})]
length of training data:  4997
training model...
trainable params: 20,994,048 || all params: 8,051,255,296 || trainable%: 0.2608
{'loss': 2.1315, 'grad_norm': 2.158790111541748, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.4004, 'grad_norm': 1.089532732963562, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.334, 'grad_norm': 1.1415969133377075, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.4887, 'grad_norm': 0.7856266498565674, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.5823, 'grad_norm': 0.5502262115478516, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.5696, 'grad_norm': 0.5786555409431458, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.4073, 'grad_norm': 0.3809696137905121, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.225, 'grad_norm': 0.6134331226348877, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2207, 'train_samples_per_second': 49.86, 'train_steps_per_second': 6.236, 'train_loss': 1.4882716463323225, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_25/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.6
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)]]
proposed candidate before processing: tensor([1.2704e-01, 1.7233e-01, 6.7509e-19, 6.4037e-02, 1.3004e-01, 1.0339e-01,
        2.2376e-01, 1.7940e-01, 2.3269e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.3815e-01, 8.7070e-02])
proposed candidate after normalizing: [tensor(0.1270), tensor(0.1723), 0, tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), 1, 1, 1, 1, 1, 1, 107, 0.08707036077976227]
iteration:  26
input_X:  [tensor(0.1270), tensor(0.1723), 0, tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), 1, 1, 1, 1, 1, 1, 107, 0.08707036077976227]
mixing data with method:  random
arranging lora config with parameters:  107 0.08707036077976227 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 21,337,088 || all params: 8,051,598,336 || trainable%: 0.2650
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1270), tensor(0.1723), 0, tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1270)
number of datapoints needed (ratio * total):  635
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1723)
number of datapoints needed (ratio * total):  861
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0640)
number of datapoints needed (ratio * total):  320
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1300)
number of datapoints needed (ratio * total):  650
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1034)
number of datapoints needed (ratio * total):  516
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2238)
number of datapoints needed (ratio * total):  1118
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1794)
number of datapoints needed (ratio * total):  896
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 635
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 861
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 320
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 650
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 516
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1118
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 896
})]
length of training data:  4996
training model...
trainable params: 21,337,088 || all params: 8,051,598,336 || trainable%: 0.2650
{'loss': 1.1153, 'grad_norm': 0.931459367275238, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.3349, 'grad_norm': 1.3199766874313354, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.2176, 'grad_norm': 0.49571236968040466, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.1116, 'grad_norm': 1.1986814737319946, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3052, 'grad_norm': 0.4024999141693115, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1991, 'grad_norm': 0.7642642855644226, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.3799, 'grad_norm': 0.4804786741733551, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.4212, 'grad_norm': 0.33219966292381287, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1573, 'train_samples_per_second': 49.882, 'train_steps_per_second': 6.24, 'train_loss': 1.2540819688541134, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_26/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.64
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)]]
proposed candidate before processing: tensor([0.1439, 0.1588, 0.0065, 0.0267, 0.1461, 0.1291, 0.2191, 0.1698, 0.0383,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8267, 0.0581])
proposed candidate after normalizing: [tensor(0.1439), tensor(0.1588), 0, 0, tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), 1, 1, 1, 1, 1, 1, 106, 0.058057162910699844]
iteration:  27
input_X:  [tensor(0.1439), tensor(0.1588), 0, 0, tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), 1, 1, 1, 1, 1, 1, 106, 0.058057162910699844]
mixing data with method:  random
arranging lora config with parameters:  106 0.058057162910699844 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,272,448 || all params: 8,037,533,696 || trainable%: 0.0905
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1439), tensor(0.1588), 0, 0, tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1439)
number of datapoints needed (ratio * total):  719
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1588)
number of datapoints needed (ratio * total):  793
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1461)
number of datapoints needed (ratio * total):  730
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1291)
number of datapoints needed (ratio * total):  645
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2191)
number of datapoints needed (ratio * total):  1095
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1698)
number of datapoints needed (ratio * total):  848
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 719
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 793
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 730
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 645
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1095
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 848
})]
length of training data:  4830
training model...
trainable params: 7,272,448 || all params: 8,037,533,696 || trainable%: 0.0905
{'loss': 2.3714, 'grad_norm': 1.5739346742630005, 'learning_rate': 0.00029494949494949493, 'epoch': 0.03}
{'loss': 1.6866, 'grad_norm': 0.32739144563674927, 'learning_rate': 0.0002848484848484848, 'epoch': 0.07}
{'loss': 1.2995, 'grad_norm': 2.1770522594451904, 'learning_rate': 0.0002747474747474747, 'epoch': 0.1}
{'loss': 1.2454, 'grad_norm': 1.4045562744140625, 'learning_rate': 0.00026464646464646464, 'epoch': 0.13}
{'loss': 1.2498, 'grad_norm': 0.4458737373352051, 'learning_rate': 0.0002545454545454545, 'epoch': 0.17}
{'loss': 1.1923, 'grad_norm': 0.26645177602767944, 'learning_rate': 0.00024444444444444443, 'epoch': 0.2}
{'loss': 1.1931, 'grad_norm': 0.6357678771018982, 'learning_rate': 0.00023434343434343432, 'epoch': 0.23}
{'loss': 1.3963, 'grad_norm': 0.8654834628105164, 'learning_rate': 0.00022424242424242424, 'epoch': 0.26}
{'loss': 1.2012, 'grad_norm': 0.3891540765762329, 'learning_rate': 0.0002141414141414141, 'epoch': 0.3}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3087, 'train_samples_per_second': 48.151, 'train_steps_per_second': 6.021, 'train_loss': 1.429647375407972, 'epoch': 0.31}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_27/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)]]
proposed candidate before processing: tensor([0.1227, 0.1633, 0.0018, 0.0858, 0.1369, 0.0938, 0.2097, 0.1860, 0.0805,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8243, 0.1000])
proposed candidate after normalizing: [tensor(0.1227), tensor(0.1633), 0, tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), 3, 1, 1, 1, 1, 1, 106, 0.10000000149011612]
iteration:  28
input_X:  [tensor(0.1227), tensor(0.1633), 0, tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), 3, 1, 1, 1, 1, 1, 106, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  106 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 21,817,344 || all params: 8,052,078,592 || trainable%: 0.2710
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1227), tensor(0.1633), 0, tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1227)
number of datapoints needed (ratio * total):  613
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1633)
number of datapoints needed (ratio * total):  816
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0858)
number of datapoints needed (ratio * total):  429
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1369)
number of datapoints needed (ratio * total):  684
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0938)
number of datapoints needed (ratio * total):  468
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2097)
number of datapoints needed (ratio * total):  1048
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1860)
number of datapoints needed (ratio * total):  930
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 613
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 816
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 429
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 684
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 468
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1048
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 930
})]
length of training data:  4988
training model...
trainable params: 21,817,344 || all params: 8,052,078,592 || trainable%: 0.2710
{'loss': 2.0784, 'grad_norm': 0.5358789563179016, 'learning_rate': 0.000295114006514658, 'epoch': 0.03}
{'loss': 1.3848, 'grad_norm': 1.2756954431533813, 'learning_rate': 0.0002853420195439739, 'epoch': 0.06}
{'loss': 1.2996, 'grad_norm': 0.9145481586456299, 'learning_rate': 0.00027557003257328987, 'epoch': 0.1}
{'loss': 1.2484, 'grad_norm': 0.6677216291427612, 'learning_rate': 0.0002657980456026058, 'epoch': 0.13}
{'loss': 1.0604, 'grad_norm': 0.9497339725494385, 'learning_rate': 0.0002560260586319218, 'epoch': 0.16}
{'loss': 1.2186, 'grad_norm': 0.48387351632118225, 'learning_rate': 0.00024625407166123777, 'epoch': 0.19}
{'loss': 0.8621, 'grad_norm': 1.0325902700424194, 'learning_rate': 0.00023648208469055374, 'epoch': 0.22}
{'loss': 1.2372, 'grad_norm': 1.1093780994415283, 'learning_rate': 0.0002267100977198697, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4602, 'train_samples_per_second': 49.651, 'train_steps_per_second': 6.211, 'train_loss': 1.3001390525272913, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_28/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1335, 0.1631, 0.0166, 0.0768, 0.1369, 0.0952, 0.1951, 0.1829, 0.0820,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8321, 0.1000])
proposed candidate after normalizing: [tensor(0.1335), tensor(0.1631), 0, tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), 3, 1, 1, 1, 1, 1, 107, 0.10000000149011612]
iteration:  29
input_X:  [tensor(0.1335), tensor(0.1631), 0, tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), 3, 1, 1, 1, 1, 1, 107, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  107 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 22,023,168 || all params: 8,052,284,416 || trainable%: 0.2735
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1335), tensor(0.1631), 0, tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1335)
number of datapoints needed (ratio * total):  667
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1631)
number of datapoints needed (ratio * total):  815
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0768)
number of datapoints needed (ratio * total):  383
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1369)
number of datapoints needed (ratio * total):  684
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0952)
number of datapoints needed (ratio * total):  475
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1951)
number of datapoints needed (ratio * total):  975
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1829)
number of datapoints needed (ratio * total):  914
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 667
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 815
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 383
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 684
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 475
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 975
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 914
})]
length of training data:  4913
training model...
trainable params: 22,023,168 || all params: 8,052,284,416 || trainable%: 0.2735
{'loss': 2.0894, 'grad_norm': 1.695186734199524, 'learning_rate': 0.0002950413223140496, 'epoch': 0.03}
{'loss': 1.629, 'grad_norm': 0.5695676803588867, 'learning_rate': 0.0002851239669421488, 'epoch': 0.07}
{'loss': 1.5901, 'grad_norm': 0.595935583114624, 'learning_rate': 0.0002752066115702479, 'epoch': 0.1}
{'loss': 1.3796, 'grad_norm': 0.8600811958312988, 'learning_rate': 0.0002652892561983471, 'epoch': 0.13}
{'loss': 1.0739, 'grad_norm': 0.8048485517501831, 'learning_rate': 0.00025537190082644627, 'epoch': 0.16}
{'loss': 1.5343, 'grad_norm': 0.7930195331573486, 'learning_rate': 0.00024545454545454545, 'epoch': 0.2}
{'loss': 0.9997, 'grad_norm': 1.446868658065796, 'learning_rate': 0.00023553719008264463, 'epoch': 0.23}
{'loss': 1.1618, 'grad_norm': 0.4725913107395172, 'learning_rate': 0.00022561983471074378, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2858, 'train_samples_per_second': 48.99, 'train_steps_per_second': 6.132, 'train_loss': 1.4140243530273438, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_29/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.6
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)]]
proposed candidate before processing: tensor([1.0901e-01, 1.3696e-01, 4.4534e-17, 5.1206e-02, 1.5335e-01, 1.5317e-01,
        2.3341e-01, 1.6290e-01, 3.0018e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.8998e-01, 7.0502e-02])
proposed candidate after normalizing: [tensor(0.1090), tensor(0.1370), 0, tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), 1, 1, 1, 1, 1, 1, 101, 0.07050154358148575]
iteration:  30
input_X:  [tensor(0.1090), tensor(0.1370), 0, tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), 1, 1, 1, 1, 1, 1, 101, 0.07050154358148575]
mixing data with method:  random
arranging lora config with parameters:  101 0.07050154358148575 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 21,611,520 || all params: 8,051,872,768 || trainable%: 0.2684
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1090), tensor(0.1370), 0, tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1090)
number of datapoints needed (ratio * total):  545
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1370)
number of datapoints needed (ratio * total):  684
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0512)
number of datapoints needed (ratio * total):  256
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1533)
number of datapoints needed (ratio * total):  766
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1532)
number of datapoints needed (ratio * total):  765
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2334)
number of datapoints needed (ratio * total):  1167
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1629)
number of datapoints needed (ratio * total):  814
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 545
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 684
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 256
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 766
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 765
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1167
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 814
})]
length of training data:  4997
training model...
trainable params: 21,611,520 || all params: 8,051,872,768 || trainable%: 0.2684
{'loss': 1.2578, 'grad_norm': 0.7807919979095459, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 0.9854, 'grad_norm': 0.48909157514572144, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.188, 'grad_norm': 0.6205106377601624, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3102, 'grad_norm': 0.730978786945343, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.1986, 'grad_norm': 0.8066574931144714, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 0.9952, 'grad_norm': 0.3892221748828888, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.1949, 'grad_norm': 1.0541924238204956, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.0258, 'grad_norm': 0.3338463008403778, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0985, 'train_samples_per_second': 49.921, 'train_steps_per_second': 6.244, 'train_loss': 1.1191947529081665, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_30/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)]]
proposed candidate before processing: tensor([9.7562e-02, 1.3418e-01, 2.3192e-17, 5.0970e-02, 1.5633e-01, 1.6823e-01,
        2.4267e-01, 1.5006e-01, 3.2342e-03, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.7936e-01, 7.1145e-02])
proposed candidate after normalizing: [tensor(0.0976), tensor(0.1342), 0, tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), 0, 1, 1, 1, 1, 1, 100, 0.0711454302072525]
iteration:  31
input_X:  [tensor(0.0976), tensor(0.1342), 0, tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), 0, 1, 1, 1, 1, 1, 100, 0.0711454302072525]
mixing data with method:  random
arranging lora config with parameters:  100 0.0711454302072525 0 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)]]
proposed candidate before processing: tensor([2.1280e-01, 9.9931e-02, 6.3423e-18, 2.9573e-02, 2.0206e-01, 1.1351e-01,
        1.4901e-01, 1.9312e-01, 1.0597e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.4263e-01, 9.8380e-02])
proposed candidate after normalizing: [tensor(0.2128), tensor(0.0999), 0, 0, tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), 3, 1, 1, 1, 1, 1, 108, 0.0983804240822792]
iteration:  32
input_X:  [tensor(0.2128), tensor(0.0999), 0, 0, tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), 3, 1, 1, 1, 1, 1, 108, 0.0983804240822792]
mixing data with method:  random
arranging lora config with parameters:  108 0.0983804240822792 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 22,228,992 || all params: 8,052,490,240 || trainable%: 0.2761
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.2128), tensor(0.0999), 0, 0, tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.2128)
number of datapoints needed (ratio * total):  1064
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0999)
number of datapoints needed (ratio * total):  499
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.2021)
number of datapoints needed (ratio * total):  1010
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1135)
number of datapoints needed (ratio * total):  567
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1490)
number of datapoints needed (ratio * total):  745
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1931)
number of datapoints needed (ratio * total):  965
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1064
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 499
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1010
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 567
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 745
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 965
})]
length of training data:  4850
training model...
trainable params: 22,228,992 || all params: 8,052,490,240 || trainable%: 0.2761
{'loss': 2.089, 'grad_norm': 0.6144755482673645, 'learning_rate': 0.0002949748743718593, 'epoch': 0.03}
{'loss': 1.3298, 'grad_norm': 0.36342471837997437, 'learning_rate': 0.0002849246231155779, 'epoch': 0.07}
{'loss': 1.4884, 'grad_norm': 1.2156561613082886, 'learning_rate': 0.0002748743718592965, 'epoch': 0.1}
{'loss': 1.4524, 'grad_norm': 1.122144103050232, 'learning_rate': 0.0002648241206030151, 'epoch': 0.13}
{'loss': 1.3677, 'grad_norm': 1.146087646484375, 'learning_rate': 0.0002547738693467337, 'epoch': 0.16}
{'loss': 1.4113, 'grad_norm': 0.8648446798324585, 'learning_rate': 0.00024472361809045227, 'epoch': 0.2}
{'loss': 1.1666, 'grad_norm': 0.6868457198143005, 'learning_rate': 0.00023467336683417084, 'epoch': 0.23}
{'loss': 1.1804, 'grad_norm': 0.4959428608417511, 'learning_rate': 0.00022462311557788943, 'epoch': 0.26}
{'loss': 1.3407, 'grad_norm': 0.5066198706626892, 'learning_rate': 0.00021457286432160802, 'epoch': 0.3}
{'loss': 1.1708, 'grad_norm': 0.9298095107078552, 'learning_rate': 0.00020452261306532662, 'epoch': 0.33}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0265, 'train_samples_per_second': 48.487, 'train_steps_per_second': 6.068, 'train_loss': 1.398125711301478, 'epoch': 0.34}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_32/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)]]
proposed candidate before processing: tensor([1.8442e-01, 1.4235e-01, 2.4931e-18, 2.7079e-02, 1.5286e-01, 1.1259e-01,
        1.7664e-01, 2.0407e-01, 1.1352e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.4271e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1844), tensor(0.1423), 0, 0, tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), 4, 1, 1, 1, 1, 1, 108, 0.10000000149011612]
iteration:  33
input_X:  [tensor(0.1844), tensor(0.1423), 0, 0, tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), 4, 1, 1, 1, 1, 1, 108, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  108 0.10000000149011612 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 29,638,656 || all params: 8,059,899,904 || trainable%: 0.3677
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1844), tensor(0.1423), 0, 0, tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1844)
number of datapoints needed (ratio * total):  922
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1423)
number of datapoints needed (ratio * total):  711
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1529)
number of datapoints needed (ratio * total):  764
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1126)
number of datapoints needed (ratio * total):  562
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1766)
number of datapoints needed (ratio * total):  883
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2041)
number of datapoints needed (ratio * total):  1020
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 922
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 711
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 764
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 562
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 883
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1020
})]
length of training data:  4862
training model...
trainable params: 29,638,656 || all params: 8,059,899,904 || trainable%: 0.3677
{'loss': 2.1167, 'grad_norm': 0.90120929479599, 'learning_rate': 0.00029498327759197324, 'epoch': 0.03}
{'loss': 1.6315, 'grad_norm': 1.2005136013031006, 'learning_rate': 0.00028494983277591973, 'epoch': 0.07}
{'loss': 1.2302, 'grad_norm': 0.6777604818344116, 'learning_rate': 0.00027491638795986616, 'epoch': 0.1}
{'loss': 1.0614, 'grad_norm': 1.495521068572998, 'learning_rate': 0.0002648829431438127, 'epoch': 0.13}
{'loss': 1.219, 'grad_norm': 0.481374055147171, 'learning_rate': 0.0002548494983277592, 'epoch': 0.16}
{'loss': 1.3203, 'grad_norm': 0.590029239654541, 'learning_rate': 0.00024481605351170567, 'epoch': 0.2}
{'loss': 1.6776, 'grad_norm': 1.4061062335968018, 'learning_rate': 0.00023478260869565215, 'epoch': 0.23}
{'loss': 1.4572, 'grad_norm': 0.9781399965286255, 'learning_rate': 0.00022474916387959864, 'epoch': 0.26}
{'loss': 1.2829, 'grad_norm': 0.44518768787384033, 'learning_rate': 0.00021471571906354512, 'epoch': 0.3}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5476, 'train_samples_per_second': 48.355, 'train_steps_per_second': 6.047, 'train_loss': 1.425505907747758, 'epoch': 0.31}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_33/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.61
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)]]
proposed candidate before processing: tensor([1.6708e-01, 1.0496e-01, 2.6068e-17, 8.0279e-02, 1.8479e-01, 1.0952e-01,
        1.7089e-01, 1.8248e-01, 1.1218e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.2040e-01, 6.9489e-02])
proposed candidate after normalizing: [tensor(0.1671), tensor(0.1050), 0, tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), 4, 1, 1, 1, 1, 1, 105, 0.06948909908533096]
iteration:  34
input_X:  [tensor(0.1671), tensor(0.1050), 0, tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), 4, 1, 1, 1, 1, 1, 105, 0.06948909908533096]
mixing data with method:  random
arranging lora config with parameters:  105 0.06948909908533096 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 28,815,360 || all params: 8,059,076,608 || trainable%: 0.3576
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1671), tensor(0.1050), 0, tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1671)
number of datapoints needed (ratio * total):  835
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1050)
number of datapoints needed (ratio * total):  524
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0803)
number of datapoints needed (ratio * total):  401
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1848)
number of datapoints needed (ratio * total):  923
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1095)
number of datapoints needed (ratio * total):  547
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1709)
number of datapoints needed (ratio * total):  854
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1825)
number of datapoints needed (ratio * total):  912
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 835
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 524
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 401
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 923
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 547
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 854
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 912
})]
length of training data:  4996
training model...
trainable params: 28,815,360 || all params: 8,059,076,608 || trainable%: 0.3576
{'loss': 2.0784, 'grad_norm': 1.8989320993423462, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.7237, 'grad_norm': 1.0981545448303223, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.196, 'grad_norm': 1.7261683940887451, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.1694, 'grad_norm': 0.9659761786460876, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.2688, 'grad_norm': 0.5416737794876099, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.24, 'grad_norm': 0.7934613227844238, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.536, 'grad_norm': 0.5336677432060242, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.391, 'grad_norm': 0.6391454935073853, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0823, 'train_samples_per_second': 49.919, 'train_steps_per_second': 6.245, 'train_loss': 1.45665102694408, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_34/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.51
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)]]
proposed candidate before processing: tensor([0.1521, 0.2156, 0.0213, 0.0155, 0.0893, 0.0829, 0.2065, 0.2169, 0.1042,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8763, 0.1000])
proposed candidate after normalizing: [tensor(0.1521), tensor(0.2156), 0, 0, tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), 3, 1, 1, 1, 1, 1, 112, 0.10000000149011612]
iteration:  35
input_X:  [tensor(0.1521), tensor(0.2156), 0, 0, tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), 3, 1, 1, 1, 1, 1, 112, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  112 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 30,256,128 || all params: 8,060,517,376 || trainable%: 0.3754
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1521), tensor(0.2156), 0, 0, tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1521)
number of datapoints needed (ratio * total):  760
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2156)
number of datapoints needed (ratio * total):  1077
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0893)
number of datapoints needed (ratio * total):  446
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0829)
number of datapoints needed (ratio * total):  414
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2065)
number of datapoints needed (ratio * total):  1032
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2169)
number of datapoints needed (ratio * total):  1084
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 760
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1077
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 446
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 414
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1032
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1084
})]
length of training data:  4813
training model...
trainable params: 30,256,128 || all params: 8,060,517,376 || trainable%: 0.3754
{'loss': 1.5578, 'grad_norm': 1.000287652015686, 'learning_rate': 0.0002949324324324324, 'epoch': 0.03}
{'loss': 1.2416, 'grad_norm': 0.41163840889930725, 'learning_rate': 0.0002847972972972973, 'epoch': 0.07}
{'loss': 1.2325, 'grad_norm': 0.46603986620903015, 'learning_rate': 0.0002746621621621621, 'epoch': 0.1}
{'loss': 1.4479, 'grad_norm': 0.46237263083457947, 'learning_rate': 0.000264527027027027, 'epoch': 0.13}
{'loss': 1.1458, 'grad_norm': 0.660324215888977, 'learning_rate': 0.0002543918918918919, 'epoch': 0.17}
{'loss': 1.2375, 'grad_norm': 0.4461439549922943, 'learning_rate': 0.00024425675675675675, 'epoch': 0.2}
{'loss': 1.0302, 'grad_norm': 1.4789323806762695, 'learning_rate': 0.00023412162162162159, 'epoch': 0.23}
{'loss': 1.3488, 'grad_norm': 0.5012162923812866, 'learning_rate': 0.00022398648648648645, 'epoch': 0.27}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3163, 'train_samples_per_second': 47.978, 'train_steps_per_second': 6.001, 'train_loss': 1.292866488408776, 'epoch': 0.3}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_35/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.69
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)]]
proposed candidate before processing: tensor([0.0041, 0.1085, 0.0000, 0.0847, 0.2586, 0.1881, 0.2425, 0.1134, 0.0712,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7903, 0.0556])
proposed candidate after normalizing: [0, tensor(0.1085), 0, tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), 2, 1, 1, 1, 1, 1, 101, 0.055645883083343506]
iteration:  36
input_X:  [0, tensor(0.1085), 0, tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), 2, 1, 1, 1, 1, 1, 101, 0.055645883083343506]
mixing data with method:  random
arranging lora config with parameters:  101 0.055645883083343506 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 21,542,912 || all params: 8,051,804,160 || trainable%: 0.2676
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.1085), 0, tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.1085)
number of datapoints needed (ratio * total):  542
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0847)
number of datapoints needed (ratio * total):  423
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.2586)
number of datapoints needed (ratio * total):  1293
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1881)
number of datapoints needed (ratio * total):  940
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2425)
number of datapoints needed (ratio * total):  1212
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1134)
number of datapoints needed (ratio * total):  566
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 542
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 423
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1293
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 940
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1212
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 566
})]
length of training data:  4976
training model...
trainable params: 21,542,912 || all params: 8,051,804,160 || trainable%: 0.2676
{'loss': 1.8685, 'grad_norm': 1.4474989175796509, 'learning_rate': 0.00029509803921568624, 'epoch': 0.03}
{'loss': 1.1205, 'grad_norm': 0.5061625242233276, 'learning_rate': 0.00028529411764705877, 'epoch': 0.06}
{'loss': 0.9668, 'grad_norm': 1.0015876293182373, 'learning_rate': 0.00027549019607843136, 'epoch': 0.1}
{'loss': 1.0889, 'grad_norm': 0.5318516492843628, 'learning_rate': 0.0002656862745098039, 'epoch': 0.13}
{'loss': 1.1983, 'grad_norm': 1.128572702407837, 'learning_rate': 0.0002558823529411764, 'epoch': 0.16}
{'loss': 1.3316, 'grad_norm': 1.9819213151931763, 'learning_rate': 0.000246078431372549, 'epoch': 0.19}
{'loss': 0.7688, 'grad_norm': 1.001289963722229, 'learning_rate': 0.00023627450980392154, 'epoch': 0.23}
{'loss': 0.9669, 'grad_norm': 0.809594452381134, 'learning_rate': 0.0002264705882352941, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3191, 'train_samples_per_second': 49.602, 'train_steps_per_second': 6.2, 'train_loss': 1.156805480658675, 'epoch': 0.29}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_36/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)]]
proposed candidate before processing: tensor([0.2982, 0.2573, 0.0000, 0.0000, 0.0034, 0.0971, 0.1520, 0.1919, 0.1116,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8848, 0.1000])
proposed candidate after normalizing: [tensor(0.2982), tensor(0.2573), 0, 0, 0, tensor(0.0971), tensor(0.1520), tensor(0.1919), 4, 1, 1, 1, 1, 1, 113, 0.10000000149011612]
iteration:  37
input_X:  [tensor(0.2982), tensor(0.2573), 0, 0, 0, tensor(0.0971), tensor(0.1520), tensor(0.1919), 4, 1, 1, 1, 1, 1, 113, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  113 0.10000000149011612 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 31,010,816 || all params: 8,061,272,064 || trainable%: 0.3847
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.2982), tensor(0.2573), 0, 0, 0, tensor(0.0971), tensor(0.1520), tensor(0.1919)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.2982)
number of datapoints needed (ratio * total):  1490
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2573)
number of datapoints needed (ratio * total):  1286
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  tensor(0.0971)
number of datapoints needed (ratio * total):  485
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1520)
number of datapoints needed (ratio * total):  760
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1919)
number of datapoints needed (ratio * total):  959
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1490
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1286
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 485
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 760
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 959
})]
length of training data:  4980
training model...
trainable params: 31,010,816 || all params: 8,061,272,064 || trainable%: 0.3847
{'loss': 1.9243, 'grad_norm': 1.2388509511947632, 'learning_rate': 0.00029510603588907015, 'epoch': 0.03}
{'loss': 1.479, 'grad_norm': 0.6257072687149048, 'learning_rate': 0.00028531810766721044, 'epoch': 0.06}
{'loss': 1.1809, 'grad_norm': 0.8294468522071838, 'learning_rate': 0.00027553017944535074, 'epoch': 0.1}
{'loss': 1.2881, 'grad_norm': 0.7082076668739319, 'learning_rate': 0.00026574225122349103, 'epoch': 0.13}
{'loss': 0.972, 'grad_norm': 1.0490260124206543, 'learning_rate': 0.0002559543230016313, 'epoch': 0.16}
{'loss': 1.2129, 'grad_norm': 0.5063419342041016, 'learning_rate': 0.0002461663947797716, 'epoch': 0.19}
{'loss': 1.1808, 'grad_norm': 0.7684871554374695, 'learning_rate': 0.00023637846655791189, 'epoch': 0.22}
{'loss': 0.9496, 'grad_norm': 0.759955644607544, 'learning_rate': 0.00022659053833605218, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1301, 'train_samples_per_second': 49.735, 'train_steps_per_second': 6.222, 'train_loss': 1.259603860832396, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_37/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1511, 0.1216, 0.0000, 0.0526, 0.2264, 0.1330, 0.1167, 0.1986, 0.0291,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9335, 0.1000])
proposed candidate after normalizing: [tensor(0.1511), tensor(0.1216), 0, tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), 1, 1, 1, 1, 1, 1, 119, 0.10000000149011612]
iteration:  38
input_X:  [tensor(0.1511), tensor(0.1216), 0, tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), 1, 1, 1, 1, 1, 1, 119, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  119 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 31,422,464 || all params: 8,061,683,712 || trainable%: 0.3898
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1511), tensor(0.1216), 0, tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1511)
number of datapoints needed (ratio * total):  755
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1216)
number of datapoints needed (ratio * total):  608
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0526)
number of datapoints needed (ratio * total):  263
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.2264)
number of datapoints needed (ratio * total):  1132
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1330)
number of datapoints needed (ratio * total):  664
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1167)
number of datapoints needed (ratio * total):  583
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1986)
number of datapoints needed (ratio * total):  992
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 755
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 608
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 263
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1132
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 664
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 583
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 992
})]
length of training data:  4997
training model...
trainable params: 31,422,464 || all params: 8,061,683,712 || trainable%: 0.3898
{'loss': 1.3714, 'grad_norm': 0.4663703143596649, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 0.8873, 'grad_norm': 2.1745831966400146, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3023, 'grad_norm': 0.7877639532089233, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3095, 'grad_norm': 0.6060256361961365, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.2637, 'grad_norm': 0.7904143333435059, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.204, 'grad_norm': 0.6114010214805603, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.1685, 'grad_norm': 1.0722118616104126, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.0679, 'grad_norm': 0.3183324933052063, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5516, 'train_samples_per_second': 49.696, 'train_steps_per_second': 6.216, 'train_loss': 1.1956328728619743, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_38/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)]]
proposed candidate before processing: tensor([1.8089e-01, 1.8979e-01, 2.6512e-17, 6.1478e-18, 1.2247e-01, 1.3563e-01,
        1.2236e-01, 2.4885e-01, 1.0335e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 9.9766e-01, 8.4955e-02])
proposed candidate after normalizing: [tensor(0.1809), tensor(0.1898), 0, 0, tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), 3, 1, 1, 1, 1, 1, 128, 0.08495503664016724]
iteration:  39
input_X:  [tensor(0.1809), tensor(0.1898), 0, 0, tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), 3, 1, 1, 1, 1, 1, 128, 0.08495503664016724]
mixing data with method:  random
arranging lora config with parameters:  128 0.08495503664016724 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1809), tensor(0.1898), 0, 0, tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1809)
number of datapoints needed (ratio * total):  904
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1898)
number of datapoints needed (ratio * total):  948
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1225)
number of datapoints needed (ratio * total):  612
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1356)
number of datapoints needed (ratio * total):  678
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1224)
number of datapoints needed (ratio * total):  611
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2489)
number of datapoints needed (ratio * total):  1244
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 904
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 948
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 612
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 678
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 611
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1244
})]
length of training data:  4997
training model...
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
{'loss': 2.267, 'grad_norm': 0.8484891057014465, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5358, 'grad_norm': 0.7185088396072388, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.2852, 'grad_norm': 0.42350003123283386, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.4738, 'grad_norm': 1.212048888206482, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.5711, 'grad_norm': 0.4930315315723419, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.5018, 'grad_norm': 0.5248860120773315, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.3102, 'grad_norm': 0.7769863605499268, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.3377, 'grad_norm': 0.9514157772064209, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.263, 'train_samples_per_second': 49.839, 'train_steps_per_second': 6.234, 'train_loss': 1.510441199361279, 'epoch': 0.29}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_39/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.59
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)]]
proposed candidate before processing: tensor([0.1080, 0.2070, 0.0000, 0.0217, 0.1588, 0.1548, 0.2164, 0.1334, 0.0615,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9187, 0.0736])
proposed candidate after normalizing: [tensor(0.1080), tensor(0.2070), 0, 0, tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), 2, 1, 1, 1, 1, 1, 118, 0.07356670498847961]
iteration:  40
input_X:  [tensor(0.1080), tensor(0.2070), 0, 0, tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), 2, 1, 1, 1, 1, 1, 118, 0.07356670498847961]
mixing data with method:  random
arranging lora config with parameters:  118 0.07356670498847961 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 24,973,312 || all params: 8,055,234,560 || trainable%: 0.3100
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1080), tensor(0.2070), 0, 0, tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1080)
number of datapoints needed (ratio * total):  539
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2070)
number of datapoints needed (ratio * total):  1034
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1588)
number of datapoints needed (ratio * total):  793
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1548)
number of datapoints needed (ratio * total):  773
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2164)
number of datapoints needed (ratio * total):  1082
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1334)
number of datapoints needed (ratio * total):  666
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 539
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1034
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 793
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 773
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1082
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 666
})]
length of training data:  4887
training model...
trainable params: 24,973,312 || all params: 8,055,234,560 || trainable%: 0.3100
{'loss': 1.3076, 'grad_norm': 0.5438219904899597, 'learning_rate': 0.00029500831946755406, 'epoch': 0.03}
{'loss': 1.1434, 'grad_norm': 0.501915693283081, 'learning_rate': 0.00028502495840266223, 'epoch': 0.07}
{'loss': 0.8777, 'grad_norm': 2.4915802478790283, 'learning_rate': 0.00027504159733777035, 'epoch': 0.1}
{'loss': 1.0277, 'grad_norm': 0.44996020197868347, 'learning_rate': 0.0002650582362728785, 'epoch': 0.13}
{'loss': 1.0974, 'grad_norm': 0.36679306626319885, 'learning_rate': 0.0002550748752079867, 'epoch': 0.16}
{'loss': 0.8477, 'grad_norm': 0.8793580532073975, 'learning_rate': 0.0002450915141430948, 'epoch': 0.2}
{'loss': 0.8135, 'grad_norm': 0.6393579244613647, 'learning_rate': 0.00023510815307820296, 'epoch': 0.23}
{'loss': 0.8767, 'grad_norm': 0.7673260569572449, 'learning_rate': 0.00022512479201331113, 'epoch': 0.26}
{'loss': 1.0788, 'grad_norm': 0.8153254985809326, 'learning_rate': 0.0002151414309484193, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4034, 'train_samples_per_second': 48.674, 'train_steps_per_second': 6.085, 'train_loss': 1.0067379243912236, 'epoch': 0.3}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_40/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.69
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)]]
proposed candidate before processing: tensor([1.5673e-01, 1.3833e-01, 8.4229e-18, 4.0185e-03, 1.7996e-01, 2.1749e-01,
        2.5510e-01, 4.8367e-02, 6.8114e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.4062e-01, 8.1006e-02])
proposed candidate after normalizing: [tensor(0.1567), tensor(0.1383), 0, 0, tensor(0.1800), tensor(0.2175), tensor(0.2551), 0, 2, 1, 1, 1, 1, 1, 108, 0.08100560307502747]
iteration:  41
input_X:  [tensor(0.1567), tensor(0.1383), 0, 0, tensor(0.1800), tensor(0.2175), tensor(0.2551), 0, 2, 1, 1, 1, 1, 1, 108, 0.08100560307502747]
mixing data with method:  random
arranging lora config with parameters:  108 0.08100560307502747 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 14,819,328 || all params: 8,045,080,576 || trainable%: 0.1842
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1567), tensor(0.1383), 0, 0, tensor(0.1800), tensor(0.2175), tensor(0.2551), 0]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1567)
number of datapoints needed (ratio * total):  783
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1383)
number of datapoints needed (ratio * total):  691
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1800)
number of datapoints needed (ratio * total):  899
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.2175)
number of datapoints needed (ratio * total):  1087
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2551)
number of datapoints needed (ratio * total):  1275
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  0
number of datapoints needed (ratio * total):  0
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 783
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 691
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 899
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1087
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1275
})]
length of training data:  4735
training model...
trainable params: 14,819,328 || all params: 8,045,080,576 || trainable%: 0.1842
{'loss': 1.716, 'grad_norm': 1.1682251691818237, 'learning_rate': 0.0002948453608247422, 'epoch': 0.03}
{'loss': 0.7038, 'grad_norm': 1.9254186153411865, 'learning_rate': 0.0002845360824742268, 'epoch': 0.07}
{'loss': 0.623, 'grad_norm': 0.7277609705924988, 'learning_rate': 0.00027422680412371133, 'epoch': 0.1}
{'loss': 0.6105, 'grad_norm': 0.8348917961120605, 'learning_rate': 0.00026391752577319583, 'epoch': 0.14}
{'loss': 0.6076, 'grad_norm': 0.6286895275115967, 'learning_rate': 0.0002536082474226804, 'epoch': 0.17}
{'loss': 0.5211, 'grad_norm': 0.6296324729919434, 'learning_rate': 0.00024329896907216493, 'epoch': 0.2}
{'loss': 0.4375, 'grad_norm': 0.5668818354606628, 'learning_rate': 0.00023298969072164946, 'epoch': 0.24}
{'loss': 0.5196, 'grad_norm': 1.4303547143936157, 'learning_rate': 0.00022268041237113401, 'epoch': 0.27}
{'loss': 0.506, 'grad_norm': 1.864249348640442, 'learning_rate': 0.00021237113402061854, 'epoch': 0.3}
{'loss': 0.4748, 'grad_norm': 1.1823132038116455, 'learning_rate': 0.0002020618556701031, 'epoch': 0.34}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5732, 'train_samples_per_second': 47.08, 'train_steps_per_second': 5.886, 'train_loss': 0.6622966988142147, 'epoch': 0.36}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_41/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)]]
proposed candidate before processing: tensor([5.2584e-18, 1.9771e-01, 2.0762e-17, 1.2003e-01, 1.8478e-01, 1.1994e-01,
        2.1500e-01, 1.6253e-01, 6.8487e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.9368e-01, 1.0000e-01])
proposed candidate after normalizing: [0, tensor(0.1977), 0, tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), 2, 1, 1, 1, 1, 1, 114, 0.10000000149011612]
iteration:  42
input_X:  [0, tensor(0.1977), 0, tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), 2, 1, 1, 1, 1, 1, 114, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  114 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 15,642,624 || all params: 8,045,903,872 || trainable%: 0.1944
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.1977), 0, tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.1977)
number of datapoints needed (ratio * total):  988
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1200)
number of datapoints needed (ratio * total):  600
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1848)
number of datapoints needed (ratio * total):  923
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1199)
number of datapoints needed (ratio * total):  599
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2150)
number of datapoints needed (ratio * total):  1075
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1625)
number of datapoints needed (ratio * total):  812
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 988
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 600
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 923
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 599
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1075
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 812
})]
length of training data:  4997
training model...
trainable params: 15,642,624 || all params: 8,045,903,872 || trainable%: 0.1944
{'loss': 2.1643, 'grad_norm': 0.8851506114006042, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.2826, 'grad_norm': 0.7219802141189575, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.4716, 'grad_norm': 0.5600650310516357, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.5304, 'grad_norm': 0.8502739667892456, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.399, 'grad_norm': 0.9598034620285034, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1679, 'grad_norm': 0.6466248631477356, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.2867, 'grad_norm': 0.7490165829658508, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6404, 'train_samples_per_second': 49.652, 'train_steps_per_second': 6.21, 'train_loss': 1.4582638678612647, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_42/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)]]
proposed candidate before processing: tensor([1.2361e-01, 1.1448e-01, 0.0000e+00, 0.0000e+00, 2.5318e-01, 1.7688e-01,
        1.4413e-01, 1.8771e-01, 7.4973e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.7562e-01, 1.7319e-18])
proposed candidate after normalizing: [tensor(0.1236), tensor(0.1145), 0, 0, tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), 2, 1, 1, 1, 1, 1, 99, 1.7319464238653134e-18]
iteration:  43
input_X:  [tensor(0.1236), tensor(0.1145), 0, 0, tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), 2, 1, 1, 1, 1, 1, 99, 1.7319464238653134e-18]
mixing data with method:  random
arranging lora config with parameters:  99 1.7319464238653134e-18 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 13,584,384 || all params: 8,043,845,632 || trainable%: 0.1689
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1236), tensor(0.1145), 0, 0, tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1236)
number of datapoints needed (ratio * total):  618
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1145)
number of datapoints needed (ratio * total):  572
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.2532)
number of datapoints needed (ratio * total):  1265
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1769)
number of datapoints needed (ratio * total):  884
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1441)
number of datapoints needed (ratio * total):  720
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1877)
number of datapoints needed (ratio * total):  938
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 618
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 572
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1265
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 884
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 720
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 938
})]
length of training data:  4997
training model...
trainable params: 13,584,384 || all params: 8,043,845,632 || trainable%: 0.1689
{'loss': 2.2108, 'grad_norm': 0.9555730819702148, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.2891, 'grad_norm': 0.7671366333961487, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.6029, 'grad_norm': 1.265960693359375, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.6106, 'grad_norm': 0.5795713067054749, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.7262, 'grad_norm': 1.012191891670227, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1226, 'grad_norm': 0.48267504572868347, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.2387, 'grad_norm': 1.508200764656067, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 0.9435, 'grad_norm': 0.662441611289978, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.0771, 'grad_norm': 0.8643873333930969, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
{'loss': 1.2735, 'grad_norm': 0.5315279960632324, 'learning_rate': 0.00020731707317073167, 'epoch': 0.32}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0493, 'train_samples_per_second': 49.945, 'train_steps_per_second': 6.247, 'train_loss': 1.4173816876990772, 'epoch': 0.34}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_43/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.56
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)]]
proposed candidate before processing: tensor([6.2731e-02, 7.2810e-02, 5.3194e-17, 7.1331e-02, 1.3688e-01, 1.4248e-01,
        3.6078e-01, 1.5298e-01, 7.0883e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.9226e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.0627), tensor(0.0728), 0, tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), 2, 1, 1, 1, 1, 1, 101, 0.10000000149011612]
iteration:  44
input_X:  [tensor(0.0627), tensor(0.0728), 0, tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), 2, 1, 1, 1, 1, 1, 101, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  101 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 13,858,816 || all params: 8,044,120,064 || trainable%: 0.1723
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0627), tensor(0.0728), 0, tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0627)
number of datapoints needed (ratio * total):  313
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0728)
number of datapoints needed (ratio * total):  364
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0713)
number of datapoints needed (ratio * total):  356
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1369)
number of datapoints needed (ratio * total):  684
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1425)
number of datapoints needed (ratio * total):  712
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.3608)
number of datapoints needed (ratio * total):  1803
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1530)
number of datapoints needed (ratio * total):  764
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 313
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 364
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 356
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 684
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 712
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1803
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 764
})]
length of training data:  4996
training model...
trainable params: 13,858,816 || all params: 8,044,120,064 || trainable%: 0.1723
{'loss': 2.2695, 'grad_norm': 0.8840143084526062, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5928, 'grad_norm': 1.2345638275146484, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.4588, 'grad_norm': 0.622990608215332, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.2493, 'grad_norm': 0.8383980989456177, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.1584, 'grad_norm': 0.9747459292411804, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.4102, 'grad_norm': 0.5131556391716003, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.4086, 'grad_norm': 0.5625510215759277, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.1977, 'grad_norm': 0.7731711864471436, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.3118, 'grad_norm': 0.9330782890319824, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3413, 'train_samples_per_second': 49.79, 'train_steps_per_second': 6.229, 'train_loss': 1.4471523243448008, 'epoch': 0.29}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_44/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.64
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1369, 0.1795, 0.0000, 0.1366, 0.1291, 0.1778, 0.1778, 0.0622, 0.0650,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8133, 0.1000])
proposed candidate after normalizing: [tensor(0.1369), tensor(0.1795), 0, tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), 2, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
iteration:  45
input_X:  [tensor(0.1369), tensor(0.1795), 0, tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), 2, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  104 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 14,270,464 || all params: 8,044,531,712 || trainable%: 0.1774
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1369), tensor(0.1795), 0, tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1369)
number of datapoints needed (ratio * total):  684
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1795)
number of datapoints needed (ratio * total):  897
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1366)
number of datapoints needed (ratio * total):  682
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1291)
number of datapoints needed (ratio * total):  645
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1778)
number of datapoints needed (ratio * total):  888
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1778)
number of datapoints needed (ratio * total):  888
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.0622)
number of datapoints needed (ratio * total):  311
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 684
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 897
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 682
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 645
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 888
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 888
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 311
})]
length of training data:  4995
training model...
trainable params: 14,270,464 || all params: 8,044,531,712 || trainable%: 0.1774
{'loss': 1.8573, 'grad_norm': 3.6632375717163086, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 0.7792, 'grad_norm': 1.4188367128372192, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 0.9016, 'grad_norm': 0.7936530709266663, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.0937, 'grad_norm': 0.5046960115432739, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.0625, 'grad_norm': 0.6028763651847839, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 0.6408, 'grad_norm': 1.0743727684020996, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.0107, 'grad_norm': 0.4831741452217102, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6864, 'train_samples_per_second': 49.609, 'train_steps_per_second': 6.207, 'train_loss': 1.027842185436151, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_45/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.59
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1168, 0.1473, 0.0197, 0.0000, 0.2238, 0.0617, 0.2960, 0.1347, 0.0669,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9118, 0.1000])
proposed candidate after normalizing: [tensor(0.1168), tensor(0.1473), 0, 0, tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), 2, 1, 1, 1, 1, 1, 117, 0.10000000149011612]
iteration:  46
input_X:  [tensor(0.1168), tensor(0.1473), 0, 0, tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), 2, 1, 1, 1, 1, 1, 117, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  117 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 16,054,272 || all params: 8,046,315,520 || trainable%: 0.1995
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1168), tensor(0.1473), 0, 0, tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1168)
number of datapoints needed (ratio * total):  584
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1473)
number of datapoints needed (ratio * total):  736
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.2238)
number of datapoints needed (ratio * total):  1118
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0617)
number of datapoints needed (ratio * total):  308
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2960)
number of datapoints needed (ratio * total):  1479
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1347)
number of datapoints needed (ratio * total):  673
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 584
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 736
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1118
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 308
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1479
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 673
})]
length of training data:  4898
training model...
trainable params: 16,054,272 || all params: 8,046,315,520 || trainable%: 0.1995
{'loss': 2.2065, 'grad_norm': 1.702887773513794, 'learning_rate': 0.0002950248756218905, 'epoch': 0.03}
{'loss': 1.4735, 'grad_norm': 0.8335264921188354, 'learning_rate': 0.00028507462686567164, 'epoch': 0.07}
{'loss': 1.1899, 'grad_norm': 1.2168824672698975, 'learning_rate': 0.00027512437810945273, 'epoch': 0.1}
{'loss': 1.1641, 'grad_norm': 1.0976402759552002, 'learning_rate': 0.0002651741293532338, 'epoch': 0.13}
{'loss': 1.092, 'grad_norm': 0.48002082109451294, 'learning_rate': 0.0002552238805970149, 'epoch': 0.16}
{'loss': 1.3436, 'grad_norm': 0.46496301889419556, 'learning_rate': 0.000245273631840796, 'epoch': 0.2}
{'loss': 1.1501, 'grad_norm': 0.5476822257041931, 'learning_rate': 0.0002353233830845771, 'epoch': 0.23}
{'loss': 1.1617, 'grad_norm': 0.8852890133857727, 'learning_rate': 0.0002253731343283582, 'epoch': 0.26}
{'loss': 0.976, 'grad_norm': 1.0409666299819946, 'learning_rate': 0.00021542288557213927, 'epoch': 0.29}
{'loss': 1.0249, 'grad_norm': 0.605350911617279, 'learning_rate': 0.0002054726368159204, 'epoch': 0.33}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6672, 'train_samples_per_second': 48.655, 'train_steps_per_second': 6.089, 'train_loss': 1.2604802131652832, 'epoch': 0.34}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_46/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.59
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)]]
proposed candidate before processing: tensor([0.0000, 0.1708, 0.0000, 0.0000, 0.1361, 0.2795, 0.2340, 0.1796, 0.0685,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8525, 0.1000])
proposed candidate after normalizing: [0, tensor(0.1708), 0, 0, tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), 2, 1, 1, 1, 1, 1, 109, 0.10000000149011612]
iteration:  47
input_X:  [0, tensor(0.1708), 0, 0, tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), 2, 1, 1, 1, 1, 1, 109, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  109 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 14,956,544 || all params: 8,045,217,792 || trainable%: 0.1859
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.1708), 0, 0, tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.1708)
number of datapoints needed (ratio * total):  854
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1361)
number of datapoints needed (ratio * total):  680
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.2795)
number of datapoints needed (ratio * total):  1397
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2340)
number of datapoints needed (ratio * total):  1169
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1796)
number of datapoints needed (ratio * total):  898
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 854
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 680
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1397
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1169
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 898
})]
length of training data:  4998
training model...
trainable params: 14,956,544 || all params: 8,045,217,792 || trainable%: 0.1859
{'loss': 2.0674, 'grad_norm': 0.6386219263076782, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.8317, 'grad_norm': 0.7854676842689514, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.2301, 'grad_norm': 0.9508220553398132, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.2016, 'grad_norm': 0.5122632384300232, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.2009, 'grad_norm': 0.6099231243133545, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.4184, 'grad_norm': 0.925971269607544, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.1077, 'grad_norm': 0.8037106394767761, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.3365, 'grad_norm': 0.7099247574806213, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.5, 'grad_norm': 0.5202586650848389, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6756, 'train_samples_per_second': 49.645, 'train_steps_per_second': 6.208, 'train_loss': 1.416580451926604, 'epoch': 0.32}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_47/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)]]
proposed candidate before processing: tensor([9.8137e-02, 2.8935e-01, 0.0000e+00, 7.6789e-17, 4.9028e-02, 6.6407e-02,
        2.7578e-01, 2.2130e-01, 1.0766e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.0540e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.0981), tensor(0.2893), 0, 0, 0, tensor(0.0664), tensor(0.2758), tensor(0.2213), 3, 1, 1, 1, 1, 1, 103, 0.10000000149011612]
iteration:  48
input_X:  [tensor(0.0981), tensor(0.2893), 0, 0, 0, tensor(0.0664), tensor(0.2758), tensor(0.2213), 3, 1, 1, 1, 1, 1, 103, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  103 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 21,199,872 || all params: 8,051,461,120 || trainable%: 0.2633
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0981), tensor(0.2893), 0, 0, 0, tensor(0.0664), tensor(0.2758), tensor(0.2213)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0981)
number of datapoints needed (ratio * total):  490
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2893)
number of datapoints needed (ratio * total):  1446
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  tensor(0.0664)
number of datapoints needed (ratio * total):  332
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2758)
number of datapoints needed (ratio * total):  1378
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2213)
number of datapoints needed (ratio * total):  1106
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 490
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1446
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 332
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1378
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1106
})]
length of training data:  4752
training model...
trainable params: 21,199,872 || all params: 8,051,461,120 || trainable%: 0.2633
{'loss': 2.1571, 'grad_norm': 0.5426724553108215, 'learning_rate': 0.00029486301369863015, 'epoch': 0.03}
{'loss': 1.6329, 'grad_norm': 1.7193596363067627, 'learning_rate': 0.0002845890410958904, 'epoch': 0.07}
{'loss': 1.3563, 'grad_norm': 0.6565366387367249, 'learning_rate': 0.0002743150684931507, 'epoch': 0.1}
{'loss': 1.4905, 'grad_norm': 1.0865848064422607, 'learning_rate': 0.0002640410958904109, 'epoch': 0.13}
{'loss': 1.3021, 'grad_norm': 1.4925096035003662, 'learning_rate': 0.0002537671232876712, 'epoch': 0.17}
{'loss': 1.4539, 'grad_norm': 0.683375895023346, 'learning_rate': 0.0002434931506849315, 'epoch': 0.2}
{'loss': 1.2306, 'grad_norm': 0.5799207091331482, 'learning_rate': 0.00023321917808219177, 'epoch': 0.24}
{'loss': 1.0945, 'grad_norm': 0.8459985852241516, 'learning_rate': 0.00022294520547945203, 'epoch': 0.27}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3395, 'train_samples_per_second': 47.359, 'train_steps_per_second': 5.92, 'train_loss': 1.43887258978451, 'epoch': 0.29}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_48/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.6
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)]]
proposed candidate before processing: tensor([1.1808e-01, 1.5667e-01, 3.0493e-18, 1.2184e-17, 1.3979e-01, 2.2625e-01,
        1.6965e-01, 1.8956e-01, 2.9239e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.5052e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1181), tensor(0.1567), 0, 0, tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), 1, 1, 1, 1, 1, 1, 109, 0.10000000149011612]
iteration:  49
input_X:  [tensor(0.1181), tensor(0.1567), 0, 0, tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), 1, 1, 1, 1, 1, 1, 109, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  109 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 21,611,520 || all params: 8,051,872,768 || trainable%: 0.2684
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1181), tensor(0.1567), 0, 0, tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1181)
number of datapoints needed (ratio * total):  590
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1567)
number of datapoints needed (ratio * total):  783
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1398)
number of datapoints needed (ratio * total):  698
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.2262)
number of datapoints needed (ratio * total):  1131
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1696)
number of datapoints needed (ratio * total):  848
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1896)
number of datapoints needed (ratio * total):  947
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 590
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 783
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 698
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1131
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 848
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 947
})]
length of training data:  4997
training model...
trainable params: 21,611,520 || all params: 8,051,872,768 || trainable%: 0.2684
{'loss': 1.1539, 'grad_norm': 0.6342682838439941, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 0.9839, 'grad_norm': 1.0188114643096924, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3383, 'grad_norm': 0.6814745664596558, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.1882, 'grad_norm': 1.1807092428207397, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.208, 'grad_norm': 0.34172841906547546, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1704, 'grad_norm': 0.8387926816940308, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.1483, 'grad_norm': 0.7283990979194641, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.1408, 'grad_norm': 1.1620670557022095, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.2051, 'grad_norm': 0.9693527817726135, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6078, 'train_samples_per_second': 49.668, 'train_steps_per_second': 6.212, 'train_loss': 1.1610626578330994, 'epoch': 0.31}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_49/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1949, 0.1402, 0.0000, 0.0000, 0.1789, 0.1217, 0.1663, 0.1981, 0.0286,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7961, 0.1000])
proposed candidate after normalizing: [tensor(0.1949), tensor(0.1402), 0, 0, tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), 1, 1, 1, 1, 1, 1, 102, 0.10000000149011612]
iteration:  50
input_X:  [tensor(0.1949), tensor(0.1402), 0, 0, tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), 1, 1, 1, 1, 1, 1, 102, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  102 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 6,998,016 || all params: 8,037,259,264 || trainable%: 0.0871
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1949), tensor(0.1402), 0, 0, tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1949)
number of datapoints needed (ratio * total):  974
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1402)
number of datapoints needed (ratio * total):  700
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1789)
number of datapoints needed (ratio * total):  894
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1217)
number of datapoints needed (ratio * total):  608
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1663)
number of datapoints needed (ratio * total):  831
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1981)
number of datapoints needed (ratio * total):  990
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 974
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 700
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 894
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 608
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 831
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 990
})]
length of training data:  4997
training model...
trainable params: 6,998,016 || all params: 8,037,259,264 || trainable%: 0.0871
{'loss': 2.4607, 'grad_norm': 1.246958613395691, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.3904, 'grad_norm': 0.4915686547756195, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3242, 'grad_norm': 0.945834755897522, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.5419, 'grad_norm': 0.409479022026062, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.637, 'grad_norm': 0.8800978660583496, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.3149, 'grad_norm': 2.4413771629333496, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.676, 'grad_norm': 0.9893025755882263, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.175, 'grad_norm': 0.9498654007911682, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.3283, 'grad_norm': 0.45327505469322205, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'loss': 1.3078, 'grad_norm': 0.3747355341911316, 'learning_rate': 0.00020731707317073167, 'epoch': 0.32}
{'train_runtime': 100.3083, 'train_samples_per_second': 49.816, 'train_steps_per_second': 6.231, 'train_loss': 1.515628309249878, 'epoch': 0.32}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_50/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.69
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)]]
proposed candidate before processing: tensor([2.2470e-01, 1.1767e-01, 1.6487e-17, 3.3380e-17, 1.7917e-01, 1.4802e-01,
        2.1192e-01, 1.1852e-01, 2.8627e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.3374e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.2247), tensor(0.1177), 0, 0, tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), 1, 1, 1, 1, 1, 1, 107, 0.10000000149011612]
iteration:  51
input_X:  [tensor(0.2247), tensor(0.1177), 0, 0, tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), 1, 1, 1, 1, 1, 1, 107, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  107 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,341,056 || all params: 8,037,602,304 || trainable%: 0.0913
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.2247), tensor(0.1177), 0, 0, tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.2247)
number of datapoints needed (ratio * total):  1123
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1177)
number of datapoints needed (ratio * total):  588
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1792)
number of datapoints needed (ratio * total):  895
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1480)
number of datapoints needed (ratio * total):  740
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2119)
number of datapoints needed (ratio * total):  1059
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1185)
number of datapoints needed (ratio * total):  592
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1123
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 588
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 895
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 740
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1059
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 592
})]
length of training data:  4997
training model...
trainable params: 7,341,056 || all params: 8,037,602,304 || trainable%: 0.0913
{'loss': 2.2811, 'grad_norm': 5.163445472717285, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.3923, 'grad_norm': 0.489165335893631, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.0256, 'grad_norm': 0.4099595546722412, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3028, 'grad_norm': 0.4695115387439728, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.4226, 'grad_norm': 0.3753325939178467, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.107, 'grad_norm': 1.54525887966156, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 0.9988, 'grad_norm': 0.4442306458950043, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.0645, 'grad_norm': 0.8621534109115601, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.0052, 'grad_norm': 1.0108271837234497, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
{'loss': 1.0921, 'grad_norm': 0.684823215007782, 'learning_rate': 0.00020731707317073167, 'epoch': 0.32}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2383, 'train_samples_per_second': 49.851, 'train_steps_per_second': 6.235, 'train_loss': 1.2802388159584661, 'epoch': 0.34}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_51/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.69
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)]]
proposed candidate before processing: tensor([1.3813e-01, 2.1550e-01, 3.6727e-18, 0.0000e+00, 1.9895e-01, 1.3278e-01,
        1.9141e-01, 1.2323e-01, 2.8695e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.2229e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1381), tensor(0.2155), 0, 0, tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), 1, 1, 1, 1, 1, 1, 105, 0.10000000149011612]
iteration:  52
input_X:  [tensor(0.1381), tensor(0.2155), 0, 0, tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), 1, 1, 1, 1, 1, 1, 105, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  105 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,203,840 || all params: 8,037,465,088 || trainable%: 0.0896
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1381), tensor(0.2155), 0, 0, tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1381)
number of datapoints needed (ratio * total):  690
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2155)
number of datapoints needed (ratio * total):  1077
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1989)
number of datapoints needed (ratio * total):  994
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1328)
number of datapoints needed (ratio * total):  663
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1914)
number of datapoints needed (ratio * total):  957
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1232)
number of datapoints needed (ratio * total):  616
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 690
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1077
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 994
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 663
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 957
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 616
})]
length of training data:  4997
training model...
trainable params: 7,203,840 || all params: 8,037,465,088 || trainable%: 0.0896
{'loss': 2.2078, 'grad_norm': 1.9463640451431274, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.1674, 'grad_norm': 0.8796271681785583, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.6395, 'grad_norm': 1.423256754875183, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 0.9405, 'grad_norm': 0.5067680478096008, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.1534, 'grad_norm': 0.49503299593925476, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.4574, 'grad_norm': 0.40706539154052734, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 0.9558, 'grad_norm': 1.0046995878219604, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.2554, 'grad_norm': 0.8372898697853088, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 0.9287, 'grad_norm': 1.5405222177505493, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5651, 'train_samples_per_second': 49.689, 'train_steps_per_second': 6.215, 'train_loss': 1.3017430228156013, 'epoch': 0.3}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_52/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.68
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)]]
proposed candidate before processing: tensor([1.6263e-18, 1.1304e-01, 0.0000e+00, 1.2091e-01, 2.0005e-01, 1.5402e-01,
        2.7163e-01, 1.4035e-01, 7.6569e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.5049e-01, 1.0000e-01])
proposed candidate after normalizing: [0, tensor(0.1130), 0, tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), 2, 1, 1, 1, 1, 1, 96, 0.10000000149011612]
iteration:  53
input_X:  [0, tensor(0.1130), 0, tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), 2, 1, 1, 1, 1, 1, 96, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  96 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 13,172,736 || all params: 8,043,433,984 || trainable%: 0.1638
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0, tensor(0.1130), 0, tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  headqa_en
ratio:  tensor(0.1130)
number of datapoints needed (ratio * total):  565
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1209)
number of datapoints needed (ratio * total):  604
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.2000)
number of datapoints needed (ratio * total):  1000
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1540)
number of datapoints needed (ratio * total):  770
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2716)
number of datapoints needed (ratio * total):  1358
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1404)
number of datapoints needed (ratio * total):  701
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 565
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 604
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1000
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 770
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1358
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 701
})]
length of training data:  4998
training model...
trainable params: 13,172,736 || all params: 8,043,433,984 || trainable%: 0.1638
{'loss': 2.2877, 'grad_norm': 0.43372711539268494, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.6495, 'grad_norm': 1.262359619140625, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3157, 'grad_norm': 0.5348338484764099, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.2622, 'grad_norm': 1.1844924688339233, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.2977, 'grad_norm': 1.2218172550201416, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.3112, 'grad_norm': 0.9888662099838257, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.586, 'grad_norm': 0.8485232591629028, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.134, 'grad_norm': 0.7523880004882812, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0281, 'train_samples_per_second': 49.966, 'train_steps_per_second': 6.248, 'train_loss': 1.4627358210569172, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_53/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)]]
proposed candidate before processing: tensor([0.2408, 0.1473, 0.0000, 0.0347, 0.1647, 0.1614, 0.1292, 0.1219, 0.0289,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8009, 0.1000])
proposed candidate after normalizing: [tensor(0.2408), tensor(0.1473), 0, 0, tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), 1, 1, 1, 1, 1, 1, 103, 0.10000000149011612]
iteration:  54
input_X:  [tensor(0.2408), tensor(0.1473), 0, 0, tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), 1, 1, 1, 1, 1, 1, 103, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  103 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 13,652,992 || all params: 8,043,914,240 || trainable%: 0.1697
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.2408), tensor(0.1473), 0, 0, tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.2408)
number of datapoints needed (ratio * total):  1203
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1473)
number of datapoints needed (ratio * total):  736
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1647)
number of datapoints needed (ratio * total):  823
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1614)
number of datapoints needed (ratio * total):  806
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1292)
number of datapoints needed (ratio * total):  646
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1219)
number of datapoints needed (ratio * total):  609
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1203
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 736
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 823
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 806
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 646
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 609
})]
length of training data:  4823
training model...
trainable params: 13,652,992 || all params: 8,043,914,240 || trainable%: 0.1697
{'loss': 0.98, 'grad_norm': 1.008767008781433, 'learning_rate': 0.0002949409780775716, 'epoch': 0.03}
{'loss': 1.1606, 'grad_norm': 0.536393940448761, 'learning_rate': 0.00028482293423271496, 'epoch': 0.07}
{'loss': 1.0599, 'grad_norm': 0.8296692967414856, 'learning_rate': 0.00027470489038785835, 'epoch': 0.1}
{'loss': 1.0109, 'grad_norm': 2.080070734024048, 'learning_rate': 0.00026458684654300164, 'epoch': 0.13}
{'loss': 1.116, 'grad_norm': 0.759255588054657, 'learning_rate': 0.000254468802698145, 'epoch': 0.17}
{'loss': 0.9959, 'grad_norm': 0.6582245826721191, 'learning_rate': 0.00024435075885328837, 'epoch': 0.2}
{'loss': 1.3269, 'grad_norm': 0.840040385723114, 'learning_rate': 0.00023423271500843168, 'epoch': 0.23}
{'loss': 0.7942, 'grad_norm': 0.6116704940795898, 'learning_rate': 0.00022411467116357502, 'epoch': 0.27}
{'loss': 0.7823, 'grad_norm': 0.7197445034980774, 'learning_rate': 0.00021399662731871836, 'epoch': 0.3}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2013, 'train_samples_per_second': 48.133, 'train_steps_per_second': 6.018, 'train_loss': 1.045358760540302, 'epoch': 0.32}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_54/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)]]
proposed candidate before processing: tensor([1.5292e-01, 7.1929e-02, 0.0000e+00, 6.3744e-17, 1.9898e-01, 1.3885e-01,
        2.4295e-01, 1.9438e-01, 2.8804e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.2250e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1529), tensor(0.0719), 0, 0, tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), 1, 1, 1, 1, 1, 1, 105, 0.10000000149011612]
iteration:  55
input_X:  [tensor(0.1529), tensor(0.0719), 0, 0, tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), 1, 1, 1, 1, 1, 1, 105, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  105 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,203,840 || all params: 8,037,465,088 || trainable%: 0.0896
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1529), tensor(0.0719), 0, 0, tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1529)
number of datapoints needed (ratio * total):  764
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0719)
number of datapoints needed (ratio * total):  359
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1990)
number of datapoints needed (ratio * total):  994
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1389)
number of datapoints needed (ratio * total):  694
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2429)
number of datapoints needed (ratio * total):  1214
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1944)
number of datapoints needed (ratio * total):  971
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 764
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 359
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 994
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 694
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1214
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 971
})]
length of training data:  4996
training model...
trainable params: 7,203,840 || all params: 8,037,465,088 || trainable%: 0.0896
{'loss': 2.4679, 'grad_norm': 1.7761059999465942, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5767, 'grad_norm': 0.4655993580818176, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.6444, 'grad_norm': 0.7942822575569153, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.5492, 'grad_norm': 1.1549042463302612, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3765, 'grad_norm': 1.5328147411346436, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.7193, 'grad_norm': 0.676190197467804, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.3037, 'grad_norm': 1.2386432886123657, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.6856, 'grad_norm': 1.1392567157745361, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.6251, 'grad_norm': 1.1568315029144287, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
{'loss': 1.4568, 'grad_norm': 1.16720712184906, 'learning_rate': 0.00020731707317073167, 'epoch': 0.32}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2239, 'train_samples_per_second': 49.848, 'train_steps_per_second': 6.236, 'train_loss': 1.6338639455298856, 'epoch': 0.35}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_55/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.7
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1407, 0.0728, 0.0839, 0.0000, 0.1875, 0.1259, 0.2057, 0.1835, 0.0285,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8296, 0.1000])
proposed candidate after normalizing: [tensor(0.1407), tensor(0.0728), tensor(0.0839), 0, tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), 1, 1, 1, 1, 1, 1, 106, 0.10000000149011612]
iteration:  56
input_X:  [tensor(0.1407), tensor(0.0728), tensor(0.0839), 0, tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), 1, 1, 1, 1, 1, 1, 106, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  106 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,272,448 || all params: 8,037,533,696 || trainable%: 0.0905
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1407), tensor(0.0728), tensor(0.0839), 0, tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1407)
number of datapoints needed (ratio * total):  703
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0728)
number of datapoints needed (ratio * total):  363
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0839)
number of datapoints needed (ratio * total):  419
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1875)
number of datapoints needed (ratio * total):  937
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1259)
number of datapoints needed (ratio * total):  629
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2057)
number of datapoints needed (ratio * total):  1028
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1835)
number of datapoints needed (ratio * total):  917
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 703
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 363
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 419
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 937
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 629
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1028
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 917
})]
length of training data:  4996
training model...
trainable params: 7,272,448 || all params: 8,037,533,696 || trainable%: 0.0905
{'loss': 2.3575, 'grad_norm': 2.5357987880706787, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.8865, 'grad_norm': 0.39529094099998474, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.5223, 'grad_norm': 0.7876579165458679, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.488, 'grad_norm': 0.5296372771263123, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.307, 'grad_norm': 1.1331403255462646, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.3675, 'grad_norm': 0.46678611636161804, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.7074, 'grad_norm': 0.5545819997787476, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.6489, 'grad_norm': 0.538323163986206, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.5143, 'grad_norm': 0.3862540125846863, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2954, 'train_samples_per_second': 49.813, 'train_steps_per_second': 6.232, 'train_loss': 1.635550232037254, 'epoch': 0.29}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_56/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.68
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1762, 0.0982, 0.0000, 0.0000, 0.1789, 0.0902, 0.2359, 0.2206, 0.0281,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8414, 0.1000])
proposed candidate after normalizing: [tensor(0.1762), tensor(0.0982), 0, 0, tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), 1, 1, 1, 1, 1, 1, 108, 0.10000000149011612]
iteration:  57
input_X:  [tensor(0.1762), tensor(0.0982), 0, 0, tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), 1, 1, 1, 1, 1, 1, 108, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  108 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,409,664 || all params: 8,037,670,912 || trainable%: 0.0922
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1762), tensor(0.0982), 0, 0, tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1762)
number of datapoints needed (ratio * total):  881
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0982)
number of datapoints needed (ratio * total):  490
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1789)
number of datapoints needed (ratio * total):  894
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0902)
number of datapoints needed (ratio * total):  451
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2359)
number of datapoints needed (ratio * total):  1179
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2206)
number of datapoints needed (ratio * total):  1103
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 881
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 490
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 894
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 451
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1179
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1103
})]
length of training data:  4998
training model...
trainable params: 7,409,664 || all params: 8,037,670,912 || trainable%: 0.0922
{'loss': 2.3852, 'grad_norm': 0.6044285297393799, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 2.0309, 'grad_norm': 1.4938002824783325, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3895, 'grad_norm': 0.9005882143974304, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3, 'grad_norm': 1.154171109199524, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.5046, 'grad_norm': 0.6634635925292969, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.6822, 'grad_norm': 0.49068400263786316, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.7916, 'grad_norm': 0.5084783434867859, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.609, 'grad_norm': 0.4495849013328552, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.5506, 'grad_norm': 1.1384680271148682, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
{'loss': 1.5603, 'grad_norm': 1.9319952726364136, 'learning_rate': 0.00020731707317073167, 'epoch': 0.32}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1263, 'train_samples_per_second': 49.917, 'train_steps_per_second': 6.242, 'train_loss': 1.652514965734749, 'epoch': 0.34}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_57/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)]]
proposed candidate before processing: tensor([1.2200e-01, 7.7725e-02, 6.5276e-03, 2.0849e-17, 2.4176e-01, 2.2886e-01,
        2.0132e-01, 1.2181e-01, 3.0240e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.8551e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1220), tensor(0.0777), 0, 0, tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), 1, 1, 1, 1, 1, 1, 101, 0.10000000149011612]
iteration:  58
input_X:  [tensor(0.1220), tensor(0.0777), 0, 0, tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), 1, 1, 1, 1, 1, 1, 101, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  101 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 6,929,408 || all params: 8,037,190,656 || trainable%: 0.0862
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1220), tensor(0.0777), 0, 0, tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1220)
number of datapoints needed (ratio * total):  609
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0777)
number of datapoints needed (ratio * total):  388
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.2418)
number of datapoints needed (ratio * total):  1208
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.2289)
number of datapoints needed (ratio * total):  1144
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2013)
number of datapoints needed (ratio * total):  1006
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1218)
number of datapoints needed (ratio * total):  609
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 609
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 388
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1208
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1144
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1006
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 609
})]
length of training data:  4964
training model...
trainable params: 6,929,408 || all params: 8,037,190,656 || trainable%: 0.0862
{'loss': 2.3508, 'grad_norm': 0.8483365774154663, 'learning_rate': 0.0002950900163666121, 'epoch': 0.03}
{'loss': 1.2053, 'grad_norm': 0.6643141508102417, 'learning_rate': 0.0002852700490998363, 'epoch': 0.06}
{'loss': 1.4627, 'grad_norm': 0.4333076775074005, 'learning_rate': 0.00027545008183306056, 'epoch': 0.1}
{'loss': 1.2843, 'grad_norm': 1.858532190322876, 'learning_rate': 0.0002656301145662848, 'epoch': 0.13}
{'loss': 1.1937, 'grad_norm': 0.9212784767150879, 'learning_rate': 0.000255810147299509, 'epoch': 0.16}
{'loss': 1.156, 'grad_norm': 1.0186121463775635, 'learning_rate': 0.0002459901800327332, 'epoch': 0.19}
{'loss': 1.0871, 'grad_norm': 0.622584879398346, 'learning_rate': 0.00023617021276595742, 'epoch': 0.23}
{'loss': 1.2313, 'grad_norm': 0.32317960262298584, 'learning_rate': 0.00022635024549918164, 'epoch': 0.26}
{'loss': 0.9837, 'grad_norm': 0.42306774854660034, 'learning_rate': 0.00021653027823240585, 'epoch': 0.29}
{'loss': 1.0774, 'grad_norm': 0.5183143019676208, 'learning_rate': 0.00020671031096563012, 'epoch': 0.32}
{'loss': 1.1805, 'grad_norm': 0.8686748743057251, 'learning_rate': 0.00019689034369885434, 'epoch': 0.35}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3879, 'train_samples_per_second': 49.448, 'train_steps_per_second': 6.186, 'train_loss': 1.2928030992809094, 'epoch': 0.37}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_58/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.68
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)]]
proposed candidate before processing: tensor([1.3149e-01, 1.1153e-01, 7.6408e-02, 1.8296e-17, 1.9408e-01, 1.8980e-01,
        2.0370e-01, 9.2997e-02, 2.9053e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.1393e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1315), tensor(0.1115), tensor(0.0764), 0, tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), 1, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
iteration:  59
input_X:  [tensor(0.1315), tensor(0.1115), tensor(0.0764), 0, tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), 1, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  104 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,135,232 || all params: 8,037,396,480 || trainable%: 0.0888
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1315), tensor(0.1115), tensor(0.0764), 0, tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1315)
number of datapoints needed (ratio * total):  657
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1115)
number of datapoints needed (ratio * total):  557
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0764)
number of datapoints needed (ratio * total):  382
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1941)
number of datapoints needed (ratio * total):  970
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1898)
number of datapoints needed (ratio * total):  948
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2037)
number of datapoints needed (ratio * total):  1018
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.0930)
number of datapoints needed (ratio * total):  464
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 657
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 557
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 382
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 970
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 948
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1018
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 464
})]
length of training data:  4996
training model...
trainable params: 7,135,232 || all params: 8,037,396,480 || trainable%: 0.0888
{'loss': 2.1397, 'grad_norm': 1.748700737953186, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.0017, 'grad_norm': 1.029289722442627, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.117, 'grad_norm': 2.210477590560913, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.1104, 'grad_norm': 1.1107773780822754, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.1887, 'grad_norm': 1.1492332220077515, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.2721, 'grad_norm': 0.8420923948287964, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.2685, 'grad_norm': 0.5267497301101685, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.0225, 'grad_norm': 1.1637078523635864, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'loss': 1.1041, 'grad_norm': 0.48441681265830994, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
{'train_runtime': 100.6608, 'train_samples_per_second': 49.632, 'train_steps_per_second': 6.209, 'train_loss': 1.247198486328125, 'epoch': 0.29}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_59/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)]]
proposed candidate before processing: tensor([1.3598e-01, 1.0546e-01, 1.6670e-18, 1.6263e-18, 2.2874e-01, 1.7365e-01,
        1.4278e-01, 2.1339e-01, 2.9560e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.5882e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1360), tensor(0.1055), 0, 0, tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), 1, 1, 1, 1, 1, 1, 97, 0.10000000149011612]
iteration:  60
input_X:  [tensor(0.1360), tensor(0.1055), 0, 0, tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), 1, 1, 1, 1, 1, 1, 97, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  97 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 6,654,976 || all params: 8,036,916,224 || trainable%: 0.0828
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1360), tensor(0.1055), 0, 0, tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1360)
number of datapoints needed (ratio * total):  679
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1055)
number of datapoints needed (ratio * total):  527
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.2287)
number of datapoints needed (ratio * total):  1143
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1736)
number of datapoints needed (ratio * total):  868
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1428)
number of datapoints needed (ratio * total):  713
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2134)
number of datapoints needed (ratio * total):  1066
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 679
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 527
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1143
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 868
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 713
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1066
})]
length of training data:  4996
training model...
trainable params: 6,654,976 || all params: 8,036,916,224 || trainable%: 0.0828
{'loss': 2.4227, 'grad_norm': 0.7458632588386536, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 2.063, 'grad_norm': 10.644370079040527, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3683, 'grad_norm': 0.3913951814174652, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.4185, 'grad_norm': 0.5885955095291138, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.5685, 'grad_norm': 0.5193697810173035, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.5374, 'grad_norm': 0.6608896851539612, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.7577, 'grad_norm': 0.4192332327365875, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.8271, 'grad_norm': 1.01539945602417, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.712, 'grad_norm': 0.8773285150527954, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
{'loss': 1.2525, 'grad_norm': 1.318949580192566, 'learning_rate': 0.00020731707317073167, 'epoch': 0.32}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0985, 'train_samples_per_second': 49.911, 'train_steps_per_second': 6.244, 'train_loss': 1.6647377279069688, 'epoch': 0.35}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_60/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1907, 0.1925, 0.0449, 0.0201, 0.0409, 0.1025, 0.1310, 0.2775, 0.1460,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8402, 0.0833])
proposed candidate after normalizing: [tensor(0.1907), tensor(0.1925), 0, 0, 0, tensor(0.1025), tensor(0.1310), tensor(0.2775), 5, 1, 1, 1, 1, 1, 108, 0.08331970870494843]
iteration:  61
input_X:  [tensor(0.1907), tensor(0.1925), 0, 0, 0, tensor(0.1025), tensor(0.1310), tensor(0.2775), 5, 1, 1, 1, 1, 1, 108, 0.08331970870494843]
mixing data with method:  random
arranging lora config with parameters:  108 0.08331970870494843 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 37,048,320 || all params: 8,067,309,568 || trainable%: 0.4592
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1907), tensor(0.1925), 0, 0, 0, tensor(0.1025), tensor(0.1310), tensor(0.2775)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1907)
number of datapoints needed (ratio * total):  953
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1925)
number of datapoints needed (ratio * total):  962
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  tensor(0.1025)
number of datapoints needed (ratio * total):  512
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1310)
number of datapoints needed (ratio * total):  654
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2775)
number of datapoints needed (ratio * total):  1387
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 953
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 962
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 512
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 654
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1387
})]
length of training data:  4468
training model...
trainable params: 37,048,320 || all params: 8,067,309,568 || trainable%: 0.4592
{'loss': 2.0699, 'grad_norm': 0.39717572927474976, 'learning_rate': 0.00029453551912568304, 'epoch': 0.04}
{'loss': 1.8303, 'grad_norm': 0.8077909350395203, 'learning_rate': 0.0002836065573770492, 'epoch': 0.07}
{'loss': 1.8178, 'grad_norm': 0.417624294757843, 'learning_rate': 0.0002726775956284153, 'epoch': 0.11}
{'loss': 1.5905, 'grad_norm': 0.38080722093582153, 'learning_rate': 0.0002617486338797814, 'epoch': 0.14}
{'loss': 1.4135, 'grad_norm': 0.7181033492088318, 'learning_rate': 0.00025081967213114756, 'epoch': 0.18}
{'loss': 1.427, 'grad_norm': 0.7010599374771118, 'learning_rate': 0.00023989071038251365, 'epoch': 0.21}
{'loss': 1.2404, 'grad_norm': 0.6444395780563354, 'learning_rate': 0.00022896174863387976, 'epoch': 0.25}
{'loss': 1.3827, 'grad_norm': 0.4457475244998932, 'learning_rate': 0.0002180327868852459, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3632, 'train_samples_per_second': 44.518, 'train_steps_per_second': 5.57, 'train_loss': 1.5757709971645422, 'epoch': 0.31}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_61/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)]]
proposed candidate before processing: tensor([2.1076e-01, 4.7154e-02, 1.6629e-17, 5.2313e-18, 1.6457e-01, 2.0102e-01,
        2.0764e-01, 1.6885e-01, 2.9427e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.1248e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.2108), 0, 0, 0, tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), 1, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
iteration:  62
input_X:  [tensor(0.2108), 0, 0, 0, tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), 1, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  104 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 36,773,888 || all params: 8,067,035,136 || trainable%: 0.4559
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.2108), 0, 0, 0, tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.2108)
number of datapoints needed (ratio * total):  1053
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1646)
number of datapoints needed (ratio * total):  822
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.2010)
number of datapoints needed (ratio * total):  1005
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2076)
number of datapoints needed (ratio * total):  1038
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1689)
number of datapoints needed (ratio * total):  844
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1053
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 822
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1005
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1038
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 844
})]
length of training data:  4762
training model...
trainable params: 36,773,888 || all params: 8,067,035,136 || trainable%: 0.4559
{'loss': 1.2263, 'grad_norm': 0.6510282754898071, 'learning_rate': 0.0002948805460750853, 'epoch': 0.03}
{'loss': 1.3354, 'grad_norm': 0.4131150245666504, 'learning_rate': 0.00028464163822525593, 'epoch': 0.07}
{'loss': 1.6826, 'grad_norm': 0.7424430847167969, 'learning_rate': 0.00027440273037542657, 'epoch': 0.1}
{'loss': 1.2786, 'grad_norm': 1.1111016273498535, 'learning_rate': 0.00026416382252559726, 'epoch': 0.13}
{'loss': 0.9694, 'grad_norm': 1.137719988822937, 'learning_rate': 0.0002539249146757679, 'epoch': 0.17}
{'loss': 1.1194, 'grad_norm': 1.1144118309020996, 'learning_rate': 0.00024368600682593853, 'epoch': 0.2}
{'loss': 1.2521, 'grad_norm': 0.8205046057701111, 'learning_rate': 0.0002334470989761092, 'epoch': 0.23}
{'loss': 1.2566, 'grad_norm': 0.3920825719833374, 'learning_rate': 0.00022320819112627984, 'epoch': 0.27}
{'loss': 1.3285, 'grad_norm': 1.4180659055709839, 'learning_rate': 0.0002129692832764505, 'epoch': 0.3}
{'loss': 1.1237, 'grad_norm': 1.0384060144424438, 'learning_rate': 0.00020273037542662114, 'epoch': 0.34}
{'loss': 0.9921, 'grad_norm': 0.5969772934913635, 'learning_rate': 0.0001924914675767918, 'epoch': 0.37}
{'loss': 1.042, 'grad_norm': 0.4835946559906006, 'learning_rate': 0.00018225255972696247, 'epoch': 0.4}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3499, 'train_samples_per_second': 47.454, 'train_steps_per_second': 5.939, 'train_loss': 1.212864323863833, 'epoch': 0.43}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_62/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1245, 0.1181, 0.0000, 0.0551, 0.2284, 0.1326, 0.1978, 0.1436, 0.0291,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8074, 0.1000])
proposed candidate after normalizing: [tensor(0.1245), tensor(0.1181), 0, tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), 1, 1, 1, 1, 1, 1, 103, 0.10000000149011612]
iteration:  63
input_X:  [tensor(0.1245), tensor(0.1181), 0, tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), 1, 1, 1, 1, 1, 1, 103, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  103 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,066,624 || all params: 8,037,327,872 || trainable%: 0.0879
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1245), tensor(0.1181), 0, tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1245)
number of datapoints needed (ratio * total):  622
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1181)
number of datapoints needed (ratio * total):  590
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0551)
number of datapoints needed (ratio * total):  275
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.2284)
number of datapoints needed (ratio * total):  1142
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1326)
number of datapoints needed (ratio * total):  662
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1978)
number of datapoints needed (ratio * total):  988
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1436)
number of datapoints needed (ratio * total):  717
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 622
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 590
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 275
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1142
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 662
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 988
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 717
})]
length of training data:  4996
training model...
trainable params: 7,066,624 || all params: 8,037,327,872 || trainable%: 0.0879
{'loss': 2.1925, 'grad_norm': 2.624330759048462, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.3418, 'grad_norm': 0.8091567754745483, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.6001, 'grad_norm': 0.3720100224018097, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3948, 'grad_norm': 0.8601948618888855, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3329, 'grad_norm': 0.8156272172927856, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.3216, 'grad_norm': 0.6412981748580933, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.3115, 'grad_norm': 0.5406975746154785, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.1354, 'grad_norm': 0.7773907780647278, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1647, 'train_samples_per_second': 49.878, 'train_steps_per_second': 6.24, 'train_loss': 1.463211144814944, 'epoch': 0.29}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_63/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)]]
proposed candidate before processing: tensor([6.1019e-02, 1.8919e-01, 2.2000e-17, 7.9143e-18, 1.5870e-01, 2.4429e-01,
        2.2819e-01, 1.1861e-01, 5.6566e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 9.6183e-01, 1.7963e-02])
proposed candidate after normalizing: [tensor(0.0610), tensor(0.1892), 0, 0, tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), 2, 1, 1, 1, 1, 1, 123, 0.01796255074441433]
iteration:  64
input_X:  [tensor(0.0610), tensor(0.1892), 0, 0, tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), 2, 1, 1, 1, 1, 1, 123, 0.01796255074441433]
mixing data with method:  random
arranging lora config with parameters:  123 0.01796255074441433 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 16,877,568 || all params: 8,047,138,816 || trainable%: 0.2097
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0610), tensor(0.1892), 0, 0, tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0610)
number of datapoints needed (ratio * total):  305
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1892)
number of datapoints needed (ratio * total):  945
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1587)
number of datapoints needed (ratio * total):  793
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.2443)
number of datapoints needed (ratio * total):  1221
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2282)
number of datapoints needed (ratio * total):  1140
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1186)
number of datapoints needed (ratio * total):  593
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 305
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 945
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 793
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1221
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1140
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 593
})]
length of training data:  4997
training model...
trainable params: 16,877,568 || all params: 8,047,138,816 || trainable%: 0.2097
{'loss': 1.9813, 'grad_norm': 1.1722800731658936, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.3258, 'grad_norm': 1.1580092906951904, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.1828, 'grad_norm': 0.4670993983745575, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 0.8946, 'grad_norm': 0.8072483539581299, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 0.9719, 'grad_norm': 0.475322425365448, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1016, 'grad_norm': 0.49470409750938416, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 0.9278, 'grad_norm': 0.5772712826728821, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 0.896, 'grad_norm': 0.9624085426330566, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 0.886, 'grad_norm': 0.6298243999481201, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2709, 'train_samples_per_second': 49.835, 'train_steps_per_second': 6.233, 'train_loss': 1.1143966416517894, 'epoch': 0.31}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_64/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.55
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)]]
proposed candidate before processing: tensor([8.8044e-02, 2.5433e-01, 4.2962e-18, 1.2875e-17, 1.2940e-01, 1.4226e-01,
        2.5107e-01, 1.3489e-01, 7.0933e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.0705e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.0880), tensor(0.2543), 0, 0, tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), 2, 1, 1, 1, 1, 1, 103, 0.10000000149011612]
iteration:  65
input_X:  [tensor(0.0880), tensor(0.2543), 0, 0, tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), 2, 1, 1, 1, 1, 1, 103, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  103 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 14,133,248 || all params: 8,044,394,496 || trainable%: 0.1757
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0880), tensor(0.2543), 0, 0, tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0880)
number of datapoints needed (ratio * total):  440
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2543)
number of datapoints needed (ratio * total):  1271
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1294)
number of datapoints needed (ratio * total):  647
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1423)
number of datapoints needed (ratio * total):  711
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2511)
number of datapoints needed (ratio * total):  1255
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1349)
number of datapoints needed (ratio * total):  674
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 440
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1271
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 647
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 711
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1255
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 674
})]
length of training data:  4998
training model...
trainable params: 14,133,248 || all params: 8,044,394,496 || trainable%: 0.1757
{'loss': 1.9451, 'grad_norm': 1.3516340255737305, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.537, 'grad_norm': 0.6560642123222351, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.1006, 'grad_norm': 0.7197337746620178, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 0.9995, 'grad_norm': 0.5800336599349976, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.2353, 'grad_norm': 1.9649275541305542, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.0851, 'grad_norm': 0.5551412105560303, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 0.9744, 'grad_norm': 0.6911002397537231, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.2132, 'grad_norm': 0.9797347784042358, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.334, 'grad_norm': 1.1975687742233276, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.307, 'train_samples_per_second': 49.827, 'train_steps_per_second': 6.231, 'train_loss': 1.273865107014693, 'epoch': 0.29}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_65/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.64
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)]]
proposed candidate before processing: tensor([1.5201e-01, 1.8639e-01, 1.0357e-16, 2.3690e-17, 1.4929e-01, 1.4806e-01,
        1.8689e-01, 1.7738e-01, 2.9636e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.9063e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1520), tensor(0.1864), 0, 0, tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), 1, 1, 1, 1, 1, 1, 101, 0.10000000149011612]
iteration:  66
input_X:  [tensor(0.1520), tensor(0.1864), 0, 0, tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), 1, 1, 1, 1, 1, 1, 101, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  101 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 13,996,032 || all params: 8,044,257,280 || trainable%: 0.1740
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1520), tensor(0.1864), 0, 0, tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1520)
number of datapoints needed (ratio * total):  760
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1864)
number of datapoints needed (ratio * total):  931
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1493)
number of datapoints needed (ratio * total):  746
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1481)
number of datapoints needed (ratio * total):  740
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1869)
number of datapoints needed (ratio * total):  934
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1774)
number of datapoints needed (ratio * total):  886
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 760
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 931
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 746
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 740
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 934
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 886
})]
length of training data:  4997
training model...
trainable params: 13,996,032 || all params: 8,044,257,280 || trainable%: 0.1740
{'loss': 1.3966, 'grad_norm': 0.6882097125053406, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.2078, 'grad_norm': 0.8946863412857056, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3346, 'grad_norm': 1.013290524482727, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.2908, 'grad_norm': 0.5600941181182861, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 0.9829, 'grad_norm': 0.6420114040374756, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.251, 'grad_norm': 0.5148501992225647, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.0827, 'grad_norm': 0.3131517767906189, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.0373, 'grad_norm': 0.7182680368423462, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.1421, 'grad_norm': 0.7789371013641357, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4157, 'train_samples_per_second': 49.763, 'train_steps_per_second': 6.224, 'train_loss': 1.187189822639924, 'epoch': 0.29}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_66/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)]]
proposed candidate before processing: tensor([1.3000e-01, 1.4726e-01, 2.5835e-17, 7.7656e-18, 2.0082e-01, 1.6390e-01,
        1.8625e-01, 1.7177e-01, 2.8858e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.2492e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1300), tensor(0.1473), 0, 0, tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), 1, 1, 1, 1, 1, 1, 106, 0.10000000149011612]
iteration:  67
input_X:  [tensor(0.1300), tensor(0.1473), 0, 0, tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), 1, 1, 1, 1, 1, 1, 106, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  106 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,272,448 || all params: 8,037,533,696 || trainable%: 0.0905
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1300), tensor(0.1473), 0, 0, tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1300)
number of datapoints needed (ratio * total):  649
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1473)
number of datapoints needed (ratio * total):  736
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.2008)
number of datapoints needed (ratio * total):  1004
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1639)
number of datapoints needed (ratio * total):  819
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1863)
number of datapoints needed (ratio * total):  931
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1718)
number of datapoints needed (ratio * total):  858
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 649
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 736
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1004
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 819
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 931
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 858
})]
length of training data:  4997
training model...
trainable params: 7,272,448 || all params: 8,037,533,696 || trainable%: 0.0905
{'loss': 2.3697, 'grad_norm': 1.4283310174942017, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.3388, 'grad_norm': 0.5957761406898499, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3509, 'grad_norm': 0.9658645391464233, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3641, 'grad_norm': 0.4464873671531677, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3643, 'grad_norm': 0.3163491487503052, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.5032, 'grad_norm': 0.6369588375091553, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.3753, 'grad_norm': 0.602344274520874, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.3241, 'grad_norm': 0.8175153732299805, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.151, 'grad_norm': 0.897467315196991, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0995, 'train_samples_per_second': 49.92, 'train_steps_per_second': 6.244, 'train_loss': 1.4461336424856475, 'epoch': 0.32}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_67/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)]]
proposed candidate before processing: tensor([2.2503e-01, 1.1424e-01, 0.0000e+00, 9.9984e-18, 1.8914e-01, 1.2017e-01,
        2.1736e-01, 1.3405e-01, 3.0115e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.7095e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.2250), tensor(0.1142), 0, 0, tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), 1, 1, 1, 1, 1, 1, 99, 0.10000000149011612]
iteration:  68
input_X:  [tensor(0.2250), tensor(0.1142), 0, 0, tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), 1, 1, 1, 1, 1, 1, 99, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  99 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 6,792,192 || all params: 8,037,053,440 || trainable%: 0.0845
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.2250), tensor(0.1142), 0, 0, tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.2250)
number of datapoints needed (ratio * total):  1125
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1142)
number of datapoints needed (ratio * total):  571
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1891)
number of datapoints needed (ratio * total):  945
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1202)
number of datapoints needed (ratio * total):  600
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2174)
number of datapoints needed (ratio * total):  1086
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1340)
number of datapoints needed (ratio * total):  670
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1125
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 571
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 945
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 600
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1086
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 670
})]
length of training data:  4997
training model...
trainable params: 6,792,192 || all params: 8,037,053,440 || trainable%: 0.0845
{'loss': 2.4697, 'grad_norm': 0.8018965125083923, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.3173, 'grad_norm': 0.5846192240715027, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3833, 'grad_norm': 0.8839753270149231, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.0472, 'grad_norm': 0.5388879776000977, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.5432, 'grad_norm': 1.567285180091858, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1403, 'grad_norm': 0.934459388256073, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.244, 'grad_norm': 0.8669016361236572, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.3433, 'grad_norm': 1.1075612306594849, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.1817, 'grad_norm': 1.8874702453613281, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
{'loss': 1.1685, 'grad_norm': 0.6733877062797546, 'learning_rate': 0.00020731707317073167, 'epoch': 0.32}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1523, 'train_samples_per_second': 49.894, 'train_steps_per_second': 6.24, 'train_loss': 1.3774771804444528, 'epoch': 0.33}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_68/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)]]
proposed candidate before processing: tensor([1.4658e-01, 9.9286e-02, 6.1593e-17, 6.7773e-02, 1.5685e-01, 1.7498e-01,
        1.6443e-01, 1.9010e-01, 2.8906e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.1708e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1466), tensor(0.0993), 0, tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), 1, 1, 1, 1, 1, 1, 105, 0.10000000149011612]
iteration:  69
input_X:  [tensor(0.1466), tensor(0.0993), 0, tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), 1, 1, 1, 1, 1, 1, 105, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  105 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,203,840 || all params: 8,037,465,088 || trainable%: 0.0896
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1466), tensor(0.0993), 0, tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1466)
number of datapoints needed (ratio * total):  732
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0993)
number of datapoints needed (ratio * total):  496
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0678)
number of datapoints needed (ratio * total):  338
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1569)
number of datapoints needed (ratio * total):  784
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1750)
number of datapoints needed (ratio * total):  874
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1644)
number of datapoints needed (ratio * total):  822
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1901)
number of datapoints needed (ratio * total):  950
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 732
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 496
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 338
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 784
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 874
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 822
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 950
})]
length of training data:  4996
training model...
trainable params: 7,203,840 || all params: 8,037,465,088 || trainable%: 0.0896
{'loss': 2.1945, 'grad_norm': 1.9775158166885376, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.7641, 'grad_norm': 0.8433579206466675, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.6985, 'grad_norm': 2.7995007038116455, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.4434, 'grad_norm': 0.6685736179351807, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.2295, 'grad_norm': 0.515949547290802, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.5957, 'grad_norm': 0.6120445728302002, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.7525, 'grad_norm': 0.41229185461997986, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.4632, 'grad_norm': 0.3811405301094055, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4629, 'train_samples_per_second': 49.73, 'train_steps_per_second': 6.221, 'train_loss': 1.6353457515889949, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_69/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)]]
proposed candidate before processing: tensor([1.7959e-01, 1.3643e-01, 0.0000e+00, 5.2897e-18, 1.8319e-01, 1.7375e-01,
        1.4644e-01, 1.8061e-01, 2.7921e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.2836e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1796), tensor(0.1364), 0, 0, tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), 1, 1, 1, 1, 1, 1, 106, 0.10000000149011612]
iteration:  70
input_X:  [tensor(0.1796), tensor(0.1364), 0, 0, tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), 1, 1, 1, 1, 1, 1, 106, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  106 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,272,448 || all params: 8,037,533,696 || trainable%: 0.0905
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1796), tensor(0.1364), 0, 0, tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1796)
number of datapoints needed (ratio * total):  897
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1364)
number of datapoints needed (ratio * total):  682
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1832)
number of datapoints needed (ratio * total):  915
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1737)
number of datapoints needed (ratio * total):  868
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1464)
number of datapoints needed (ratio * total):  732
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1806)
number of datapoints needed (ratio * total):  903
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 897
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 682
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 915
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 868
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 732
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 903
})]
length of training data:  4997
training model...
trainable params: 7,272,448 || all params: 8,037,533,696 || trainable%: 0.0905
{'loss': 2.3313, 'grad_norm': 2.0124964714050293, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.2165, 'grad_norm': 1.3199479579925537, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.8543, 'grad_norm': 0.7249260544776917, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.269, 'grad_norm': 0.8350036144256592, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.615, 'grad_norm': 0.5563883781433105, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.3617, 'grad_norm': 0.48120319843292236, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.3076, 'grad_norm': 0.8437857031822205, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.3769, 'grad_norm': 0.6675361394882202, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.4859, 'grad_norm': 3.0126209259033203, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0147, 'train_samples_per_second': 49.963, 'train_steps_per_second': 6.249, 'train_loss': 1.528389988523541, 'epoch': 0.32}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_70/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.64
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)]]
proposed candidate before processing: tensor([7.2485e-02, 1.0208e-01, 6.0417e-17, 2.4517e-02, 1.7068e-01, 1.7760e-01,
        2.7183e-01, 1.8080e-01, 3.0000e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.0949e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.0725), tensor(0.1021), 0, 0, tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), 1, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
iteration:  71
input_X:  [tensor(0.0725), tensor(0.1021), 0, 0, tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), 1, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  104 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,135,232 || all params: 8,037,396,480 || trainable%: 0.0888
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0725), tensor(0.1021), 0, 0, tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0725)
number of datapoints needed (ratio * total):  362
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1021)
number of datapoints needed (ratio * total):  510
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1707)
number of datapoints needed (ratio * total):  853
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1776)
number of datapoints needed (ratio * total):  888
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2718)
number of datapoints needed (ratio * total):  1359
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1808)
number of datapoints needed (ratio * total):  904
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 362
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 510
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 853
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 888
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1359
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 904
})]
length of training data:  4876
training model...
trainable params: 7,135,232 || all params: 8,037,396,480 || trainable%: 0.0888
{'loss': 2.6067, 'grad_norm': 5.1213459968566895, 'learning_rate': 0.00029499999999999996, 'epoch': 0.03}
{'loss': 1.504, 'grad_norm': 1.8442132472991943, 'learning_rate': 0.000285, 'epoch': 0.07}
{'loss': 1.6039, 'grad_norm': 0.3590572476387024, 'learning_rate': 0.00027499999999999996, 'epoch': 0.1}
{'loss': 1.718, 'grad_norm': 0.3859763741493225, 'learning_rate': 0.000265, 'epoch': 0.13}
{'loss': 1.4473, 'grad_norm': 0.35173550248146057, 'learning_rate': 0.00025499999999999996, 'epoch': 0.16}
{'loss': 1.3573, 'grad_norm': 0.595922589302063, 'learning_rate': 0.000245, 'epoch': 0.2}
{'loss': 1.462, 'grad_norm': 0.4394579529762268, 'learning_rate': 0.00023499999999999997, 'epoch': 0.23}
{'loss': 1.2406, 'grad_norm': 0.8211057782173157, 'learning_rate': 0.000225, 'epoch': 0.26}
{'loss': 1.6535, 'grad_norm': 0.8561753034591675, 'learning_rate': 0.000215, 'epoch': 0.3}
{'loss': 1.3517, 'grad_norm': 0.8393810987472534, 'learning_rate': 0.000205, 'epoch': 0.33}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4732, 'train_samples_per_second': 48.53, 'train_steps_per_second': 6.071, 'train_loss': 1.5683681840580221, 'epoch': 0.35}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_71/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1625, 0.1182, 0.0000, 0.0736, 0.1525, 0.1446, 0.2106, 0.1380, 0.0292,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8236, 0.1000])
proposed candidate after normalizing: [tensor(0.1625), tensor(0.1182), 0, tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), 1, 1, 1, 1, 1, 1, 105, 0.10000000149011612]
iteration:  72
input_X:  [tensor(0.1625), tensor(0.1182), 0, tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), 1, 1, 1, 1, 1, 1, 105, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  105 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,203,840 || all params: 8,037,465,088 || trainable%: 0.0896
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1625), tensor(0.1182), 0, tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1625)
number of datapoints needed (ratio * total):  812
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1182)
number of datapoints needed (ratio * total):  591
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0736)
number of datapoints needed (ratio * total):  368
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1525)
number of datapoints needed (ratio * total):  762
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1446)
number of datapoints needed (ratio * total):  722
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2106)
number of datapoints needed (ratio * total):  1052
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1380)
number of datapoints needed (ratio * total):  689
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 812
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 591
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 368
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 762
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 722
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1052
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 689
})]
length of training data:  4996
training model...
trainable params: 7,203,840 || all params: 8,037,465,088 || trainable%: 0.0896
{'loss': 1.9903, 'grad_norm': 3.65724778175354, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.4742, 'grad_norm': 1.955047607421875, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.2336, 'grad_norm': 1.0480520725250244, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3083, 'grad_norm': 0.5273043513298035, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.1362, 'grad_norm': 0.4019591212272644, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.417, 'grad_norm': 2.6862215995788574, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.4641, 'grad_norm': 0.2945871353149414, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.4193, 'grad_norm': 0.8393158912658691, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1962, 'train_samples_per_second': 49.862, 'train_steps_per_second': 6.238, 'train_loss': 1.4397344646339645, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_72/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)]]
proposed candidate before processing: tensor([8.1975e-02, 1.5526e-01, 0.0000e+00, 1.9536e-17, 2.0546e-01, 1.5211e-01,
        2.1038e-01, 1.9481e-01, 2.8930e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.9808e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.0820), tensor(0.1553), 0, 0, tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), 1, 1, 1, 1, 1, 1, 102, 0.10000000149011612]
iteration:  73
input_X:  [tensor(0.0820), tensor(0.1553), 0, 0, tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), 1, 1, 1, 1, 1, 1, 102, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  102 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 6,998,016 || all params: 8,037,259,264 || trainable%: 0.0871
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0820), tensor(0.1553), 0, 0, tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0820)
number of datapoints needed (ratio * total):  409
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1553)
number of datapoints needed (ratio * total):  776
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.2055)
number of datapoints needed (ratio * total):  1027
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1521)
number of datapoints needed (ratio * total):  760
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2104)
number of datapoints needed (ratio * total):  1051
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1948)
number of datapoints needed (ratio * total):  974
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 409
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 776
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1027
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 760
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1051
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 974
})]
length of training data:  4997
training model...
trainable params: 6,998,016 || all params: 8,037,259,264 || trainable%: 0.0871
{'loss': 2.4055, 'grad_norm': 2.4664740562438965, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.6882, 'grad_norm': 0.617448091506958, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.8867, 'grad_norm': 0.612208366394043, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.6806, 'grad_norm': 1.0793648958206177, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.4362, 'grad_norm': 0.5632242560386658, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.4123, 'grad_norm': 0.3728485107421875, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.3577, 'grad_norm': 1.9588934183120728, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.3585, 'grad_norm': 0.7897284030914307, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.3804, 'grad_norm': 0.8873730897903442, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0517, 'train_samples_per_second': 49.944, 'train_steps_per_second': 6.247, 'train_loss': 1.596377010146777, 'epoch': 0.31}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_73/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1385, 0.0754, 0.0000, 0.0670, 0.1465, 0.1846, 0.2236, 0.1644, 0.0725,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8564, 0.1000])
proposed candidate after normalizing: [tensor(0.1385), tensor(0.0754), 0, tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), 2, 1, 1, 1, 1, 1, 110, 0.10000000149011612]
iteration:  74
input_X:  [tensor(0.1385), tensor(0.0754), 0, tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), 2, 1, 1, 1, 1, 1, 110, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  110 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 15,093,760 || all params: 8,045,355,008 || trainable%: 0.1876
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1385), tensor(0.0754), 0, tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1385)
number of datapoints needed (ratio * total):  692
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0754)
number of datapoints needed (ratio * total):  377
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0670)
number of datapoints needed (ratio * total):  335
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1465)
number of datapoints needed (ratio * total):  732
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1846)
number of datapoints needed (ratio * total):  923
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2236)
number of datapoints needed (ratio * total):  1117
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1644)
number of datapoints needed (ratio * total):  821
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 692
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 377
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 335
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 732
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 923
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1117
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 821
})]
length of training data:  4997
training model...
trainable params: 15,093,760 || all params: 8,045,355,008 || trainable%: 0.1876
{'loss': 2.3392, 'grad_norm': 1.2158721685409546, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5228, 'grad_norm': 0.7390814423561096, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.6234, 'grad_norm': 0.8454488515853882, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.2405, 'grad_norm': 1.136673092842102, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.2224, 'grad_norm': 0.5856835842132568, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1781, 'grad_norm': 0.4959763288497925, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.2646, 'grad_norm': 1.5690034627914429, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.3467, 'grad_norm': 0.892139196395874, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1104, 'train_samples_per_second': 49.915, 'train_steps_per_second': 6.243, 'train_loss': 1.4432416658722953, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_74/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1194, 0.1194, 0.0000, 0.0445, 0.1493, 0.1831, 0.2213, 0.1632, 0.0740,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8581, 0.0654])
proposed candidate after normalizing: [tensor(0.1194), tensor(0.1194), 0, 0, tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), 2, 1, 1, 1, 1, 1, 110, 0.06541059166193008]
iteration:  75
input_X:  [tensor(0.1194), tensor(0.1194), 0, 0, tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), 2, 1, 1, 1, 1, 1, 110, 0.06541059166193008]
mixing data with method:  random
arranging lora config with parameters:  110 0.06541059166193008 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 15,093,760 || all params: 8,045,355,008 || trainable%: 0.1876
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1194), tensor(0.1194), 0, 0, tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1194)
number of datapoints needed (ratio * total):  596
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1194)
number of datapoints needed (ratio * total):  596
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1493)
number of datapoints needed (ratio * total):  746
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1831)
number of datapoints needed (ratio * total):  915
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2213)
number of datapoints needed (ratio * total):  1106
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1632)
number of datapoints needed (ratio * total):  815
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 596
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 596
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 746
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 915
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1106
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 815
})]
length of training data:  4774
training model...
trainable params: 15,093,760 || all params: 8,045,355,008 || trainable%: 0.1876
{'loss': 2.1529, 'grad_norm': 1.0704538822174072, 'learning_rate': 0.00029488926746166946, 'epoch': 0.03}
{'loss': 1.6559, 'grad_norm': 0.7548031210899353, 'learning_rate': 0.0002846678023850085, 'epoch': 0.07}
{'loss': 1.4441, 'grad_norm': 0.5992575287818909, 'learning_rate': 0.0002744463373083475, 'epoch': 0.1}
{'loss': 1.2854, 'grad_norm': 0.8915033340454102, 'learning_rate': 0.00026422487223168653, 'epoch': 0.13}
{'loss': 1.6129, 'grad_norm': 0.9544844627380371, 'learning_rate': 0.0002540034071550255, 'epoch': 0.17}
{'loss': 1.2558, 'grad_norm': 1.079672932624817, 'learning_rate': 0.00024378194207836453, 'epoch': 0.2}
{'loss': 1.2421, 'grad_norm': 1.6351981163024902, 'learning_rate': 0.00023356047700170358, 'epoch': 0.23}
{'loss': 1.1998, 'grad_norm': 0.62631756067276, 'learning_rate': 0.00022333901192504258, 'epoch': 0.27}
{'loss': 1.3988, 'grad_norm': 0.5614868402481079, 'learning_rate': 0.00021311754684838157, 'epoch': 0.3}
{'loss': 1.3005, 'grad_norm': 0.6062290668487549, 'learning_rate': 0.0002028960817717206, 'epoch': 0.34}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0123, 'train_samples_per_second': 47.734, 'train_steps_per_second': 5.969, 'train_loss': 1.4498348188872385, 'epoch': 0.34}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_75/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)]]
proposed candidate before processing: tensor([0.1627, 0.0690, 0.0000, 0.0000, 0.2184, 0.1703, 0.2247, 0.1549, 0.0283,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8239, 0.1000])
proposed candidate after normalizing: [tensor(0.1627), tensor(0.0690), 0, 0, tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), 1, 1, 1, 1, 1, 1, 105, 0.10000000149011612]
iteration:  76
input_X:  [tensor(0.1627), tensor(0.0690), 0, 0, tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), 1, 1, 1, 1, 1, 1, 105, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  105 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 14,750,720 || all params: 8,045,011,968 || trainable%: 0.1834
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1627), tensor(0.0690), 0, 0, tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1627)
number of datapoints needed (ratio * total):  813
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0690)
number of datapoints needed (ratio * total):  344
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.2184)
number of datapoints needed (ratio * total):  1092
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1703)
number of datapoints needed (ratio * total):  851
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2247)
number of datapoints needed (ratio * total):  1123
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1549)
number of datapoints needed (ratio * total):  774
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 813
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 344
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1092
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 851
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1123
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 774
})]
length of training data:  4997
training model...
trainable params: 14,750,720 || all params: 8,045,011,968 || trainable%: 0.1834
{'loss': 1.1605, 'grad_norm': 0.5246034264564514, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.1822, 'grad_norm': 1.380948781967163, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3265, 'grad_norm': 1.2423419952392578, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.1041, 'grad_norm': 0.7300271987915039, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.2941, 'grad_norm': 0.4703705310821533, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1975, 'grad_norm': 0.9978631138801575, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.0279, 'grad_norm': 0.6052769422531128, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.1047, 'grad_norm': 1.0019809007644653, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.0829, 'grad_norm': 0.5948785543441772, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
{'loss': 1.1998, 'grad_norm': 0.7304390072822571, 'learning_rate': 0.00020731707317073167, 'epoch': 0.32}
{'loss': 1.1901, 'grad_norm': 0.513761043548584, 'learning_rate': 0.0001975609756097561, 'epoch': 0.35}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0189, 'train_samples_per_second': 49.961, 'train_steps_per_second': 6.249, 'train_loss': 1.1789828067355685, 'epoch': 0.36}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_76/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1669, 0.1550, 0.0000, 0.0000, 0.1324, 0.1668, 0.1963, 0.1825, 0.0286,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7913, 0.1000])
proposed candidate after normalizing: [tensor(0.1669), tensor(0.1550), 0, 0, tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), 1, 1, 1, 1, 1, 1, 101, 0.10000000149011612]
iteration:  77
input_X:  [tensor(0.1669), tensor(0.1550), 0, 0, tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), 1, 1, 1, 1, 1, 1, 101, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  101 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 6,929,408 || all params: 8,037,190,656 || trainable%: 0.0862
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1669), tensor(0.1550), 0, 0, tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1669)
number of datapoints needed (ratio * total):  834
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1550)
number of datapoints needed (ratio * total):  775
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1324)
number of datapoints needed (ratio * total):  661
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1668)
number of datapoints needed (ratio * total):  834
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1963)
number of datapoints needed (ratio * total):  981
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1825)
number of datapoints needed (ratio * total):  912
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 834
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 775
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 661
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 834
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 981
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 912
})]
length of training data:  4997
training model...
trainable params: 6,929,408 || all params: 8,037,190,656 || trainable%: 0.0862
{'loss': 2.2686, 'grad_norm': 1.4883902072906494, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5258, 'grad_norm': 0.42265185713768005, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.6625, 'grad_norm': 0.38360536098480225, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.7648, 'grad_norm': 0.3362232446670532, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.467, 'grad_norm': 0.6562390327453613, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.5103, 'grad_norm': 0.9790031313896179, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.6802, 'grad_norm': 0.40285974740982056, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.1688, 'grad_norm': 1.303523063659668, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.3622, 'grad_norm': 0.3404785394668579, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1395, 'train_samples_per_second': 49.9, 'train_steps_per_second': 6.241, 'train_loss': 1.5703671773274739, 'epoch': 0.3}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_77/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)]]
proposed candidate before processing: tensor([0.0959, 0.0814, 0.0776, 0.0850, 0.1778, 0.0544, 0.1817, 0.2462, 0.1490,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8565, 0.0386])
proposed candidate after normalizing: [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), 5, 1, 1, 1, 1, 1, 110, 0.0385541208088398]
iteration:  78
input_X:  [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), 5, 1, 1, 1, 1, 1, 110, 0.0385541208088398]
mixing data with method:  random
arranging lora config with parameters:  110 0.0385541208088398 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 37,734,400 || all params: 8,067,995,648 || trainable%: 0.4677
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0959)
number of datapoints needed (ratio * total):  479
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0814)
number of datapoints needed (ratio * total):  407
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0776)
number of datapoints needed (ratio * total):  387
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0850)
number of datapoints needed (ratio * total):  425
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1778)
number of datapoints needed (ratio * total):  888
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0544)
number of datapoints needed (ratio * total):  272
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1817)
number of datapoints needed (ratio * total):  908
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2462)
number of datapoints needed (ratio * total):  1231
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 479
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 407
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 387
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 425
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 888
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 272
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 908
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1231
})]
length of training data:  4997
training model...
trainable params: 37,734,400 || all params: 8,067,995,648 || trainable%: 0.4677
{'loss': 2.2375, 'grad_norm': 1.8226643800735474, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5632, 'grad_norm': 0.5273023843765259, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.7291, 'grad_norm': 1.3979506492614746, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.6894, 'grad_norm': 0.929706871509552, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3468, 'grad_norm': 0.6021082997322083, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.6357, 'grad_norm': 0.43194133043289185, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.4332, 'grad_norm': 0.6478168368339539, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2165, 'train_samples_per_second': 49.862, 'train_steps_per_second': 6.236, 'train_loss': 1.6376066519544015, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_78/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.51
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)]]
proposed candidate before processing: tensor([0.1974, 0.1742, 0.0401, 0.0278, 0.0600, 0.1249, 0.1475, 0.2281, 0.1374,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8603, 0.0959])
proposed candidate after normalizing: [tensor(0.1974), tensor(0.1742), 0, 0, tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), 4, 1, 1, 1, 1, 1, 110, 0.09590566158294678]
iteration:  79
input_X:  [tensor(0.1974), tensor(0.1742), 0, 0, tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), 4, 1, 1, 1, 1, 1, 110, 0.09590566158294678]
mixing data with method:  random
arranging lora config with parameters:  110 0.09590566158294678 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 37,734,400 || all params: 8,067,995,648 || trainable%: 0.4677
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1974), tensor(0.1742), 0, 0, tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1974)
number of datapoints needed (ratio * total):  986
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1742)
number of datapoints needed (ratio * total):  870
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0600)
number of datapoints needed (ratio * total):  299
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1249)
number of datapoints needed (ratio * total):  624
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1475)
number of datapoints needed (ratio * total):  737
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2281)
number of datapoints needed (ratio * total):  1140
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 986
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 870
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 299
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 624
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 737
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1140
})]
length of training data:  4656
training model...
trainable params: 37,734,400 || all params: 8,067,995,648 || trainable%: 0.4677
{'loss': 1.5116, 'grad_norm': 0.5952611565589905, 'learning_rate': 0.0002947552447552447, 'epoch': 0.03}
{'loss': 1.693, 'grad_norm': 0.832825779914856, 'learning_rate': 0.0002842657342657343, 'epoch': 0.07}
{'loss': 1.4116, 'grad_norm': 1.0957443714141846, 'learning_rate': 0.0002737762237762238, 'epoch': 0.1}
{'loss': 1.2777, 'grad_norm': 1.0465527772903442, 'learning_rate': 0.0002632867132867133, 'epoch': 0.14}
{'loss': 1.0446, 'grad_norm': 0.8063099384307861, 'learning_rate': 0.0002527972027972028, 'epoch': 0.17}
{'loss': 1.0261, 'grad_norm': 0.8559229373931885, 'learning_rate': 0.0002423076923076923, 'epoch': 0.21}
{'loss': 1.3179, 'grad_norm': 1.6350412368774414, 'learning_rate': 0.0002318181818181818, 'epoch': 0.24}
{'loss': 1.0326, 'grad_norm': 0.45385968685150146, 'learning_rate': 0.00022132867132867133, 'epoch': 0.27}
⏰ Max training time of 100 seconds reached. Stopping.
{'loss': 1.5874, 'grad_norm': 0.5321873426437378, 'learning_rate': 0.00021083916083916083, 'epoch': 0.31}
{'train_runtime': 100.2549, 'train_samples_per_second': 46.442, 'train_steps_per_second': 5.805, 'train_loss': 1.322498713599311, 'epoch': 0.31}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_79/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)]]
proposed candidate before processing: tensor([2.6043e-01, 2.2932e-01, 9.3919e-18, 0.0000e+00, 9.5301e-02, 9.9402e-02,
        1.2738e-01, 1.8816e-01, 1.0301e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.4109e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.2604), tensor(0.2293), 0, 0, tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), 3, 1, 1, 1, 1, 1, 108, 0.10000000149011612]
iteration:  80
input_X:  [tensor(0.2604), tensor(0.2293), 0, 0, tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), 3, 1, 1, 1, 1, 1, 108, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  108 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 29,775,872 || all params: 8,060,037,120 || trainable%: 0.3694
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.2604), tensor(0.2293), 0, 0, tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.2604)
number of datapoints needed (ratio * total):  1302
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2293)
number of datapoints needed (ratio * total):  1146
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0953)
number of datapoints needed (ratio * total):  476
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0994)
number of datapoints needed (ratio * total):  497
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1274)
number of datapoints needed (ratio * total):  636
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1882)
number of datapoints needed (ratio * total):  940
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1302
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1146
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 476
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 497
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 636
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 940
})]
length of training data:  4997
training model...
trainable params: 29,775,872 || all params: 8,060,037,120 || trainable%: 0.3694
{'loss': 1.92, 'grad_norm': 0.7639895677566528, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.0675, 'grad_norm': 0.9697594046592712, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.4086, 'grad_norm': 0.6837319731712341, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.1711, 'grad_norm': 0.7743757963180542, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.0831, 'grad_norm': 0.6009290218353271, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.169, 'grad_norm': 0.5025752782821655, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.1819, 'grad_norm': 0.45366084575653076, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.162, 'grad_norm': 0.5745194554328918, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2659, 'train_samples_per_second': 49.837, 'train_steps_per_second': 6.233, 'train_loss': 1.2604313872077249, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_80/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)]]
proposed candidate before processing: tensor([1.5830e-01, 1.6098e-01, 0.0000e+00, 4.0658e-17, 2.1194e-01, 1.2751e-01,
        2.0914e-01, 1.3213e-01, 2.8802e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.2579e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1583), tensor(0.1610), 0, 0, tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), 1, 1, 1, 1, 1, 1, 106, 0.10000000149011612]
iteration:  81
input_X:  [tensor(0.1583), tensor(0.1610), 0, 0, tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), 1, 1, 1, 1, 1, 1, 106, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  106 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 22,091,776 || all params: 8,052,353,024 || trainable%: 0.2744
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1583), tensor(0.1610), 0, 0, tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1583)
number of datapoints needed (ratio * total):  791
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1610)
number of datapoints needed (ratio * total):  804
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.2119)
number of datapoints needed (ratio * total):  1059
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1275)
number of datapoints needed (ratio * total):  637
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2091)
number of datapoints needed (ratio * total):  1045
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1321)
number of datapoints needed (ratio * total):  660
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 791
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 804
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1059
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 637
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1045
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 660
})]
length of training data:  4996
training model...
trainable params: 22,091,776 || all params: 8,052,353,024 || trainable%: 0.2744
{'loss': 1.0834, 'grad_norm': 0.7911878824234009, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.263, 'grad_norm': 0.6470532417297363, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.0031, 'grad_norm': 0.5991451740264893, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.0055, 'grad_norm': 0.5955168604850769, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.0088, 'grad_norm': 0.8570336103439331, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.0716, 'grad_norm': 0.7461422681808472, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.1678, 'grad_norm': 0.5048165321350098, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.2002, 'grad_norm': 0.35478606820106506, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.0835, 'grad_norm': 0.42615485191345215, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6467, 'train_samples_per_second': 49.639, 'train_steps_per_second': 6.21, 'train_loss': 1.0992002585499556, 'epoch': 0.31}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_81/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)]]
proposed candidate before processing: tensor([1.4553e-01, 4.6631e-02, 6.1257e-18, 4.8670e-02, 2.0418e-01, 1.6818e-01,
        1.8984e-01, 1.9696e-01, 2.9228e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.8202e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1455), 0, 0, 0, tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), 1, 1, 1, 1, 1, 1, 100, 0.10000000149011612]
iteration:  82
input_X:  [tensor(0.1455), 0, 0, 0, tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), 1, 1, 1, 1, 1, 1, 100, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  100 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 6,860,800 || all params: 8,037,122,048 || trainable%: 0.0854
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1455), 0, 0, 0, tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1455)
number of datapoints needed (ratio * total):  727
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.2042)
number of datapoints needed (ratio * total):  1020
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1682)
number of datapoints needed (ratio * total):  840
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1898)
number of datapoints needed (ratio * total):  949
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1970)
number of datapoints needed (ratio * total):  984
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 727
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1020
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 840
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 949
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 984
})]
length of training data:  4520
training model...
trainable params: 6,860,800 || all params: 8,037,122,048 || trainable%: 0.0854
{'loss': 2.5172, 'grad_norm': 2.2823190689086914, 'learning_rate': 0.00029459459459459455, 'epoch': 0.04}
{'loss': 1.6217, 'grad_norm': 0.5262113213539124, 'learning_rate': 0.00028378378378378377, 'epoch': 0.07}
{'loss': 1.6421, 'grad_norm': 0.7305527329444885, 'learning_rate': 0.000272972972972973, 'epoch': 0.11}
{'loss': 1.5316, 'grad_norm': 1.6077786684036255, 'learning_rate': 0.00026216216216216215, 'epoch': 0.14}
{'loss': 1.5619, 'grad_norm': 0.3484228551387787, 'learning_rate': 0.0002513513513513513, 'epoch': 0.18}
{'loss': 1.6797, 'grad_norm': 0.9801660180091858, 'learning_rate': 0.00024054054054054052, 'epoch': 0.21}
{'loss': 1.6598, 'grad_norm': 0.5967627167701721, 'learning_rate': 0.00022972972972972968, 'epoch': 0.25}
{'loss': 1.5365, 'grad_norm': 0.9359356164932251, 'learning_rate': 0.0002189189189189189, 'epoch': 0.28}
{'loss': 1.4809, 'grad_norm': 1.2274726629257202, 'learning_rate': 0.00020810810810810808, 'epoch': 0.32}
{'loss': 1.8819, 'grad_norm': 0.6415224075317383, 'learning_rate': 0.0001972972972972973, 'epoch': 0.35}
{'loss': 1.6658, 'grad_norm': 0.823886513710022, 'learning_rate': 0.00018648648648648646, 'epoch': 0.39}
{'loss': 1.2599, 'grad_norm': 0.4342055022716522, 'learning_rate': 0.00017567567567567568, 'epoch': 0.42}
{'loss': 1.5696, 'grad_norm': 1.2354857921600342, 'learning_rate': 0.00016486486486486486, 'epoch': 0.46}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1124, 'train_samples_per_second': 45.149, 'train_steps_per_second': 5.644, 'train_loss': 1.6621251097127396, 'epoch': 0.46}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_82/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.61
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)]]
proposed candidate before processing: tensor([1.6516e-01, 1.8379e-01, 8.9807e-17, 5.0195e-18, 1.4545e-01, 1.4953e-01,
        2.3559e-01, 1.2049e-01, 2.8908e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.7271e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1652), tensor(0.1838), 0, 0, tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), 1, 1, 1, 1, 1, 1, 112, 0.10000000149011612]
iteration:  83
input_X:  [tensor(0.1652), tensor(0.1838), 0, 0, tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), 1, 1, 1, 1, 1, 1, 112, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  112 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,684,096 || all params: 8,037,945,344 || trainable%: 0.0956
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1652), tensor(0.1838), 0, 0, tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1652)
number of datapoints needed (ratio * total):  825
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1838)
number of datapoints needed (ratio * total):  918
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1454)
number of datapoints needed (ratio * total):  727
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1495)
number of datapoints needed (ratio * total):  747
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2356)
number of datapoints needed (ratio * total):  1177
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1205)
number of datapoints needed (ratio * total):  602
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 825
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 918
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 727
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 747
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1177
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 602
})]
length of training data:  4996
training model...
trainable params: 7,684,096 || all params: 8,037,945,344 || trainable%: 0.0956
{'loss': 2.1328, 'grad_norm': 0.6560613512992859, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5085, 'grad_norm': 1.0095847845077515, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.0197, 'grad_norm': 0.6389626860618591, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.2504, 'grad_norm': 0.577759861946106, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 0.8977, 'grad_norm': 0.7132015228271484, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1096, 'grad_norm': 4.179157257080078, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.5163, 'grad_norm': 0.2972586452960968, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.1004, 'grad_norm': 0.6049385070800781, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.3107, 'grad_norm': 0.7466598153114319, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3762, 'train_samples_per_second': 49.773, 'train_steps_per_second': 6.227, 'train_loss': 1.3176786422729492, 'epoch': 0.3}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_83/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)]]
proposed candidate before processing: tensor([1.5231e-01, 2.2433e-01, 0.0000e+00, 1.8662e-17, 2.1576e-01, 1.2029e-01,
        1.2224e-01, 1.6508e-01, 2.9722e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.9020e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1523), tensor(0.2243), 0, 0, tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), 1, 1, 1, 1, 1, 1, 101, 0.10000000149011612]
iteration:  84
input_X:  [tensor(0.1523), tensor(0.2243), 0, 0, tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), 1, 1, 1, 1, 1, 1, 101, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  101 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 6,929,408 || all params: 8,037,190,656 || trainable%: 0.0862
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1523), tensor(0.2243), 0, 0, tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1523)
number of datapoints needed (ratio * total):  761
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2243)
number of datapoints needed (ratio * total):  1121
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.2158)
number of datapoints needed (ratio * total):  1078
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1203)
number of datapoints needed (ratio * total):  601
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1222)
number of datapoints needed (ratio * total):  611
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1651)
number of datapoints needed (ratio * total):  825
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 761
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1121
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1078
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 601
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 611
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 825
})]
length of training data:  4997
training model...
trainable params: 6,929,408 || all params: 8,037,190,656 || trainable%: 0.0862
{'loss': 2.311, 'grad_norm': 1.2164963483810425, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.4741, 'grad_norm': 0.33048883080482483, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.4752, 'grad_norm': 0.6266393661499023, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3407, 'grad_norm': 0.9626732468605042, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.2318, 'grad_norm': 0.7268570065498352, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.3459, 'grad_norm': 0.6461414694786072, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.3306, 'grad_norm': 0.8683003187179565, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.3268, 'grad_norm': 0.8042144179344177, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.1832, 'grad_norm': 0.6552261710166931, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2647, 'train_samples_per_second': 49.838, 'train_steps_per_second': 6.233, 'train_loss': 1.4267491301974735, 'epoch': 0.3}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_84/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)], [tensor(0.1523), tensor(0.2243), tensor(0.), tensor(1.8662e-17), tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), tensor(0.0297), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7902), tensor(0.1000)]]
proposed candidate before processing: tensor([1.6202e-01, 1.6532e-01, 0.0000e+00, 3.8218e-18, 2.0623e-01, 1.4702e-01,
        1.5611e-01, 1.6330e-01, 2.8703e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.0907e-01, 3.0834e-02])
proposed candidate after normalizing: [tensor(0.1620), tensor(0.1653), 0, 0, tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), 1, 1, 1, 1, 1, 1, 104, 0.030833659693598747]
iteration:  85
input_X:  [tensor(0.1620), tensor(0.1653), 0, 0, tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), 1, 1, 1, 1, 1, 1, 104, 0.030833659693598747]
mixing data with method:  random
arranging lora config with parameters:  104 0.030833659693598747 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,135,232 || all params: 8,037,396,480 || trainable%: 0.0888
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1620), tensor(0.1653), 0, 0, tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1620)
number of datapoints needed (ratio * total):  810
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1653)
number of datapoints needed (ratio * total):  826
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.2062)
number of datapoints needed (ratio * total):  1031
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1470)
number of datapoints needed (ratio * total):  735
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1561)
number of datapoints needed (ratio * total):  780
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1633)
number of datapoints needed (ratio * total):  816
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 810
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 826
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1031
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 735
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 780
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 816
})]
length of training data:  4998
training model...
trainable params: 7,135,232 || all params: 8,037,396,480 || trainable%: 0.0888
{'loss': 2.3101, 'grad_norm': 0.5824267864227295, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.6866, 'grad_norm': 0.819784939289093, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.2771, 'grad_norm': 1.1430624723434448, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3163, 'grad_norm': 0.5920378565788269, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.6291, 'grad_norm': 0.34014225006103516, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1511, 'grad_norm': 0.521518349647522, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.3725, 'grad_norm': 1.0069870948791504, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.441, 'grad_norm': 1.339564323425293, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.5841, 'grad_norm': 1.0013830661773682, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0424, 'train_samples_per_second': 49.959, 'train_steps_per_second': 6.247, 'train_loss': 1.4983086897500197, 'epoch': 0.32}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_85/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)], [tensor(0.1523), tensor(0.2243), tensor(0.), tensor(1.8662e-17), tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), tensor(0.0297), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7902), tensor(0.1000)], [tensor(0.1620), tensor(0.1653), tensor(0.), tensor(3.8218e-18), tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8091), tensor(0.0308)]]
proposed candidate before processing: tensor([1.8014e-01, 2.0565e-01, 1.6880e-17, 1.7212e-18, 2.0450e-01, 1.2724e-01,
        1.1108e-01, 1.7140e-01, 2.8495e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.1080e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1801), tensor(0.2056), 0, 0, tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714), 1, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
iteration:  86
input_X:  [tensor(0.1801), tensor(0.2056), 0, 0, tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714), 1, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  104 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,135,232 || all params: 8,037,396,480 || trainable%: 0.0888
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1801), tensor(0.2056), 0, 0, tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1801)
number of datapoints needed (ratio * total):  900
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2056)
number of datapoints needed (ratio * total):  1028
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.2045)
number of datapoints needed (ratio * total):  1022
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1272)
number of datapoints needed (ratio * total):  636
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1111)
number of datapoints needed (ratio * total):  555
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1714)
number of datapoints needed (ratio * total):  856
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 900
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1028
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1022
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 636
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 555
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 856
})]
length of training data:  4997
training model...
trainable params: 7,135,232 || all params: 8,037,396,480 || trainable%: 0.0888
{'loss': 2.488, 'grad_norm': 1.7492955923080444, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.3362, 'grad_norm': 1.9248058795928955, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.8129, 'grad_norm': 0.7203558683395386, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.3166, 'grad_norm': 0.9864768385887146, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.4569, 'grad_norm': 0.5275213718414307, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.3838, 'grad_norm': 1.741929054260254, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.3493, 'grad_norm': 0.542589008808136, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.1824, 'grad_norm': 0.5040572285652161, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.3173, 'grad_norm': 0.410040944814682, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.464, 'train_samples_per_second': 49.739, 'train_steps_per_second': 6.221, 'train_loss': 1.5074225849008815, 'epoch': 0.3}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_86/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)], [tensor(0.1523), tensor(0.2243), tensor(0.), tensor(1.8662e-17), tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), tensor(0.0297), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7902), tensor(0.1000)], [tensor(0.1620), tensor(0.1653), tensor(0.), tensor(3.8218e-18), tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8091), tensor(0.0308)], [tensor(0.1801), tensor(0.2056), tensor(1.6880e-17), tensor(1.7212e-18), tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8108), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1418, 0.1422, 0.0000, 0.0000, 0.1991, 0.1625, 0.1919, 0.1626, 0.0286,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8124, 0.0181])
proposed candidate after normalizing: [tensor(0.1418), tensor(0.1422), 0, 0, tensor(0.1991), tensor(0.1625), tensor(0.1919), tensor(0.1626), 1, 1, 1, 1, 1, 1, 104, 0.01809866726398468]
iteration:  87
input_X:  [tensor(0.1418), tensor(0.1422), 0, 0, tensor(0.1991), tensor(0.1625), tensor(0.1919), tensor(0.1626), 1, 1, 1, 1, 1, 1, 104, 0.01809866726398468]
mixing data with method:  random
arranging lora config with parameters:  104 0.01809866726398468 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,135,232 || all params: 8,037,396,480 || trainable%: 0.0888
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1418), tensor(0.1422), 0, 0, tensor(0.1991), tensor(0.1625), tensor(0.1919), tensor(0.1626)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1418)
number of datapoints needed (ratio * total):  708
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1422)
number of datapoints needed (ratio * total):  711
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1991)
number of datapoints needed (ratio * total):  995
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1625)
number of datapoints needed (ratio * total):  812
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1919)
number of datapoints needed (ratio * total):  959
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1626)
number of datapoints needed (ratio * total):  813
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 708
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 711
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 995
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 812
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 959
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 813
})]
length of training data:  4998
training model...
trainable params: 7,135,232 || all params: 8,037,396,480 || trainable%: 0.0888
{'loss': 2.2267, 'grad_norm': 0.30006828904151917, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.6803, 'grad_norm': 0.9939015507698059, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.2623, 'grad_norm': 1.180264949798584, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.2185, 'grad_norm': 0.5893502831459045, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.1132, 'grad_norm': 0.587852954864502, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.4181, 'grad_norm': 1.1138217449188232, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.2271, 'grad_norm': 0.7059483528137207, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.5101, 'grad_norm': 0.5229507684707642, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.6871, 'grad_norm': 0.5868340134620667, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
{'loss': 1.2809, 'grad_norm': 0.3833494484424591, 'learning_rate': 0.00020731707317073167, 'epoch': 0.32}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1538, 'train_samples_per_second': 49.903, 'train_steps_per_second': 6.24, 'train_loss': 1.470522038529559, 'epoch': 0.33}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_87/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)], [tensor(0.1523), tensor(0.2243), tensor(0.), tensor(1.8662e-17), tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), tensor(0.0297), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7902), tensor(0.1000)], [tensor(0.1620), tensor(0.1653), tensor(0.), tensor(3.8218e-18), tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8091), tensor(0.0308)], [tensor(0.1801), tensor(0.2056), tensor(1.6880e-17), tensor(1.7212e-18), tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8108), tensor(0.1000)], [tensor(0.1418), tensor(0.1422), tensor(0.), tensor(0.), tensor(0.1991), tensor(0.1625), tensor(0.1919), tensor(0.1626), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8124), tensor(0.0181)]]
proposed candidate before processing: tensor([1.2285e-01, 2.0124e-01, 9.6358e-18, 1.0576e-02, 8.9587e-02, 1.6615e-01,
        2.8267e-01, 1.2693e-01, 1.0450e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.9464e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1229), tensor(0.2012), 0, 0, tensor(0.0896), tensor(0.1661), tensor(0.2827), tensor(0.1269), 3, 1, 1, 1, 1, 1, 115, 0.10000000149011612]
iteration:  88
input_X:  [tensor(0.1229), tensor(0.2012), 0, 0, tensor(0.0896), tensor(0.1661), tensor(0.2827), tensor(0.1269), 3, 1, 1, 1, 1, 1, 115, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  115 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 23,669,760 || all params: 8,053,931,008 || trainable%: 0.2939
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1229), tensor(0.2012), 0, 0, tensor(0.0896), tensor(0.1661), tensor(0.2827), tensor(0.1269)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1229)
number of datapoints needed (ratio * total):  614
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2012)
number of datapoints needed (ratio * total):  1006
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0896)
number of datapoints needed (ratio * total):  447
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1661)
number of datapoints needed (ratio * total):  830
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2827)
number of datapoints needed (ratio * total):  1413
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1269)
number of datapoints needed (ratio * total):  634
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 614
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1006
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 447
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 830
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1413
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 634
})]
length of training data:  4944
training model...
trainable params: 23,669,760 || all params: 8,053,931,008 || trainable%: 0.2939
{'loss': 2.106, 'grad_norm': 0.7149903774261475, 'learning_rate': 0.0002950657894736842, 'epoch': 0.03}
{'loss': 1.4939, 'grad_norm': 0.36927348375320435, 'learning_rate': 0.0002851973684210526, 'epoch': 0.06}
{'loss': 1.1166, 'grad_norm': 0.708840012550354, 'learning_rate': 0.00027532894736842105, 'epoch': 0.1}
{'loss': 1.0126, 'grad_norm': 0.3917597234249115, 'learning_rate': 0.0002654605263157894, 'epoch': 0.13}
{'loss': 1.1104, 'grad_norm': 0.7306318283081055, 'learning_rate': 0.00025559210526315785, 'epoch': 0.16}
{'loss': 1.2258, 'grad_norm': 0.7319197654724121, 'learning_rate': 0.0002457236842105263, 'epoch': 0.19}
{'loss': 0.9184, 'grad_norm': 0.40956562757492065, 'learning_rate': 0.00023585526315789474, 'epoch': 0.23}
{'loss': 0.9488, 'grad_norm': 0.9058547019958496, 'learning_rate': 0.00022598684210526314, 'epoch': 0.26}
{'loss': 1.2632, 'grad_norm': 0.7868751883506775, 'learning_rate': 0.00021611842105263157, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.402, 'train_samples_per_second': 49.242, 'train_steps_per_second': 6.155, 'train_loss': 1.2386552076288724, 'epoch': 0.3}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_88/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.59
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)], [tensor(0.1523), tensor(0.2243), tensor(0.), tensor(1.8662e-17), tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), tensor(0.0297), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7902), tensor(0.1000)], [tensor(0.1620), tensor(0.1653), tensor(0.), tensor(3.8218e-18), tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8091), tensor(0.0308)], [tensor(0.1801), tensor(0.2056), tensor(1.6880e-17), tensor(1.7212e-18), tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8108), tensor(0.1000)], [tensor(0.1418), tensor(0.1422), tensor(0.), tensor(0.), tensor(0.1991), tensor(0.1625), tensor(0.1919), tensor(0.1626), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8124), tensor(0.0181)], [tensor(0.1229), tensor(0.2012), tensor(9.6358e-18), tensor(0.0106), tensor(0.0896), tensor(0.1661), tensor(0.2827), tensor(0.1269), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8946), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1544, 0.2443, 0.0161, 0.0868, 0.0055, 0.0823, 0.1775, 0.2331, 0.1394,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8776, 0.0668])
proposed candidate after normalizing: [tensor(0.1544), tensor(0.2443), 0, tensor(0.0868), 0, tensor(0.0823), tensor(0.1775), tensor(0.2331), 4, 1, 1, 1, 1, 1, 112, 0.0668272003531456]
iteration:  89
input_X:  [tensor(0.1544), tensor(0.2443), 0, tensor(0.0868), 0, tensor(0.0823), tensor(0.1775), tensor(0.2331), 4, 1, 1, 1, 1, 1, 112, 0.0668272003531456]
mixing data with method:  random
arranging lora config with parameters:  112 0.0668272003531456 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 30,736,384 || all params: 8,060,997,632 || trainable%: 0.3813
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1544), tensor(0.2443), 0, tensor(0.0868), 0, tensor(0.0823), tensor(0.1775), tensor(0.2331)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1544)
number of datapoints needed (ratio * total):  772
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2443)
number of datapoints needed (ratio * total):  1221
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0868)
number of datapoints needed (ratio * total):  433
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  triviaqa
ratio:  tensor(0.0823)
number of datapoints needed (ratio * total):  411
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1775)
number of datapoints needed (ratio * total):  887
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2331)
number of datapoints needed (ratio * total):  1165
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 772
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1221
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 433
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 411
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 887
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1165
})]
length of training data:  4889
training model...
trainable params: 30,736,384 || all params: 8,060,997,632 || trainable%: 0.3813
{'loss': 2.115, 'grad_norm': 0.9885814189910889, 'learning_rate': 0.00029501661129568106, 'epoch': 0.03}
{'loss': 1.5932, 'grad_norm': 0.9079840183258057, 'learning_rate': 0.0002850498338870432, 'epoch': 0.07}
{'loss': 1.8239, 'grad_norm': 0.6658146977424622, 'learning_rate': 0.0002750830564784053, 'epoch': 0.1}
{'loss': 1.302, 'grad_norm': 1.0876649618148804, 'learning_rate': 0.00026511627906976743, 'epoch': 0.13}
{'loss': 1.34, 'grad_norm': 0.510871410369873, 'learning_rate': 0.00025514950166112955, 'epoch': 0.16}
{'loss': 1.1534, 'grad_norm': 0.601959764957428, 'learning_rate': 0.0002451827242524917, 'epoch': 0.2}
{'loss': 1.265, 'grad_norm': 0.45440736413002014, 'learning_rate': 0.0002352159468438538, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3973, 'train_samples_per_second': 48.697, 'train_steps_per_second': 6.096, 'train_loss': 1.5257999469072392, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_89/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.51
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)], [tensor(0.1523), tensor(0.2243), tensor(0.), tensor(1.8662e-17), tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), tensor(0.0297), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7902), tensor(0.1000)], [tensor(0.1620), tensor(0.1653), tensor(0.), tensor(3.8218e-18), tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8091), tensor(0.0308)], [tensor(0.1801), tensor(0.2056), tensor(1.6880e-17), tensor(1.7212e-18), tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8108), tensor(0.1000)], [tensor(0.1418), tensor(0.1422), tensor(0.), tensor(0.), tensor(0.1991), tensor(0.1625), tensor(0.1919), tensor(0.1626), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8124), tensor(0.0181)], [tensor(0.1229), tensor(0.2012), tensor(9.6358e-18), tensor(0.0106), tensor(0.0896), tensor(0.1661), tensor(0.2827), tensor(0.1269), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8946), tensor(0.1000)], [tensor(0.1544), tensor(0.2443), tensor(0.0161), tensor(0.0868), tensor(0.0055), tensor(0.0823), tensor(0.1775), tensor(0.2331), tensor(0.1394), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8776), tensor(0.0668)]]
proposed candidate before processing: tensor([1.1526e-01, 2.2545e-01, 5.6839e-17, 5.4103e-02, 1.9052e-01, 1.5708e-01,
        1.5925e-01, 9.8343e-02, 6.5743e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.7685e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1153), tensor(0.2254), 0, tensor(0.0541), tensor(0.1905), tensor(0.1571), tensor(0.1593), tensor(0.0983), 2, 1, 1, 1, 1, 1, 99, 0.10000000149011612]
iteration:  90
input_X:  [tensor(0.1153), tensor(0.2254), 0, tensor(0.0541), tensor(0.1905), tensor(0.1571), tensor(0.1593), tensor(0.0983), 2, 1, 1, 1, 1, 1, 99, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  99 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 28,952,576 || all params: 8,059,213,824 || trainable%: 0.3592
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1153), tensor(0.2254), 0, tensor(0.0541), tensor(0.1905), tensor(0.1571), tensor(0.1593), tensor(0.0983)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1153)
number of datapoints needed (ratio * total):  576
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2254)
number of datapoints needed (ratio * total):  1127
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0541)
number of datapoints needed (ratio * total):  270
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1905)
number of datapoints needed (ratio * total):  952
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1571)
number of datapoints needed (ratio * total):  785
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1593)
number of datapoints needed (ratio * total):  796
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.0983)
number of datapoints needed (ratio * total):  491
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 576
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1127
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 270
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 952
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 785
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 796
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 491
})]
length of training data:  4997
training model...
trainable params: 28,952,576 || all params: 8,059,213,824 || trainable%: 0.3592
{'loss': 0.9761, 'grad_norm': 0.4063001275062561, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.0283, 'grad_norm': 2.510636329650879, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.051, 'grad_norm': 0.4541871249675751, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 0.6951, 'grad_norm': 0.4936941862106323, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 0.8975, 'grad_norm': 0.4570836126804352, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 0.9106, 'grad_norm': 1.2032743692398071, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 0.7745, 'grad_norm': 1.0471925735473633, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 0.765, 'grad_norm': 0.33869674801826477, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6289, 'train_samples_per_second': 49.658, 'train_steps_per_second': 6.211, 'train_loss': 0.8808175997300581, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_90/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)], [tensor(0.1523), tensor(0.2243), tensor(0.), tensor(1.8662e-17), tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), tensor(0.0297), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7902), tensor(0.1000)], [tensor(0.1620), tensor(0.1653), tensor(0.), tensor(3.8218e-18), tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8091), tensor(0.0308)], [tensor(0.1801), tensor(0.2056), tensor(1.6880e-17), tensor(1.7212e-18), tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8108), tensor(0.1000)], [tensor(0.1418), tensor(0.1422), tensor(0.), tensor(0.), tensor(0.1991), tensor(0.1625), tensor(0.1919), tensor(0.1626), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8124), tensor(0.0181)], [tensor(0.1229), tensor(0.2012), tensor(9.6358e-18), tensor(0.0106), tensor(0.0896), tensor(0.1661), tensor(0.2827), tensor(0.1269), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8946), tensor(0.1000)], [tensor(0.1544), tensor(0.2443), tensor(0.0161), tensor(0.0868), tensor(0.0055), tensor(0.0823), tensor(0.1775), tensor(0.2331), tensor(0.1394), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8776), tensor(0.0668)], [tensor(0.1153), tensor(0.2254), tensor(5.6839e-17), tensor(0.0541), tensor(0.1905), tensor(0.1571), tensor(0.1593), tensor(0.0983), tensor(0.0657), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7768), tensor(0.1000)]]
proposed candidate before processing: tensor([2.2331e-01, 8.5752e-02, 8.0834e-02, 1.7429e-16, 1.2265e-01, 1.5968e-01,
        1.2142e-01, 2.0635e-01, 1.4015e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.6467e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.2233), tensor(0.0858), tensor(0.0808), 0, tensor(0.1227), tensor(0.1597), tensor(0.1214), tensor(0.2063), 4, 1, 1, 1, 1, 1, 111, 0.10000000149011612]
iteration:  91
input_X:  [tensor(0.2233), tensor(0.0858), tensor(0.0808), 0, tensor(0.1227), tensor(0.1597), tensor(0.1214), tensor(0.2063), 4, 1, 1, 1, 1, 1, 111, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  111 0.10000000149011612 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 30,461,952 || all params: 8,060,723,200 || trainable%: 0.3779
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.2233), tensor(0.0858), tensor(0.0808), 0, tensor(0.1227), tensor(0.1597), tensor(0.1214), tensor(0.2063)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.2233)
number of datapoints needed (ratio * total):  1116
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0858)
number of datapoints needed (ratio * total):  428
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0808)
number of datapoints needed (ratio * total):  404
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1227)
number of datapoints needed (ratio * total):  613
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1597)
number of datapoints needed (ratio * total):  798
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1214)
number of datapoints needed (ratio * total):  607
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2063)
number of datapoints needed (ratio * total):  1031
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1116
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 428
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 404
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 613
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 798
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 607
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1031
})]
length of training data:  4997
training model...
trainable params: 30,461,952 || all params: 8,060,723,200 || trainable%: 0.3779
{'loss': 2.4702, 'grad_norm': 1.2190550565719604, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.1864, 'grad_norm': 0.7374607920646667, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.5263, 'grad_norm': 0.4788914620876312, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.313, 'grad_norm': 1.3179430961608887, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3362, 'grad_norm': 2.064495086669922, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.0515, 'grad_norm': 0.7073999643325806, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.1172, 'grad_norm': 0.5396151542663574, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.1239, 'grad_norm': 1.020331621170044, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3535, 'train_samples_per_second': 49.794, 'train_steps_per_second': 6.228, 'train_loss': 1.3701362555054413, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_91/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.56
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)], [tensor(0.1523), tensor(0.2243), tensor(0.), tensor(1.8662e-17), tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), tensor(0.0297), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7902), tensor(0.1000)], [tensor(0.1620), tensor(0.1653), tensor(0.), tensor(3.8218e-18), tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8091), tensor(0.0308)], [tensor(0.1801), tensor(0.2056), tensor(1.6880e-17), tensor(1.7212e-18), tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8108), tensor(0.1000)], [tensor(0.1418), tensor(0.1422), tensor(0.), tensor(0.), tensor(0.1991), tensor(0.1625), tensor(0.1919), tensor(0.1626), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8124), tensor(0.0181)], [tensor(0.1229), tensor(0.2012), tensor(9.6358e-18), tensor(0.0106), tensor(0.0896), tensor(0.1661), tensor(0.2827), tensor(0.1269), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8946), tensor(0.1000)], [tensor(0.1544), tensor(0.2443), tensor(0.0161), tensor(0.0868), tensor(0.0055), tensor(0.0823), tensor(0.1775), tensor(0.2331), tensor(0.1394), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8776), tensor(0.0668)], [tensor(0.1153), tensor(0.2254), tensor(5.6839e-17), tensor(0.0541), tensor(0.1905), tensor(0.1571), tensor(0.1593), tensor(0.0983), tensor(0.0657), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7768), tensor(0.1000)], [tensor(0.2233), tensor(0.0858), tensor(0.0808), tensor(1.7429e-16), tensor(0.1227), tensor(0.1597), tensor(0.1214), tensor(0.2063), tensor(0.1401), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8647), tensor(0.1000)]]
proposed candidate before processing: tensor([0.2156, 0.1890, 0.0000, 0.0621, 0.0747, 0.0113, 0.1541, 0.2931, 0.1065,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8346, 0.1000])
proposed candidate after normalizing: [tensor(0.2156), tensor(0.1890), 0, tensor(0.0621), tensor(0.0747), 0, tensor(0.1541), tensor(0.2931), 3, 1, 1, 1, 1, 1, 107, 0.10000000149011612]
iteration:  92
input_X:  [tensor(0.2156), tensor(0.1890), 0, tensor(0.0621), tensor(0.0747), 0, tensor(0.1541), tensor(0.2931), 3, 1, 1, 1, 1, 1, 107, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  107 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 29,638,656 || all params: 8,059,899,904 || trainable%: 0.3677
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.2156), tensor(0.1890), 0, tensor(0.0621), tensor(0.0747), 0, tensor(0.1541), tensor(0.2931)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.2156)
number of datapoints needed (ratio * total):  1078
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1890)
number of datapoints needed (ratio * total):  945
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0621)
number of datapoints needed (ratio * total):  310
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0747)
number of datapoints needed (ratio * total):  373
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1541)
number of datapoints needed (ratio * total):  770
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2931)
number of datapoints needed (ratio * total):  1465
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1078
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 945
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 310
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 373
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 770
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1465
})]
length of training data:  4941
training model...
trainable params: 29,638,656 || all params: 8,059,899,904 || trainable%: 0.3677
{'loss': 2.026, 'grad_norm': 0.5276804566383362, 'learning_rate': 0.0002950657894736842, 'epoch': 0.03}
{'loss': 1.469, 'grad_norm': 0.4930298328399658, 'learning_rate': 0.0002851973684210526, 'epoch': 0.06}
{'loss': 1.6031, 'grad_norm': 1.2408167123794556, 'learning_rate': 0.00027532894736842105, 'epoch': 0.1}
{'loss': 1.3147, 'grad_norm': 0.6228423118591309, 'learning_rate': 0.0002654605263157894, 'epoch': 0.13}
{'loss': 1.3602, 'grad_norm': 0.6683839559555054, 'learning_rate': 0.00025559210526315785, 'epoch': 0.16}
{'loss': 1.1766, 'grad_norm': 0.555305004119873, 'learning_rate': 0.0002457236842105263, 'epoch': 0.19}
{'loss': 1.5637, 'grad_norm': 0.5230661034584045, 'learning_rate': 0.00023585526315789474, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4491, 'train_samples_per_second': 49.189, 'train_steps_per_second': 6.152, 'train_loss': 1.5072787358210638, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_92/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.64
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)], [tensor(0.1523), tensor(0.2243), tensor(0.), tensor(1.8662e-17), tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), tensor(0.0297), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7902), tensor(0.1000)], [tensor(0.1620), tensor(0.1653), tensor(0.), tensor(3.8218e-18), tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8091), tensor(0.0308)], [tensor(0.1801), tensor(0.2056), tensor(1.6880e-17), tensor(1.7212e-18), tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8108), tensor(0.1000)], [tensor(0.1418), tensor(0.1422), tensor(0.), tensor(0.), tensor(0.1991), tensor(0.1625), tensor(0.1919), tensor(0.1626), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8124), tensor(0.0181)], [tensor(0.1229), tensor(0.2012), tensor(9.6358e-18), tensor(0.0106), tensor(0.0896), tensor(0.1661), tensor(0.2827), tensor(0.1269), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8946), tensor(0.1000)], [tensor(0.1544), tensor(0.2443), tensor(0.0161), tensor(0.0868), tensor(0.0055), tensor(0.0823), tensor(0.1775), tensor(0.2331), tensor(0.1394), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8776), tensor(0.0668)], [tensor(0.1153), tensor(0.2254), tensor(5.6839e-17), tensor(0.0541), tensor(0.1905), tensor(0.1571), tensor(0.1593), tensor(0.0983), tensor(0.0657), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7768), tensor(0.1000)], [tensor(0.2233), tensor(0.0858), tensor(0.0808), tensor(1.7429e-16), tensor(0.1227), tensor(0.1597), tensor(0.1214), tensor(0.2063), tensor(0.1401), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8647), tensor(0.1000)], [tensor(0.2156), tensor(0.1890), tensor(0.), tensor(0.0621), tensor(0.0747), tensor(0.0113), tensor(0.1541), tensor(0.2931), tensor(0.1065), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8346), tensor(0.1000)]]
proposed candidate before processing: tensor([0.0868, 0.2151, 0.0215, 0.0232, 0.1054, 0.0455, 0.2318, 0.2706, 0.1401,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8423, 0.0811])
proposed candidate after normalizing: [tensor(0.0868), tensor(0.2151), 0, 0, tensor(0.1054), 0, tensor(0.2318), tensor(0.2706), 4, 1, 1, 1, 1, 1, 108, 0.0811242163181305]
iteration:  93
input_X:  [tensor(0.0868), tensor(0.2151), 0, 0, tensor(0.1054), 0, tensor(0.2318), tensor(0.2706), 4, 1, 1, 1, 1, 1, 108, 0.0811242163181305]
mixing data with method:  random
arranging lora config with parameters:  108 0.0811242163181305 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 29,638,656 || all params: 8,059,899,904 || trainable%: 0.3677
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0868), tensor(0.2151), 0, 0, tensor(0.1054), 0, tensor(0.2318), tensor(0.2706)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0868)
number of datapoints needed (ratio * total):  434
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.2151)
number of datapoints needed (ratio * total):  1075
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1054)
number of datapoints needed (ratio * total):  527
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2318)
number of datapoints needed (ratio * total):  1159
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2706)
number of datapoints needed (ratio * total):  1352
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 434
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1075
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 527
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1159
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1352
})]
length of training data:  4547
training model...
trainable params: 29,638,656 || all params: 8,059,899,904 || trainable%: 0.3677
{'loss': 2.1174, 'grad_norm': 0.5130640864372253, 'learning_rate': 0.0002946332737030411, 'epoch': 0.04}
{'loss': 1.5674, 'grad_norm': 0.5701268315315247, 'learning_rate': 0.0002838998211091234, 'epoch': 0.07}
{'loss': 1.8951, 'grad_norm': 0.5557135939598083, 'learning_rate': 0.0002731663685152057, 'epoch': 0.11}
{'loss': 1.4928, 'grad_norm': 0.6762027144432068, 'learning_rate': 0.000262432915921288, 'epoch': 0.14}
{'loss': 1.3083, 'grad_norm': 0.7469286918640137, 'learning_rate': 0.00025169946332737026, 'epoch': 0.18}
{'loss': 1.547, 'grad_norm': 0.6740226745605469, 'learning_rate': 0.00024096601073345258, 'epoch': 0.21}
{'loss': 1.3555, 'grad_norm': 1.1321063041687012, 'learning_rate': 0.00023023255813953486, 'epoch': 0.25}
{'loss': 1.4532, 'grad_norm': 0.5560164451599121, 'learning_rate': 0.00021949910554561715, 'epoch': 0.28}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4732, 'train_samples_per_second': 45.256, 'train_steps_per_second': 5.663, 'train_loss': 1.584412911034733, 'epoch': 0.3}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_93/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.57
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)], [tensor(0.1523), tensor(0.2243), tensor(0.), tensor(1.8662e-17), tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), tensor(0.0297), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7902), tensor(0.1000)], [tensor(0.1620), tensor(0.1653), tensor(0.), tensor(3.8218e-18), tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8091), tensor(0.0308)], [tensor(0.1801), tensor(0.2056), tensor(1.6880e-17), tensor(1.7212e-18), tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8108), tensor(0.1000)], [tensor(0.1418), tensor(0.1422), tensor(0.), tensor(0.), tensor(0.1991), tensor(0.1625), tensor(0.1919), tensor(0.1626), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8124), tensor(0.0181)], [tensor(0.1229), tensor(0.2012), tensor(9.6358e-18), tensor(0.0106), tensor(0.0896), tensor(0.1661), tensor(0.2827), tensor(0.1269), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8946), tensor(0.1000)], [tensor(0.1544), tensor(0.2443), tensor(0.0161), tensor(0.0868), tensor(0.0055), tensor(0.0823), tensor(0.1775), tensor(0.2331), tensor(0.1394), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8776), tensor(0.0668)], [tensor(0.1153), tensor(0.2254), tensor(5.6839e-17), tensor(0.0541), tensor(0.1905), tensor(0.1571), tensor(0.1593), tensor(0.0983), tensor(0.0657), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7768), tensor(0.1000)], [tensor(0.2233), tensor(0.0858), tensor(0.0808), tensor(1.7429e-16), tensor(0.1227), tensor(0.1597), tensor(0.1214), tensor(0.2063), tensor(0.1401), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8647), tensor(0.1000)], [tensor(0.2156), tensor(0.1890), tensor(0.), tensor(0.0621), tensor(0.0747), tensor(0.0113), tensor(0.1541), tensor(0.2931), tensor(0.1065), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8346), tensor(0.1000)], [tensor(0.0868), tensor(0.2151), tensor(0.0215), tensor(0.0232), tensor(0.1054), tensor(0.0455), tensor(0.2318), tensor(0.2706), tensor(0.1401), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8423), tensor(0.0811)]]
proposed candidate before processing: tensor([0.1941, 0.1543, 0.0000, 0.0342, 0.0799, 0.1163, 0.1579, 0.2633, 0.1060,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7872, 0.1000])
proposed candidate after normalizing: [tensor(0.1941), tensor(0.1543), 0, 0, tensor(0.0799), tensor(0.1163), tensor(0.1579), tensor(0.2633), 3, 1, 1, 1, 1, 1, 101, 0.10000000149011612]
iteration:  94
input_X:  [tensor(0.1941), tensor(0.1543), 0, 0, tensor(0.0799), tensor(0.1163), tensor(0.1579), tensor(0.2633), 3, 1, 1, 1, 1, 1, 101, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  101 0.10000000149011612 3 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 28,197,888 || all params: 8,058,459,136 || trainable%: 0.3499
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1941), tensor(0.1543), 0, 0, tensor(0.0799), tensor(0.1163), tensor(0.1579), tensor(0.2633)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1941)
number of datapoints needed (ratio * total):  970
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1543)
number of datapoints needed (ratio * total):  771
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.0799)
number of datapoints needed (ratio * total):  399
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1163)
number of datapoints needed (ratio * total):  581
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1579)
number of datapoints needed (ratio * total):  789
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2633)
number of datapoints needed (ratio * total):  1316
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 970
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 771
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 399
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 581
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 789
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1316
})]
length of training data:  4826
training model...
trainable params: 28,197,888 || all params: 8,058,459,136 || trainable%: 0.3499
{'loss': 1.3836, 'grad_norm': 0.7023411989212036, 'learning_rate': 0.00029494949494949493, 'epoch': 0.03}
{'loss': 1.6988, 'grad_norm': 0.6447986364364624, 'learning_rate': 0.0002848484848484848, 'epoch': 0.07}
{'loss': 1.1865, 'grad_norm': 0.8212807774543762, 'learning_rate': 0.0002747474747474747, 'epoch': 0.1}
{'loss': 1.5154, 'grad_norm': 0.4887814521789551, 'learning_rate': 0.00026464646464646464, 'epoch': 0.13}
{'loss': 1.5824, 'grad_norm': 0.7285077571868896, 'learning_rate': 0.0002545454545454545, 'epoch': 0.17}
{'loss': 1.2921, 'grad_norm': 0.5685107111930847, 'learning_rate': 0.00024444444444444443, 'epoch': 0.2}
{'loss': 1.444, 'grad_norm': 0.891425371170044, 'learning_rate': 0.00023434343434343432, 'epoch': 0.23}
{'loss': 1.1909, 'grad_norm': 0.6309680342674255, 'learning_rate': 0.00022424242424242424, 'epoch': 0.26}
{'loss': 1.2875, 'grad_norm': 0.399956077337265, 'learning_rate': 0.0002141414141414141, 'epoch': 0.3}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1357, 'train_samples_per_second': 48.195, 'train_steps_per_second': 6.032, 'train_loss': 1.3740932146708171, 'epoch': 0.31}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_94/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)], [tensor(0.1523), tensor(0.2243), tensor(0.), tensor(1.8662e-17), tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), tensor(0.0297), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7902), tensor(0.1000)], [tensor(0.1620), tensor(0.1653), tensor(0.), tensor(3.8218e-18), tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8091), tensor(0.0308)], [tensor(0.1801), tensor(0.2056), tensor(1.6880e-17), tensor(1.7212e-18), tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8108), tensor(0.1000)], [tensor(0.1418), tensor(0.1422), tensor(0.), tensor(0.), tensor(0.1991), tensor(0.1625), tensor(0.1919), tensor(0.1626), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8124), tensor(0.0181)], [tensor(0.1229), tensor(0.2012), tensor(9.6358e-18), tensor(0.0106), tensor(0.0896), tensor(0.1661), tensor(0.2827), tensor(0.1269), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8946), tensor(0.1000)], [tensor(0.1544), tensor(0.2443), tensor(0.0161), tensor(0.0868), tensor(0.0055), tensor(0.0823), tensor(0.1775), tensor(0.2331), tensor(0.1394), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8776), tensor(0.0668)], [tensor(0.1153), tensor(0.2254), tensor(5.6839e-17), tensor(0.0541), tensor(0.1905), tensor(0.1571), tensor(0.1593), tensor(0.0983), tensor(0.0657), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7768), tensor(0.1000)], [tensor(0.2233), tensor(0.0858), tensor(0.0808), tensor(1.7429e-16), tensor(0.1227), tensor(0.1597), tensor(0.1214), tensor(0.2063), tensor(0.1401), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8647), tensor(0.1000)], [tensor(0.2156), tensor(0.1890), tensor(0.), tensor(0.0621), tensor(0.0747), tensor(0.0113), tensor(0.1541), tensor(0.2931), tensor(0.1065), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8346), tensor(0.1000)], [tensor(0.0868), tensor(0.2151), tensor(0.0215), tensor(0.0232), tensor(0.1054), tensor(0.0455), tensor(0.2318), tensor(0.2706), tensor(0.1401), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8423), tensor(0.0811)], [tensor(0.1941), tensor(0.1543), tensor(0.), tensor(0.0342), tensor(0.0799), tensor(0.1163), tensor(0.1579), tensor(0.2633), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7872), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1137, 0.1251, 0.0000, 0.0634, 0.1878, 0.1657, 0.2194, 0.1250, 0.0678,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7936, 0.1000])
proposed candidate after normalizing: [tensor(0.1137), tensor(0.1251), 0, tensor(0.0634), tensor(0.1878), tensor(0.1657), tensor(0.2194), tensor(0.1250), 2, 1, 1, 1, 1, 1, 102, 0.10000000149011612]
iteration:  95
input_X:  [tensor(0.1137), tensor(0.1251), 0, tensor(0.0634), tensor(0.1878), tensor(0.1657), tensor(0.2194), tensor(0.1250), 2, 1, 1, 1, 1, 1, 102, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  102 0.10000000149011612 2 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 20,925,440 || all params: 8,051,186,688 || trainable%: 0.2599
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1137), tensor(0.1251), 0, tensor(0.0634), tensor(0.1878), tensor(0.1657), tensor(0.2194), tensor(0.1250)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1137)
number of datapoints needed (ratio * total):  568
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1251)
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0634)
number of datapoints needed (ratio * total):  316
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1878)
number of datapoints needed (ratio * total):  939
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1657)
number of datapoints needed (ratio * total):  828
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2194)
number of datapoints needed (ratio * total):  1097
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1250)
number of datapoints needed (ratio * total):  624
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 568
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 316
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 939
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 828
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1097
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 624
})]
length of training data:  4997
training model...
trainable params: 20,925,440 || all params: 8,051,186,688 || trainable%: 0.2599
{'loss': 1.8913, 'grad_norm': 1.738161325454712, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.3249, 'grad_norm': 0.8378667235374451, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.211, 'grad_norm': 0.7684532403945923, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.0537, 'grad_norm': 0.9767788052558899, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.1209, 'grad_norm': 0.5789696574211121, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.0586, 'grad_norm': 0.6155748963356018, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.0763, 'grad_norm': 0.757715106010437, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.1626, 'grad_norm': 0.9228763580322266, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3163, 'train_samples_per_second': 49.812, 'train_steps_per_second': 6.23, 'train_loss': 1.213806806608688, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_95/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)], [tensor(0.1523), tensor(0.2243), tensor(0.), tensor(1.8662e-17), tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), tensor(0.0297), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7902), tensor(0.1000)], [tensor(0.1620), tensor(0.1653), tensor(0.), tensor(3.8218e-18), tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8091), tensor(0.0308)], [tensor(0.1801), tensor(0.2056), tensor(1.6880e-17), tensor(1.7212e-18), tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8108), tensor(0.1000)], [tensor(0.1418), tensor(0.1422), tensor(0.), tensor(0.), tensor(0.1991), tensor(0.1625), tensor(0.1919), tensor(0.1626), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8124), tensor(0.0181)], [tensor(0.1229), tensor(0.2012), tensor(9.6358e-18), tensor(0.0106), tensor(0.0896), tensor(0.1661), tensor(0.2827), tensor(0.1269), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8946), tensor(0.1000)], [tensor(0.1544), tensor(0.2443), tensor(0.0161), tensor(0.0868), tensor(0.0055), tensor(0.0823), tensor(0.1775), tensor(0.2331), tensor(0.1394), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8776), tensor(0.0668)], [tensor(0.1153), tensor(0.2254), tensor(5.6839e-17), tensor(0.0541), tensor(0.1905), tensor(0.1571), tensor(0.1593), tensor(0.0983), tensor(0.0657), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7768), tensor(0.1000)], [tensor(0.2233), tensor(0.0858), tensor(0.0808), tensor(1.7429e-16), tensor(0.1227), tensor(0.1597), tensor(0.1214), tensor(0.2063), tensor(0.1401), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8647), tensor(0.1000)], [tensor(0.2156), tensor(0.1890), tensor(0.), tensor(0.0621), tensor(0.0747), tensor(0.0113), tensor(0.1541), tensor(0.2931), tensor(0.1065), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8346), tensor(0.1000)], [tensor(0.0868), tensor(0.2151), tensor(0.0215), tensor(0.0232), tensor(0.1054), tensor(0.0455), tensor(0.2318), tensor(0.2706), tensor(0.1401), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8423), tensor(0.0811)], [tensor(0.1941), tensor(0.1543), tensor(0.), tensor(0.0342), tensor(0.0799), tensor(0.1163), tensor(0.1579), tensor(0.2633), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7872), tensor(0.1000)], [tensor(0.1137), tensor(0.1251), tensor(0.), tensor(0.0634), tensor(0.1878), tensor(0.1657), tensor(0.2194), tensor(0.1250), tensor(0.0678), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7936), tensor(0.1000)]]
proposed candidate before processing: tensor([1.2545e-01, 1.7910e-01, 3.8489e-18, 0.0000e+00, 1.9842e-01, 1.5547e-01,
        1.9476e-01, 1.4680e-01, 3.0619e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 7.2655e-01, 5.9551e-02])
proposed candidate after normalizing: [tensor(0.1255), tensor(0.1791), 0, 0, tensor(0.1984), tensor(0.1555), tensor(0.1948), tensor(0.1468), 1, 1, 1, 1, 1, 1, 93, 0.05955074727535248]
iteration:  96
input_X:  [tensor(0.1255), tensor(0.1791), 0, 0, tensor(0.1984), tensor(0.1555), tensor(0.1948), tensor(0.1468), 1, 1, 1, 1, 1, 1, 93, 0.05955074727535248]
mixing data with method:  random
arranging lora config with parameters:  93 0.05955074727535248 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 13,378,560 || all params: 8,043,639,808 || trainable%: 0.1663
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1255), tensor(0.1791), 0, 0, tensor(0.1984), tensor(0.1555), tensor(0.1948), tensor(0.1468)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1255)
number of datapoints needed (ratio * total):  627
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1791)
number of datapoints needed (ratio * total):  895
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1984)
number of datapoints needed (ratio * total):  992
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1555)
number of datapoints needed (ratio * total):  777
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1948)
number of datapoints needed (ratio * total):  973
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1468)
number of datapoints needed (ratio * total):  733
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 627
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 895
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 992
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 777
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 973
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 733
})]
length of training data:  4997
training model...
trainable params: 13,378,560 || all params: 8,043,639,808 || trainable%: 0.1663
{'loss': 1.4382, 'grad_norm': 1.139615535736084, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.1699, 'grad_norm': 0.6820814609527588, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3489, 'grad_norm': 0.8140643835067749, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.0092, 'grad_norm': 0.8623493909835815, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.1007, 'grad_norm': 1.1972243785858154, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1479, 'grad_norm': 0.6464530229568481, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 0.8768, 'grad_norm': 0.596424400806427, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.2766, 'grad_norm': 0.7458080053329468, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.1691, 'grad_norm': 0.6955316066741943, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2777, 'train_samples_per_second': 49.832, 'train_steps_per_second': 6.233, 'train_loss': 1.160107344867074, 'epoch': 0.3}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_96/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)], [tensor(0.1523), tensor(0.2243), tensor(0.), tensor(1.8662e-17), tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), tensor(0.0297), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7902), tensor(0.1000)], [tensor(0.1620), tensor(0.1653), tensor(0.), tensor(3.8218e-18), tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8091), tensor(0.0308)], [tensor(0.1801), tensor(0.2056), tensor(1.6880e-17), tensor(1.7212e-18), tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8108), tensor(0.1000)], [tensor(0.1418), tensor(0.1422), tensor(0.), tensor(0.), tensor(0.1991), tensor(0.1625), tensor(0.1919), tensor(0.1626), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8124), tensor(0.0181)], [tensor(0.1229), tensor(0.2012), tensor(9.6358e-18), tensor(0.0106), tensor(0.0896), tensor(0.1661), tensor(0.2827), tensor(0.1269), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8946), tensor(0.1000)], [tensor(0.1544), tensor(0.2443), tensor(0.0161), tensor(0.0868), tensor(0.0055), tensor(0.0823), tensor(0.1775), tensor(0.2331), tensor(0.1394), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8776), tensor(0.0668)], [tensor(0.1153), tensor(0.2254), tensor(5.6839e-17), tensor(0.0541), tensor(0.1905), tensor(0.1571), tensor(0.1593), tensor(0.0983), tensor(0.0657), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7768), tensor(0.1000)], [tensor(0.2233), tensor(0.0858), tensor(0.0808), tensor(1.7429e-16), tensor(0.1227), tensor(0.1597), tensor(0.1214), tensor(0.2063), tensor(0.1401), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8647), tensor(0.1000)], [tensor(0.2156), tensor(0.1890), tensor(0.), tensor(0.0621), tensor(0.0747), tensor(0.0113), tensor(0.1541), tensor(0.2931), tensor(0.1065), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8346), tensor(0.1000)], [tensor(0.0868), tensor(0.2151), tensor(0.0215), tensor(0.0232), tensor(0.1054), tensor(0.0455), tensor(0.2318), tensor(0.2706), tensor(0.1401), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8423), tensor(0.0811)], [tensor(0.1941), tensor(0.1543), tensor(0.), tensor(0.0342), tensor(0.0799), tensor(0.1163), tensor(0.1579), tensor(0.2633), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7872), tensor(0.1000)], [tensor(0.1137), tensor(0.1251), tensor(0.), tensor(0.0634), tensor(0.1878), tensor(0.1657), tensor(0.2194), tensor(0.1250), tensor(0.0678), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7936), tensor(0.1000)], [tensor(0.1255), tensor(0.1791), tensor(3.8489e-18), tensor(0.), tensor(0.1984), tensor(0.1555), tensor(0.1948), tensor(0.1468), tensor(0.0306), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7265), tensor(0.0596)]]
proposed candidate before processing: tensor([1.0278e-01, 1.6105e-01, 3.5914e-18, 4.0115e-18, 1.8366e-01, 1.5356e-01,
        1.8326e-01, 2.1568e-01, 2.9289e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.1586e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1028), tensor(0.1611), 0, 0, tensor(0.1837), tensor(0.1536), tensor(0.1833), tensor(0.2157), 1, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
iteration:  97
input_X:  [tensor(0.1028), tensor(0.1611), 0, 0, tensor(0.1837), tensor(0.1536), tensor(0.1833), tensor(0.2157), 1, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  104 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,135,232 || all params: 8,037,396,480 || trainable%: 0.0888
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1028), tensor(0.1611), 0, 0, tensor(0.1837), tensor(0.1536), tensor(0.1833), tensor(0.2157)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1028)
number of datapoints needed (ratio * total):  513
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1611)
number of datapoints needed (ratio * total):  805
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1837)
number of datapoints needed (ratio * total):  918
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1536)
number of datapoints needed (ratio * total):  767
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1833)
number of datapoints needed (ratio * total):  916
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2157)
number of datapoints needed (ratio * total):  1078
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 513
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 805
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 918
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 767
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 916
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1078
})]
length of training data:  4997
training model...
trainable params: 7,135,232 || all params: 8,037,396,480 || trainable%: 0.0888
{'loss': 2.2927, 'grad_norm': 1.8990097045898438, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.4928, 'grad_norm': 0.6951242685317993, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.7697, 'grad_norm': 0.621538519859314, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.4837, 'grad_norm': 1.0638729333877563, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.5076, 'grad_norm': 0.4584473669528961, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.578, 'grad_norm': 0.7517099976539612, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.5485, 'grad_norm': 0.413850337266922, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.5077, 'grad_norm': 0.3744814991950989, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.4358, 'grad_norm': 0.5179904103279114, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1217, 'train_samples_per_second': 49.909, 'train_steps_per_second': 6.242, 'train_loss': 1.5999871078802614, 'epoch': 0.31}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_97/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)], [tensor(0.1523), tensor(0.2243), tensor(0.), tensor(1.8662e-17), tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), tensor(0.0297), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7902), tensor(0.1000)], [tensor(0.1620), tensor(0.1653), tensor(0.), tensor(3.8218e-18), tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8091), tensor(0.0308)], [tensor(0.1801), tensor(0.2056), tensor(1.6880e-17), tensor(1.7212e-18), tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8108), tensor(0.1000)], [tensor(0.1418), tensor(0.1422), tensor(0.), tensor(0.), tensor(0.1991), tensor(0.1625), tensor(0.1919), tensor(0.1626), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8124), tensor(0.0181)], [tensor(0.1229), tensor(0.2012), tensor(9.6358e-18), tensor(0.0106), tensor(0.0896), tensor(0.1661), tensor(0.2827), tensor(0.1269), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8946), tensor(0.1000)], [tensor(0.1544), tensor(0.2443), tensor(0.0161), tensor(0.0868), tensor(0.0055), tensor(0.0823), tensor(0.1775), tensor(0.2331), tensor(0.1394), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8776), tensor(0.0668)], [tensor(0.1153), tensor(0.2254), tensor(5.6839e-17), tensor(0.0541), tensor(0.1905), tensor(0.1571), tensor(0.1593), tensor(0.0983), tensor(0.0657), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7768), tensor(0.1000)], [tensor(0.2233), tensor(0.0858), tensor(0.0808), tensor(1.7429e-16), tensor(0.1227), tensor(0.1597), tensor(0.1214), tensor(0.2063), tensor(0.1401), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8647), tensor(0.1000)], [tensor(0.2156), tensor(0.1890), tensor(0.), tensor(0.0621), tensor(0.0747), tensor(0.0113), tensor(0.1541), tensor(0.2931), tensor(0.1065), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8346), tensor(0.1000)], [tensor(0.0868), tensor(0.2151), tensor(0.0215), tensor(0.0232), tensor(0.1054), tensor(0.0455), tensor(0.2318), tensor(0.2706), tensor(0.1401), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8423), tensor(0.0811)], [tensor(0.1941), tensor(0.1543), tensor(0.), tensor(0.0342), tensor(0.0799), tensor(0.1163), tensor(0.1579), tensor(0.2633), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7872), tensor(0.1000)], [tensor(0.1137), tensor(0.1251), tensor(0.), tensor(0.0634), tensor(0.1878), tensor(0.1657), tensor(0.2194), tensor(0.1250), tensor(0.0678), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7936), tensor(0.1000)], [tensor(0.1255), tensor(0.1791), tensor(3.8489e-18), tensor(0.), tensor(0.1984), tensor(0.1555), tensor(0.1948), tensor(0.1468), tensor(0.0306), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7265), tensor(0.0596)], [tensor(0.1028), tensor(0.1611), tensor(3.5914e-18), tensor(4.0115e-18), tensor(0.1837), tensor(0.1536), tensor(0.1833), tensor(0.2157), tensor(0.0293), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8159), tensor(0.1000)]]
proposed candidate before processing: tensor([1.5736e-01, 1.4654e-01, 0.0000e+00, 1.5850e-17, 1.9581e-01, 1.9462e-01,
        1.7447e-01, 1.3120e-01, 2.9128e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.1120e-01, 1.0000e-01])
proposed candidate after normalizing: [tensor(0.1574), tensor(0.1465), 0, 0, tensor(0.1958), tensor(0.1946), tensor(0.1745), tensor(0.1312), 1, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
iteration:  98
input_X:  [tensor(0.1574), tensor(0.1465), 0, 0, tensor(0.1958), tensor(0.1946), tensor(0.1745), tensor(0.1312), 1, 1, 1, 1, 1, 1, 104, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  104 0.10000000149011612 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,135,232 || all params: 8,037,396,480 || trainable%: 0.0888
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1574), tensor(0.1465), 0, 0, tensor(0.1958), tensor(0.1946), tensor(0.1745), tensor(0.1312)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1574)
number of datapoints needed (ratio * total):  786
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1465)
number of datapoints needed (ratio * total):  732
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1958)
number of datapoints needed (ratio * total):  979
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1946)
number of datapoints needed (ratio * total):  973
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1745)
number of datapoints needed (ratio * total):  872
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1312)
number of datapoints needed (ratio * total):  655
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 786
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 732
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 979
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 973
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 872
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 655
})]
length of training data:  4997
training model...
trainable params: 7,135,232 || all params: 8,037,396,480 || trainable%: 0.0888
{'loss': 2.2118, 'grad_norm': 2.5526435375213623, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.4606, 'grad_norm': 0.5456794500350952, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.1964, 'grad_norm': 1.0696706771850586, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.1629, 'grad_norm': 0.4360179007053375, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.1569, 'grad_norm': 0.674601674079895, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.2922, 'grad_norm': 0.45348644256591797, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 0.8978, 'grad_norm': 0.396648645401001, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.4339, 'grad_norm': 0.5672667026519775, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
{'loss': 1.1904, 'grad_norm': 0.5951176285743713, 'learning_rate': 0.0002170731707317073, 'epoch': 0.29}
{'loss': 1.0631, 'grad_norm': 0.7503721714019775, 'learning_rate': 0.00020731707317073167, 'epoch': 0.32}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0767, 'train_samples_per_second': 49.932, 'train_steps_per_second': 6.245, 'train_loss': 1.3133873729144825, 'epoch': 0.33}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_98/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)], [tensor(0.1523), tensor(0.2243), tensor(0.), tensor(1.8662e-17), tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), tensor(0.0297), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7902), tensor(0.1000)], [tensor(0.1620), tensor(0.1653), tensor(0.), tensor(3.8218e-18), tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8091), tensor(0.0308)], [tensor(0.1801), tensor(0.2056), tensor(1.6880e-17), tensor(1.7212e-18), tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8108), tensor(0.1000)], [tensor(0.1418), tensor(0.1422), tensor(0.), tensor(0.), tensor(0.1991), tensor(0.1625), tensor(0.1919), tensor(0.1626), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8124), tensor(0.0181)], [tensor(0.1229), tensor(0.2012), tensor(9.6358e-18), tensor(0.0106), tensor(0.0896), tensor(0.1661), tensor(0.2827), tensor(0.1269), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8946), tensor(0.1000)], [tensor(0.1544), tensor(0.2443), tensor(0.0161), tensor(0.0868), tensor(0.0055), tensor(0.0823), tensor(0.1775), tensor(0.2331), tensor(0.1394), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8776), tensor(0.0668)], [tensor(0.1153), tensor(0.2254), tensor(5.6839e-17), tensor(0.0541), tensor(0.1905), tensor(0.1571), tensor(0.1593), tensor(0.0983), tensor(0.0657), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7768), tensor(0.1000)], [tensor(0.2233), tensor(0.0858), tensor(0.0808), tensor(1.7429e-16), tensor(0.1227), tensor(0.1597), tensor(0.1214), tensor(0.2063), tensor(0.1401), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8647), tensor(0.1000)], [tensor(0.2156), tensor(0.1890), tensor(0.), tensor(0.0621), tensor(0.0747), tensor(0.0113), tensor(0.1541), tensor(0.2931), tensor(0.1065), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8346), tensor(0.1000)], [tensor(0.0868), tensor(0.2151), tensor(0.0215), tensor(0.0232), tensor(0.1054), tensor(0.0455), tensor(0.2318), tensor(0.2706), tensor(0.1401), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8423), tensor(0.0811)], [tensor(0.1941), tensor(0.1543), tensor(0.), tensor(0.0342), tensor(0.0799), tensor(0.1163), tensor(0.1579), tensor(0.2633), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7872), tensor(0.1000)], [tensor(0.1137), tensor(0.1251), tensor(0.), tensor(0.0634), tensor(0.1878), tensor(0.1657), tensor(0.2194), tensor(0.1250), tensor(0.0678), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7936), tensor(0.1000)], [tensor(0.1255), tensor(0.1791), tensor(3.8489e-18), tensor(0.), tensor(0.1984), tensor(0.1555), tensor(0.1948), tensor(0.1468), tensor(0.0306), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7265), tensor(0.0596)], [tensor(0.1028), tensor(0.1611), tensor(3.5914e-18), tensor(4.0115e-18), tensor(0.1837), tensor(0.1536), tensor(0.1833), tensor(0.2157), tensor(0.0293), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8159), tensor(0.1000)], [tensor(0.1574), tensor(0.1465), tensor(0.), tensor(1.5850e-17), tensor(0.1958), tensor(0.1946), tensor(0.1745), tensor(0.1312), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.1000)]]
proposed candidate before processing: tensor([1.5610e-01, 1.4648e-01, 6.3901e-02, 3.8664e-17, 1.7340e-01, 8.8863e-02,
        1.6809e-01, 2.0317e-01, 2.8924e-02, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        1.0000e+00, 1.0000e+00, 8.0182e-01, 4.0686e-02])
proposed candidate after normalizing: [tensor(0.1561), tensor(0.1465), tensor(0.0639), 0, tensor(0.1734), tensor(0.0889), tensor(0.1681), tensor(0.2032), 1, 1, 1, 1, 1, 1, 103, 0.040685687214136124]
iteration:  99
input_X:  [tensor(0.1561), tensor(0.1465), tensor(0.0639), 0, tensor(0.1734), tensor(0.0889), tensor(0.1681), tensor(0.2032), 1, 1, 1, 1, 1, 1, 103, 0.040685687214136124]
mixing data with method:  random
arranging lora config with parameters:  103 0.040685687214136124 1 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 7,066,624 || all params: 8,037,327,872 || trainable%: 0.0879
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1561), tensor(0.1465), tensor(0.0639), 0, tensor(0.1734), tensor(0.0889), tensor(0.1681), tensor(0.2032)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1561)
number of datapoints needed (ratio * total):  780
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1465)
number of datapoints needed (ratio * total):  732
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0639)
number of datapoints needed (ratio * total):  319
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1734)
number of datapoints needed (ratio * total):  867
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0889)
number of datapoints needed (ratio * total):  444
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1681)
number of datapoints needed (ratio * total):  840
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2032)
number of datapoints needed (ratio * total):  1015
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 780
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 732
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 319
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 867
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 444
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 840
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1015
})]
length of training data:  4997
training model...
trainable params: 7,066,624 || all params: 8,037,327,872 || trainable%: 0.0879
{'loss': 2.3785, 'grad_norm': 1.950860619544983, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.3628, 'grad_norm': 0.2833986282348633, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.7906, 'grad_norm': 0.8960118889808655, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.7865, 'grad_norm': 0.8339568376541138, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.5786, 'grad_norm': 0.9419549703598022, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.4018, 'grad_norm': 0.6294741034507751, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.1531, 'grad_norm': 0.9447641372680664, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
{'loss': 1.3267, 'grad_norm': 0.9481390714645386, 'learning_rate': 0.00022682926829268292, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.44, 'train_samples_per_second': 49.751, 'train_steps_per_second': 6.223, 'train_loss': 1.567605878031531, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_99/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0626), tensor(0.0674), tensor(0.1261), tensor(0.0863), tensor(0.1798), tensor(0.2245), tensor(0.1441), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8695), tensor(0.0580)], [tensor(0.0820), tensor(0.0924), tensor(0.0352), tensor(0.0142), tensor(0.1860), tensor(0.1108), tensor(0.2588), tensor(0.2205), tensor(0.0900), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8135), tensor(0.0557)], [tensor(0.1516), tensor(0.1312), tensor(0.0480), tensor(0.0335), tensor(0.1182), tensor(0.0711), tensor(0.1538), tensor(0.2926), tensor(0.0667), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7729), tensor(0.0654)], [tensor(0.0830), tensor(0.1132), tensor(0.0681), tensor(0.1023), tensor(0.1554), tensor(0.1093), tensor(0.2417), tensor(0.1268), tensor(0.1365), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9347), tensor(0.0473)], [tensor(0.1058), tensor(0.0974), tensor(0.0433), tensor(1.4934e-19), tensor(0.1611), tensor(0.1014), tensor(0.2168), tensor(0.2740), tensor(0.2283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8943), tensor(0.0612)], [tensor(0.1176), tensor(0.1303), tensor(0.0702), tensor(0.0945), tensor(0.1347), tensor(0.0942), tensor(0.1923), tensor(0.1663), tensor(0.0738), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7685), tensor(0.0528)], [tensor(0.0978), tensor(0.1001), tensor(0.0329), tensor(0.1305), tensor(0.1403), tensor(0.0828), tensor(0.1987), tensor(0.2169), tensor(0.0953), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8060), tensor(0.0575)], [tensor(0.1351), tensor(0.1722), tensor(0.0984), tensor(0.0117), tensor(0.1208), tensor(0.0804), tensor(0.2026), tensor(0.1788), tensor(0.1014), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.0504)], [tensor(0.1458), tensor(0.0882), tensor(0.0725), tensor(0.0596), tensor(0.1594), tensor(0.1507), tensor(0.1488), tensor(0.1749), tensor(0.1005), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8024), tensor(0.0609)], [tensor(0.0721), tensor(0.1330), tensor(0.0959), tensor(0.0723), tensor(0.1627), tensor(0.0948), tensor(0.1693), tensor(0.1999), tensor(0.1094), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7948), tensor(0.0291)], [tensor(0.1876), tensor(0.1061), tensor(0.0077), tensor(0.0757), tensor(0.1040), tensor(0.0999), tensor(0.2660), tensor(0.1530), tensor(0.1035), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7959), tensor(0.1000)], [tensor(0.1451), tensor(0.1227), tensor(0.0323), tensor(0.0753), tensor(0.0743), tensor(0.1437), tensor(0.2303), tensor(0.1763), tensor(0.0990), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8082), tensor(0.1000)], [tensor(0.2004), tensor(0.0922), tensor(0.0291), tensor(0.0781), tensor(0.1802), tensor(0.0391), tensor(0.2277), tensor(0.1532), tensor(0.1034), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8047), tensor(0.0597)], [tensor(0.1496), tensor(0.1100), tensor(0.0163), tensor(0.0627), tensor(0.0997), tensor(0.1371), tensor(0.2511), tensor(0.1735), tensor(0.0989), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7854), tensor(0.0317)], [tensor(0.1392), tensor(0.0760), tensor(0.0917), tensor(0.0733), tensor(0.1025), tensor(0.1002), tensor(0.2389), tensor(0.1783), tensor(0.0969), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7991), tensor(0.1000)], [tensor(0.1412), tensor(0.1970), tensor(2.2441e-18), tensor(0.0702), tensor(0.1452), tensor(0.1303), tensor(0.1607), tensor(0.1554), tensor(0.1058), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.0993)], [tensor(0.1604), tensor(0.1783), tensor(1.0066e-17), tensor(0.0907), tensor(0.1111), tensor(0.1233), tensor(0.1645), tensor(0.1718), tensor(0.1132), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8235), tensor(0.0460)], [tensor(0.1154), tensor(0.1607), tensor(0.), tensor(0.0462), tensor(0.1617), tensor(0.1266), tensor(0.2249), tensor(0.1645), tensor(0.0974), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7774), tensor(0.1000)], [tensor(0.1331), tensor(0.1420), tensor(4.1505e-18), tensor(0.0532), tensor(0.1610), tensor(0.1366), tensor(0.1973), tensor(0.1768), tensor(0.0731), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8155), tensor(0.1000)], [tensor(0.1358), tensor(0.1344), tensor(8.6559e-18), tensor(0.0370), tensor(0.1762), tensor(0.1497), tensor(0.1898), tensor(0.1772), tensor(0.0217), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.1000)], [tensor(0.1361), tensor(0.1308), tensor(6.1812e-18), tensor(0.0537), tensor(0.1735), tensor(0.1397), tensor(0.1837), tensor(0.1825), tensor(0.0457), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8301), tensor(0.1000)], [tensor(0.1242), tensor(0.1610), tensor(0.), tensor(0.0329), tensor(0.1526), tensor(0.1206), tensor(0.2335), tensor(0.1753), tensor(0.0519), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8199), tensor(0.1000)], [tensor(0.1413), tensor(0.1235), tensor(1.2636e-17), tensor(0.0648), tensor(0.1692), tensor(0.1561), tensor(0.1717), tensor(0.1734), tensor(0.1021), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7983), tensor(0.1000)], [tensor(0.1270), tensor(0.1723), tensor(6.7509e-19), tensor(0.0640), tensor(0.1300), tensor(0.1034), tensor(0.2238), tensor(0.1794), tensor(0.0233), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8381), tensor(0.0871)], [tensor(0.1439), tensor(0.1588), tensor(0.0065), tensor(0.0267), tensor(0.1461), tensor(0.1291), tensor(0.2191), tensor(0.1698), tensor(0.0383), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8267), tensor(0.0581)], [tensor(0.1227), tensor(0.1633), tensor(0.0018), tensor(0.0858), tensor(0.1369), tensor(0.0938), tensor(0.2097), tensor(0.1860), tensor(0.0805), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8243), tensor(0.1000)], [tensor(0.1335), tensor(0.1631), tensor(0.0166), tensor(0.0768), tensor(0.1369), tensor(0.0952), tensor(0.1951), tensor(0.1829), tensor(0.0820), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8321), tensor(0.1000)], [tensor(0.1090), tensor(0.1370), tensor(4.4534e-17), tensor(0.0512), tensor(0.1533), tensor(0.1532), tensor(0.2334), tensor(0.1629), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7900), tensor(0.0705)], [tensor(0.0976), tensor(0.1342), tensor(2.3192e-17), tensor(0.0510), tensor(0.1563), tensor(0.1682), tensor(0.2427), tensor(0.1501), tensor(0.0032), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7794), tensor(0.0711)], [tensor(0.2128), tensor(0.0999), tensor(6.3423e-18), tensor(0.0296), tensor(0.2021), tensor(0.1135), tensor(0.1490), tensor(0.1931), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8426), tensor(0.0984)], [tensor(0.1844), tensor(0.1423), tensor(2.4931e-18), tensor(0.0271), tensor(0.1529), tensor(0.1126), tensor(0.1766), tensor(0.2041), tensor(0.1135), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8427), tensor(0.1000)], [tensor(0.1671), tensor(0.1050), tensor(2.6068e-17), tensor(0.0803), tensor(0.1848), tensor(0.1095), tensor(0.1709), tensor(0.1825), tensor(0.1122), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8204), tensor(0.0695)], [tensor(0.1521), tensor(0.2156), tensor(0.0213), tensor(0.0155), tensor(0.0893), tensor(0.0829), tensor(0.2065), tensor(0.2169), tensor(0.1042), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8763), tensor(0.1000)], [tensor(0.0041), tensor(0.1085), tensor(0.), tensor(0.0847), tensor(0.2586), tensor(0.1881), tensor(0.2425), tensor(0.1134), tensor(0.0712), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7903), tensor(0.0556)], [tensor(0.2982), tensor(0.2573), tensor(0.), tensor(0.), tensor(0.0034), tensor(0.0971), tensor(0.1520), tensor(0.1919), tensor(0.1116), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8848), tensor(0.1000)], [tensor(0.1511), tensor(0.1216), tensor(0.), tensor(0.0526), tensor(0.2264), tensor(0.1330), tensor(0.1167), tensor(0.1986), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9335), tensor(0.1000)], [tensor(0.1809), tensor(0.1898), tensor(2.6512e-17), tensor(6.1478e-18), tensor(0.1225), tensor(0.1356), tensor(0.1224), tensor(0.2489), tensor(0.1033), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9977), tensor(0.0850)], [tensor(0.1080), tensor(0.2070), tensor(0.), tensor(0.0217), tensor(0.1588), tensor(0.1548), tensor(0.2164), tensor(0.1334), tensor(0.0615), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9187), tensor(0.0736)], [tensor(0.1567), tensor(0.1383), tensor(8.4229e-18), tensor(0.0040), tensor(0.1800), tensor(0.2175), tensor(0.2551), tensor(0.0484), tensor(0.0681), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8406), tensor(0.0810)], [tensor(5.2584e-18), tensor(0.1977), tensor(2.0762e-17), tensor(0.1200), tensor(0.1848), tensor(0.1199), tensor(0.2150), tensor(0.1625), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8937), tensor(0.1000)], [tensor(0.1236), tensor(0.1145), tensor(0.), tensor(0.), tensor(0.2532), tensor(0.1769), tensor(0.1441), tensor(0.1877), tensor(0.0750), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7756), tensor(1.7319e-18)], [tensor(0.0627), tensor(0.0728), tensor(5.3194e-17), tensor(0.0713), tensor(0.1369), tensor(0.1425), tensor(0.3608), tensor(0.1530), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7923), tensor(0.1000)], [tensor(0.1369), tensor(0.1795), tensor(0.), tensor(0.1366), tensor(0.1291), tensor(0.1778), tensor(0.1778), tensor(0.0622), tensor(0.0650), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8133), tensor(0.1000)], [tensor(0.1168), tensor(0.1473), tensor(0.0197), tensor(0.), tensor(0.2238), tensor(0.0617), tensor(0.2960), tensor(0.1347), tensor(0.0669), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9118), tensor(0.1000)], [tensor(0.), tensor(0.1708), tensor(0.), tensor(0.), tensor(0.1361), tensor(0.2795), tensor(0.2340), tensor(0.1796), tensor(0.0685), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8525), tensor(0.1000)], [tensor(0.0981), tensor(0.2893), tensor(0.), tensor(7.6789e-17), tensor(0.0490), tensor(0.0664), tensor(0.2758), tensor(0.2213), tensor(0.1077), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8054), tensor(0.1000)], [tensor(0.1181), tensor(0.1567), tensor(3.0493e-18), tensor(1.2184e-17), tensor(0.1398), tensor(0.2262), tensor(0.1696), tensor(0.1896), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8505), tensor(0.1000)], [tensor(0.1949), tensor(0.1402), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.1217), tensor(0.1663), tensor(0.1981), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7961), tensor(0.1000)], [tensor(0.2247), tensor(0.1177), tensor(1.6487e-17), tensor(3.3380e-17), tensor(0.1792), tensor(0.1480), tensor(0.2119), tensor(0.1185), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8337), tensor(0.1000)], [tensor(0.1381), tensor(0.2155), tensor(3.6727e-18), tensor(0.), tensor(0.1989), tensor(0.1328), tensor(0.1914), tensor(0.1232), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8223), tensor(0.1000)], [tensor(1.6263e-18), tensor(0.1130), tensor(0.), tensor(0.1209), tensor(0.2000), tensor(0.1540), tensor(0.2716), tensor(0.1404), tensor(0.0766), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7505), tensor(0.1000)], [tensor(0.2408), tensor(0.1473), tensor(0.), tensor(0.0347), tensor(0.1647), tensor(0.1614), tensor(0.1292), tensor(0.1219), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8009), tensor(0.1000)], [tensor(0.1529), tensor(0.0719), tensor(0.), tensor(6.3744e-17), tensor(0.1990), tensor(0.1389), tensor(0.2429), tensor(0.1944), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8225), tensor(0.1000)], [tensor(0.1407), tensor(0.0728), tensor(0.0839), tensor(0.), tensor(0.1875), tensor(0.1259), tensor(0.2057), tensor(0.1835), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8296), tensor(0.1000)], [tensor(0.1762), tensor(0.0982), tensor(0.), tensor(0.), tensor(0.1789), tensor(0.0902), tensor(0.2359), tensor(0.2206), tensor(0.0281), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8414), tensor(0.1000)], [tensor(0.1220), tensor(0.0777), tensor(0.0065), tensor(2.0849e-17), tensor(0.2418), tensor(0.2289), tensor(0.2013), tensor(0.1218), tensor(0.0302), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7855), tensor(0.1000)], [tensor(0.1315), tensor(0.1115), tensor(0.0764), tensor(1.8296e-17), tensor(0.1941), tensor(0.1898), tensor(0.2037), tensor(0.0930), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8139), tensor(0.1000)], [tensor(0.1360), tensor(0.1055), tensor(1.6670e-18), tensor(1.6263e-18), tensor(0.2287), tensor(0.1736), tensor(0.1428), tensor(0.2134), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7588), tensor(0.1000)], [tensor(0.1907), tensor(0.1925), tensor(0.0449), tensor(0.0201), tensor(0.0409), tensor(0.1025), tensor(0.1310), tensor(0.2775), tensor(0.1460), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8402), tensor(0.0833)], [tensor(0.2108), tensor(0.0472), tensor(1.6629e-17), tensor(5.2313e-18), tensor(0.1646), tensor(0.2010), tensor(0.2076), tensor(0.1689), tensor(0.0294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8125), tensor(0.1000)], [tensor(0.1245), tensor(0.1181), tensor(0.), tensor(0.0551), tensor(0.2284), tensor(0.1326), tensor(0.1978), tensor(0.1436), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8074), tensor(0.1000)], [tensor(0.0610), tensor(0.1892), tensor(2.2000e-17), tensor(7.9143e-18), tensor(0.1587), tensor(0.2443), tensor(0.2282), tensor(0.1186), tensor(0.0566), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9618), tensor(0.0180)], [tensor(0.0880), tensor(0.2543), tensor(4.2962e-18), tensor(1.2875e-17), tensor(0.1294), tensor(0.1423), tensor(0.2511), tensor(0.1349), tensor(0.0709), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8071), tensor(0.1000)], [tensor(0.1520), tensor(0.1864), tensor(1.0357e-16), tensor(2.3690e-17), tensor(0.1493), tensor(0.1481), tensor(0.1869), tensor(0.1774), tensor(0.0296), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7906), tensor(0.1000)], [tensor(0.1300), tensor(0.1473), tensor(2.5835e-17), tensor(7.7656e-18), tensor(0.2008), tensor(0.1639), tensor(0.1863), tensor(0.1718), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.1000)], [tensor(0.2250), tensor(0.1142), tensor(0.), tensor(9.9984e-18), tensor(0.1891), tensor(0.1202), tensor(0.2174), tensor(0.1340), tensor(0.0301), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7709), tensor(0.1000)], [tensor(0.1466), tensor(0.0993), tensor(6.1593e-17), tensor(0.0678), tensor(0.1569), tensor(0.1750), tensor(0.1644), tensor(0.1901), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8171), tensor(0.1000)], [tensor(0.1796), tensor(0.1364), tensor(0.), tensor(5.2897e-18), tensor(0.1832), tensor(0.1737), tensor(0.1464), tensor(0.1806), tensor(0.0279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8284), tensor(0.1000)], [tensor(0.0725), tensor(0.1021), tensor(6.0417e-17), tensor(0.0245), tensor(0.1707), tensor(0.1776), tensor(0.2718), tensor(0.1808), tensor(0.0300), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8095), tensor(0.1000)], [tensor(0.1625), tensor(0.1182), tensor(0.), tensor(0.0736), tensor(0.1525), tensor(0.1446), tensor(0.2106), tensor(0.1380), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8236), tensor(0.1000)], [tensor(0.0820), tensor(0.1553), tensor(0.), tensor(1.9536e-17), tensor(0.2055), tensor(0.1521), tensor(0.2104), tensor(0.1948), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7981), tensor(0.1000)], [tensor(0.1385), tensor(0.0754), tensor(0.), tensor(0.0670), tensor(0.1465), tensor(0.1846), tensor(0.2236), tensor(0.1644), tensor(0.0725), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8564), tensor(0.1000)], [tensor(0.1194), tensor(0.1194), tensor(0.), tensor(0.0445), tensor(0.1493), tensor(0.1831), tensor(0.2213), tensor(0.1632), tensor(0.0740), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8581), tensor(0.0654)], [tensor(0.1627), tensor(0.0690), tensor(0.), tensor(0.), tensor(0.2184), tensor(0.1703), tensor(0.2247), tensor(0.1549), tensor(0.0283), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8239), tensor(0.1000)], [tensor(0.1669), tensor(0.1550), tensor(0.), tensor(0.), tensor(0.1324), tensor(0.1668), tensor(0.1963), tensor(0.1825), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7913), tensor(0.1000)], [tensor(0.0959), tensor(0.0814), tensor(0.0776), tensor(0.0850), tensor(0.1778), tensor(0.0544), tensor(0.1817), tensor(0.2462), tensor(0.1490), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8565), tensor(0.0386)], [tensor(0.1974), tensor(0.1742), tensor(0.0401), tensor(0.0278), tensor(0.0600), tensor(0.1249), tensor(0.1475), tensor(0.2281), tensor(0.1374), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8603), tensor(0.0959)], [tensor(0.2604), tensor(0.2293), tensor(9.3919e-18), tensor(0.), tensor(0.0953), tensor(0.0994), tensor(0.1274), tensor(0.1882), tensor(0.1030), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8411), tensor(0.1000)], [tensor(0.1583), tensor(0.1610), tensor(0.), tensor(4.0658e-17), tensor(0.2119), tensor(0.1275), tensor(0.2091), tensor(0.1321), tensor(0.0288), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8258), tensor(0.1000)], [tensor(0.1455), tensor(0.0466), tensor(6.1257e-18), tensor(0.0487), tensor(0.2042), tensor(0.1682), tensor(0.1898), tensor(0.1970), tensor(0.0292), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7820), tensor(0.1000)], [tensor(0.1652), tensor(0.1838), tensor(8.9807e-17), tensor(5.0195e-18), tensor(0.1454), tensor(0.1495), tensor(0.2356), tensor(0.1205), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8727), tensor(0.1000)], [tensor(0.1523), tensor(0.2243), tensor(0.), tensor(1.8662e-17), tensor(0.2158), tensor(0.1203), tensor(0.1222), tensor(0.1651), tensor(0.0297), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7902), tensor(0.1000)], [tensor(0.1620), tensor(0.1653), tensor(0.), tensor(3.8218e-18), tensor(0.2062), tensor(0.1470), tensor(0.1561), tensor(0.1633), tensor(0.0287), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8091), tensor(0.0308)], [tensor(0.1801), tensor(0.2056), tensor(1.6880e-17), tensor(1.7212e-18), tensor(0.2045), tensor(0.1272), tensor(0.1111), tensor(0.1714), tensor(0.0285), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8108), tensor(0.1000)], [tensor(0.1418), tensor(0.1422), tensor(0.), tensor(0.), tensor(0.1991), tensor(0.1625), tensor(0.1919), tensor(0.1626), tensor(0.0286), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8124), tensor(0.0181)], [tensor(0.1229), tensor(0.2012), tensor(9.6358e-18), tensor(0.0106), tensor(0.0896), tensor(0.1661), tensor(0.2827), tensor(0.1269), tensor(0.1045), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8946), tensor(0.1000)], [tensor(0.1544), tensor(0.2443), tensor(0.0161), tensor(0.0868), tensor(0.0055), tensor(0.0823), tensor(0.1775), tensor(0.2331), tensor(0.1394), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8776), tensor(0.0668)], [tensor(0.1153), tensor(0.2254), tensor(5.6839e-17), tensor(0.0541), tensor(0.1905), tensor(0.1571), tensor(0.1593), tensor(0.0983), tensor(0.0657), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7768), tensor(0.1000)], [tensor(0.2233), tensor(0.0858), tensor(0.0808), tensor(1.7429e-16), tensor(0.1227), tensor(0.1597), tensor(0.1214), tensor(0.2063), tensor(0.1401), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8647), tensor(0.1000)], [tensor(0.2156), tensor(0.1890), tensor(0.), tensor(0.0621), tensor(0.0747), tensor(0.0113), tensor(0.1541), tensor(0.2931), tensor(0.1065), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8346), tensor(0.1000)], [tensor(0.0868), tensor(0.2151), tensor(0.0215), tensor(0.0232), tensor(0.1054), tensor(0.0455), tensor(0.2318), tensor(0.2706), tensor(0.1401), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8423), tensor(0.0811)], [tensor(0.1941), tensor(0.1543), tensor(0.), tensor(0.0342), tensor(0.0799), tensor(0.1163), tensor(0.1579), tensor(0.2633), tensor(0.1060), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7872), tensor(0.1000)], [tensor(0.1137), tensor(0.1251), tensor(0.), tensor(0.0634), tensor(0.1878), tensor(0.1657), tensor(0.2194), tensor(0.1250), tensor(0.0678), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7936), tensor(0.1000)], [tensor(0.1255), tensor(0.1791), tensor(3.8489e-18), tensor(0.), tensor(0.1984), tensor(0.1555), tensor(0.1948), tensor(0.1468), tensor(0.0306), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7265), tensor(0.0596)], [tensor(0.1028), tensor(0.1611), tensor(3.5914e-18), tensor(4.0115e-18), tensor(0.1837), tensor(0.1536), tensor(0.1833), tensor(0.2157), tensor(0.0293), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8159), tensor(0.1000)], [tensor(0.1574), tensor(0.1465), tensor(0.), tensor(1.5850e-17), tensor(0.1958), tensor(0.1946), tensor(0.1745), tensor(0.1312), tensor(0.0291), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8112), tensor(0.1000)], [tensor(0.1561), tensor(0.1465), tensor(0.0639), tensor(3.8664e-17), tensor(0.1734), tensor(0.0889), tensor(0.1681), tensor(0.2032), tensor(0.0289), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8018), tensor(0.0407)]]
proposed candidate before processing: tensor([0.1413, 0.1595, 0.0745, 0.0048, 0.1740, 0.0779, 0.1656, 0.2024, 0.0291,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7938, 0.0786])
proposed candidate after normalizing: [tensor(0.1413), tensor(0.1595), tensor(0.0745), 0, tensor(0.1740), tensor(0.0779), tensor(0.1656), tensor(0.2024), 1, 1, 1, 1, 1, 1, 102, 0.07857336848974228]
Best at every step: [0.35, 0.35, 0.6, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7]
running BO on both data and model with fixed feature list
commonsense_qa
headqa_en
hellaswag
pubmedqa
sciq
triviaqa
truthfulqa_gen
wikitext
fixed feature list generated:
{9: 0, 10: 0, 11: 0, 12: 0, 13: 0}
{9: 0, 10: 0, 11: 0, 12: 0, 13: 1}
{9: 0, 10: 0, 11: 0, 12: 1, 13: 0}
{9: 0, 10: 0, 11: 0, 12: 1, 13: 1}
{9: 0, 10: 0, 11: 1, 12: 0, 13: 0}
{9: 0, 10: 0, 11: 1, 12: 0, 13: 1}
{9: 0, 10: 0, 11: 1, 12: 1, 13: 0}
{9: 0, 10: 0, 11: 1, 12: 1, 13: 1}
{9: 0, 10: 1, 11: 0, 12: 0, 13: 0}
{9: 0, 10: 1, 11: 0, 12: 0, 13: 1}
{9: 0, 10: 1, 11: 0, 12: 1, 13: 0}
{9: 0, 10: 1, 11: 0, 12: 1, 13: 1}
{9: 0, 10: 1, 11: 1, 12: 0, 13: 0}
{9: 0, 10: 1, 11: 1, 12: 0, 13: 1}
{9: 0, 10: 1, 11: 1, 12: 1, 13: 0}
{9: 0, 10: 1, 11: 1, 12: 1, 13: 1}
{9: 1, 10: 0, 11: 0, 12: 0, 13: 0}
{9: 1, 10: 0, 11: 0, 12: 0, 13: 1}
{9: 1, 10: 0, 11: 0, 12: 1, 13: 0}
{9: 1, 10: 0, 11: 0, 12: 1, 13: 1}
{9: 1, 10: 0, 11: 1, 12: 0, 13: 0}
{9: 1, 10: 0, 11: 1, 12: 0, 13: 1}
{9: 1, 10: 0, 11: 1, 12: 1, 13: 0}
{9: 1, 10: 0, 11: 1, 12: 1, 13: 1}
{9: 1, 10: 1, 11: 0, 12: 0, 13: 0}
{9: 1, 10: 1, 11: 0, 12: 0, 13: 1}
{9: 1, 10: 1, 11: 0, 12: 1, 13: 0}
{9: 1, 10: 1, 11: 0, 12: 1, 13: 1}
{9: 1, 10: 1, 11: 1, 12: 0, 13: 0}
{9: 1, 10: 1, 11: 1, 12: 0, 13: 1}
{9: 1, 10: 1, 11: 1, 12: 1, 13: 0}
{9: 1, 10: 1, 11: 1, 12: 1, 13: 1}
iteration:  0
input_X:  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 16, 1, 1, 1, 1, 1, 72, 0.05]
mixing data with method:  random
arranging lora config with parameters:  72 0.05 16 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 79,036,416 || all params: 8,109,297,664 || trainable%: 0.9746
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  0.125
number of datapoints needed (ratio * total):  625
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 625
})]
length of training data:  5000
training model...
trainable params: 79,036,416 || all params: 8,109,297,664 || trainable%: 0.9746
{'loss': 1.8494, 'grad_norm': 0.8403694033622742, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.2571, 'grad_norm': 0.6450926661491394, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 0.8298, 'grad_norm': 0.45925360918045044, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 0.9204, 'grad_norm': 0.6809854507446289, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.0549, 'grad_norm': 0.6768918037414551, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 0.6906, 'grad_norm': 0.7942788004875183, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4718, 'train_samples_per_second': 49.765, 'train_steps_per_second': 6.221, 'train_loss': 1.0660570789785946, 'epoch': 0.22}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_0/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.33
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05]]
proposed candidate before processing: tensor([0.1320, 0.0412, 0.0666, 0.3804, 0.0453, 0.0872, 0.0189, 0.2285, 0.7649,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0406, 0.0897])
proposed candidate after normalizing: [tensor(0.1320), 0, tensor(0.0666), tensor(0.3804), 0, tensor(0.0872), 0, tensor(0.2285), 24, 0, 0, 0, 0, 0, 5, 0.08970030397176743]
iteration:  1
input_X:  [tensor(0.1320), 0, tensor(0.0666), tensor(0.3804), 0, tensor(0.0872), 0, tensor(0.2285), 24, 0, 0, 0, 0, 0, 5, 0.08970030397176743]
mixing data with method:  random
arranging lora config with parameters:  5 0.08970030397176743 24 [0, 0, 0, 0, 0]
[]
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)]]
proposed candidate before processing: tensor([0.1265, 0.1256, 0.0871, 0.0900, 0.1257, 0.1015, 0.1582, 0.1853, 0.2839,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7487, 0.0549])
proposed candidate after normalizing: [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), 9, 1, 1, 1, 1, 1, 96, 0.054870955646038055]
iteration:  2
input_X:  [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), 9, 1, 1, 1, 1, 1, 96, 0.054870955646038055]
mixing data with method:  random
arranging lora config with parameters:  96 0.054870955646038055 9 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 93,855,744 || all params: 8,124,116,992 || trainable%: 1.1553
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1265)
number of datapoints needed (ratio * total):  632
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1256)
number of datapoints needed (ratio * total):  628
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0871)
number of datapoints needed (ratio * total):  435
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0900)
number of datapoints needed (ratio * total):  450
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1257)
number of datapoints needed (ratio * total):  628
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1015)
number of datapoints needed (ratio * total):  507
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1582)
number of datapoints needed (ratio * total):  791
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1853)
number of datapoints needed (ratio * total):  926
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 632
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 628
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 435
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 450
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 628
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 507
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 791
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 926
})]
length of training data:  4997
training model...
trainable params: 93,855,744 || all params: 8,124,116,992 || trainable%: 1.1553
{'loss': 1.4105, 'grad_norm': 0.6050357818603516, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.0182, 'grad_norm': 0.8227397203445435, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.5088, 'grad_norm': 0.8475647568702698, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.252, 'grad_norm': 0.9890297651290894, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.3488, 'grad_norm': 0.44567498564720154, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1049, 'grad_norm': 0.7129473686218262, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0807, 'train_samples_per_second': 49.93, 'train_steps_per_second': 6.245, 'train_loss': 1.2705512973044415, 'epoch': 0.22}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_2/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.67
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)]]
proposed candidate before processing: tensor([0.1274, 0.1260, 0.0631, 0.0679, 0.1261, 0.0866, 0.1793, 0.2236, 0.1469,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8667, 0.0580])
proposed candidate after normalizing: [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), 5, 1, 1, 1, 1, 1, 111, 0.05796121433377266]
iteration:  3
input_X:  [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), 5, 1, 1, 1, 1, 1, 111, 0.05796121433377266]
mixing data with method:  random
arranging lora config with parameters:  111 0.05796121433377266 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 64,422,912 || all params: 8,094,684,160 || trainable%: 0.7959
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1274)
number of datapoints needed (ratio * total):  636
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1260)
number of datapoints needed (ratio * total):  629
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0631)
number of datapoints needed (ratio * total):  315
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0679)
number of datapoints needed (ratio * total):  339
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1261)
number of datapoints needed (ratio * total):  630
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0866)
number of datapoints needed (ratio * total):  433
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1793)
number of datapoints needed (ratio * total):  896
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2236)
number of datapoints needed (ratio * total):  1118
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 636
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 629
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 315
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 339
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 630
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 433
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 896
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1118
})]
length of training data:  4996
training model...
trainable params: 64,422,912 || all params: 8,094,684,160 || trainable%: 0.7959
{'loss': 1.9338, 'grad_norm': 0.4242940843105316, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.2707, 'grad_norm': 0.49855008721351624, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.3434, 'grad_norm': 0.6860007643699646, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.226, 'grad_norm': 1.0102777481079102, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.215, 'grad_norm': 0.564972460269928, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.5016, 'grad_norm': 0.7138013243675232, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.4967, 'grad_norm': 0.3741109073162079, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5336, 'train_samples_per_second': 49.695, 'train_steps_per_second': 6.217, 'train_loss': 1.406482222939835, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_3/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), tensor(0.1469), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8667), tensor(0.0580)]]
proposed candidate before processing: tensor([0.0824, 0.1702, 0.0712, 0.1161, 0.1012, 0.1705, 0.1466, 0.1418, 0.2273,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8683, 0.0473])
proposed candidate after normalizing: [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), 7, 1, 1, 1, 1, 1, 111, 0.047284409403800964]
iteration:  4
input_X:  [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), 7, 1, 1, 1, 1, 1, 111, 0.047284409403800964]
mixing data with method:  random
arranging lora config with parameters:  111 0.047284409403800964 7 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 53,308,416 || all params: 8,083,569,664 || trainable%: 0.6595
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0824)
number of datapoints needed (ratio * total):  412
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1702)
number of datapoints needed (ratio * total):  851
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0712)
number of datapoints needed (ratio * total):  355
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.1161)
number of datapoints needed (ratio * total):  580
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1012)
number of datapoints needed (ratio * total):  505
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1705)
number of datapoints needed (ratio * total):  852
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1466)
number of datapoints needed (ratio * total):  732
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1418)
number of datapoints needed (ratio * total):  709
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 412
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 851
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 355
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 580
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 505
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 852
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 732
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 709
})]
length of training data:  4996
training model...
trainable params: 53,308,416 || all params: 8,083,569,664 || trainable%: 0.6595
{'loss': 1.5207, 'grad_norm': 1.6668916940689087, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.1549, 'grad_norm': 0.5140755772590637, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 0.988, 'grad_norm': 1.0303956270217896, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.2242, 'grad_norm': 0.8321516513824463, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.0585, 'grad_norm': 0.46693989634513855, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.1915, 'grad_norm': 0.847231388092041, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.3641, 'grad_norm': 1.1120790243148804, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.4434, 'train_samples_per_second': 49.739, 'train_steps_per_second': 6.222, 'train_loss': 1.2180580602933282, 'epoch': 0.23}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_4/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.54
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), tensor(0.1469), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8667), tensor(0.0580)], [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), tensor(0.2273), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8683), tensor(0.0473)]]
proposed candidate before processing: tensor([0.1745, 0.0784, 0.0793, 0.0392, 0.1523, 0.0124, 0.1925, 0.2715, 0.2022,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7432, 0.0662])
proposed candidate after normalizing: [tensor(0.1745), tensor(0.0784), tensor(0.0793), 0, tensor(0.1523), 0, tensor(0.1925), tensor(0.2715), 6, 1, 1, 1, 1, 1, 95, 0.06618102639913559]
iteration:  5
input_X:  [tensor(0.1745), tensor(0.0784), tensor(0.0793), 0, tensor(0.1523), 0, tensor(0.1925), tensor(0.2715), 6, 1, 1, 1, 1, 1, 95, 0.06618102639913559]
mixing data with method:  random
arranging lora config with parameters:  95 0.06618102639913559 6 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 46,722,048 || all params: 8,076,983,296 || trainable%: 0.5785
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1745), tensor(0.0784), tensor(0.0793), 0, tensor(0.1523), 0, tensor(0.1925), tensor(0.2715)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1745)
number of datapoints needed (ratio * total):  872
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0784)
number of datapoints needed (ratio * total):  391
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0793)
number of datapoints needed (ratio * total):  396
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1523)
number of datapoints needed (ratio * total):  761
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1925)
number of datapoints needed (ratio * total):  962
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2715)
number of datapoints needed (ratio * total):  1357
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 872
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 391
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 396
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 761
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 962
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1357
})]
length of training data:  4739
training model...
trainable params: 46,722,048 || all params: 8,076,983,296 || trainable%: 0.5785
{'loss': 1.8746, 'grad_norm': 0.8023365139961243, 'learning_rate': 0.0002948542024013722, 'epoch': 0.03}
{'loss': 1.6945, 'grad_norm': 0.46297672390937805, 'learning_rate': 0.0002845626072041166, 'epoch': 0.07}
{'loss': 1.265, 'grad_norm': 1.3736318349838257, 'learning_rate': 0.00027427101200686103, 'epoch': 0.1}
{'loss': 1.5255, 'grad_norm': 1.7954524755477905, 'learning_rate': 0.00026397941680960545, 'epoch': 0.13}
{'loss': 1.2691, 'grad_norm': 0.9488698840141296, 'learning_rate': 0.0002536878216123499, 'epoch': 0.17}
{'loss': 1.4763, 'grad_norm': 0.8253591060638428, 'learning_rate': 0.00024339622641509433, 'epoch': 0.2}
{'loss': 1.0366, 'grad_norm': 0.37579670548439026, 'learning_rate': 0.00023310463121783875, 'epoch': 0.24}
{'loss': 1.4157, 'grad_norm': 1.2689659595489502, 'learning_rate': 0.00022281303602058315, 'epoch': 0.27}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2781, 'train_samples_per_second': 47.259, 'train_steps_per_second': 5.914, 'train_loss': 1.4378260988177676, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_5/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), tensor(0.1469), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8667), tensor(0.0580)], [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), tensor(0.2273), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8683), tensor(0.0473)], [tensor(0.1745), tensor(0.0784), tensor(0.0793), tensor(0.0392), tensor(0.1523), tensor(0.0124), tensor(0.1925), tensor(0.2715), tensor(0.2022), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7432), tensor(0.0662)]]
proposed candidate before processing: tensor([0.0894, 0.0980, 0.0828, 0.0563, 0.1756, 0.0663, 0.1821, 0.2494, 0.2663,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8249, 0.0473])
proposed candidate after normalizing: [tensor(0.0894), tensor(0.0980), tensor(0.0828), tensor(0.0563), tensor(0.1756), tensor(0.0663), tensor(0.1821), tensor(0.2494), 9, 1, 1, 1, 1, 1, 106, 0.04733053222298622]
iteration:  6
input_X:  [tensor(0.0894), tensor(0.0980), tensor(0.0828), tensor(0.0563), tensor(0.1756), tensor(0.0663), tensor(0.1821), tensor(0.2494), 9, 1, 1, 1, 1, 1, 106, 0.04733053222298622]
mixing data with method:  random
arranging lora config with parameters:  106 0.04733053222298622 9 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 65,452,032 || all params: 8,095,713,280 || trainable%: 0.8085
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.0894), tensor(0.0980), tensor(0.0828), tensor(0.0563), tensor(0.1756), tensor(0.0663), tensor(0.1821), tensor(0.2494)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.0894)
number of datapoints needed (ratio * total):  447
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0980)
number of datapoints needed (ratio * total):  490
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0828)
number of datapoints needed (ratio * total):  414
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0563)
number of datapoints needed (ratio * total):  281
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1756)
number of datapoints needed (ratio * total):  878
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0663)
number of datapoints needed (ratio * total):  331
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1821)
number of datapoints needed (ratio * total):  910
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2494)
number of datapoints needed (ratio * total):  1246
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 447
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 490
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 414
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 281
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 878
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 331
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 910
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1246
})]
length of training data:  4997
training model...
trainable params: 65,452,032 || all params: 8,095,713,280 || trainable%: 0.8085
{'loss': 2.3375, 'grad_norm': 1.5358396768569946, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5079, 'grad_norm': 0.9492323994636536, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.7006, 'grad_norm': 0.4948790669441223, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.5948, 'grad_norm': 0.6725776195526123, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.5477, 'grad_norm': 0.6091270446777344, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.509, 'grad_norm': 0.7562521696090698, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.3858, 'grad_norm': 0.9070731997489929, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.7503, 'train_samples_per_second': 49.598, 'train_steps_per_second': 6.203, 'train_loss': 1.6137567866932263, 'epoch': 0.25}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_6/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.59
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), tensor(0.1469), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8667), tensor(0.0580)], [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), tensor(0.2273), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8683), tensor(0.0473)], [tensor(0.1745), tensor(0.0784), tensor(0.0793), tensor(0.0392), tensor(0.1523), tensor(0.0124), tensor(0.1925), tensor(0.2715), tensor(0.2022), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7432), tensor(0.0662)], [tensor(0.0894), tensor(0.0980), tensor(0.0828), tensor(0.0563), tensor(0.1756), tensor(0.0663), tensor(0.1821), tensor(0.2494), tensor(0.2663), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.0473)]]
proposed candidate before processing: tensor([0.2018, 0.1324, 0.0700, 0.0856, 0.0794, 0.0815, 0.1644, 0.1850, 0.1562,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7390, 0.0732])
proposed candidate after normalizing: [tensor(0.2018), tensor(0.1324), tensor(0.0700), tensor(0.0856), tensor(0.0794), tensor(0.0815), tensor(0.1644), tensor(0.1850), 5, 1, 1, 1, 1, 1, 95, 0.0732487142086029]
iteration:  7
input_X:  [tensor(0.2018), tensor(0.1324), tensor(0.0700), tensor(0.0856), tensor(0.0794), tensor(0.0815), tensor(0.1644), tensor(0.1850), 5, 1, 1, 1, 1, 1, 95, 0.0732487142086029]
mixing data with method:  random
arranging lora config with parameters:  95 0.0732487142086029 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 61,678,592 || all params: 8,091,939,840 || trainable%: 0.7622
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.2018), tensor(0.1324), tensor(0.0700), tensor(0.0856), tensor(0.0794), tensor(0.0815), tensor(0.1644), tensor(0.1850)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.2018)
number of datapoints needed (ratio * total):  1009
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1324)
number of datapoints needed (ratio * total):  661
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0700)
number of datapoints needed (ratio * total):  349
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0856)
number of datapoints needed (ratio * total):  428
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0794)
number of datapoints needed (ratio * total):  396
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0815)
number of datapoints needed (ratio * total):  407
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1644)
number of datapoints needed (ratio * total):  822
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1850)
number of datapoints needed (ratio * total):  924
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1009
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 661
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 349
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 428
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 396
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 407
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 822
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 924
})]
length of training data:  4996
training model...
trainable params: 61,678,592 || all params: 8,091,939,840 || trainable%: 0.7622
{'loss': 1.3386, 'grad_norm': 0.5116511583328247, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.2033, 'grad_norm': 0.7856239080429077, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 0.9796, 'grad_norm': 0.3855993151664734, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.2799, 'grad_norm': 0.4458272457122803, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.0923, 'grad_norm': 0.32791608572006226, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.0278, 'grad_norm': 0.4871785044670105, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.2317, 'grad_norm': 0.6538485288619995, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2731, 'train_samples_per_second': 49.824, 'train_steps_per_second': 6.233, 'train_loss': 1.163711237580809, 'epoch': 0.23}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_7/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.7
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), tensor(0.1469), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8667), tensor(0.0580)], [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), tensor(0.2273), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8683), tensor(0.0473)], [tensor(0.1745), tensor(0.0784), tensor(0.0793), tensor(0.0392), tensor(0.1523), tensor(0.0124), tensor(0.1925), tensor(0.2715), tensor(0.2022), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7432), tensor(0.0662)], [tensor(0.0894), tensor(0.0980), tensor(0.0828), tensor(0.0563), tensor(0.1756), tensor(0.0663), tensor(0.1821), tensor(0.2494), tensor(0.2663), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.0473)], [tensor(0.2018), tensor(0.1324), tensor(0.0700), tensor(0.0856), tensor(0.0794), tensor(0.0815), tensor(0.1644), tensor(0.1850), tensor(0.1562), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7390), tensor(0.0732)]]
proposed candidate before processing: tensor([0.1366, 0.1227, 0.0808, 0.0714, 0.1215, 0.1047, 0.1713, 0.1909, 0.1311,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7103, 0.0602])
proposed candidate after normalizing: [tensor(0.1366), tensor(0.1227), tensor(0.0808), tensor(0.0714), tensor(0.1215), tensor(0.1047), tensor(0.1713), tensor(0.1909), 4, 1, 1, 1, 1, 1, 91, 0.0602293461561203]
iteration:  8
input_X:  [tensor(0.1366), tensor(0.1227), tensor(0.0808), tensor(0.0714), tensor(0.1215), tensor(0.1047), tensor(0.1713), tensor(0.1909), 4, 1, 1, 1, 1, 1, 91, 0.0602293461561203]
mixing data with method:  random
arranging lora config with parameters:  91 0.0602293461561203 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 31,491,072 || all params: 8,061,752,320 || trainable%: 0.3906
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1366), tensor(0.1227), tensor(0.0808), tensor(0.0714), tensor(0.1215), tensor(0.1047), tensor(0.1713), tensor(0.1909)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1366)
number of datapoints needed (ratio * total):  683
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1227)
number of datapoints needed (ratio * total):  613
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0808)
number of datapoints needed (ratio * total):  403
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0714)
number of datapoints needed (ratio * total):  357
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1215)
number of datapoints needed (ratio * total):  607
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1047)
number of datapoints needed (ratio * total):  523
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1713)
number of datapoints needed (ratio * total):  856
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1909)
number of datapoints needed (ratio * total):  954
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 683
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 613
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 403
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 357
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 607
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 523
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 856
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 954
})]
length of training data:  4996
training model...
trainable params: 31,491,072 || all params: 8,061,752,320 || trainable%: 0.3906
{'loss': 1.9841, 'grad_norm': 0.7575230598449707, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.3771, 'grad_norm': 0.7692310214042664, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.403, 'grad_norm': 0.4993244409561157, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.2759, 'grad_norm': 0.7251402139663696, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.1659, 'grad_norm': 1.123130440711975, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.3851, 'grad_norm': 0.4798513352870941, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.4647, 'grad_norm': 1.0872039794921875, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.5236, 'train_samples_per_second': 49.7, 'train_steps_per_second': 6.217, 'train_loss': 1.4416438582675908, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_8/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), tensor(0.1469), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8667), tensor(0.0580)], [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), tensor(0.2273), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8683), tensor(0.0473)], [tensor(0.1745), tensor(0.0784), tensor(0.0793), tensor(0.0392), tensor(0.1523), tensor(0.0124), tensor(0.1925), tensor(0.2715), tensor(0.2022), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7432), tensor(0.0662)], [tensor(0.0894), tensor(0.0980), tensor(0.0828), tensor(0.0563), tensor(0.1756), tensor(0.0663), tensor(0.1821), tensor(0.2494), tensor(0.2663), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.0473)], [tensor(0.2018), tensor(0.1324), tensor(0.0700), tensor(0.0856), tensor(0.0794), tensor(0.0815), tensor(0.1644), tensor(0.1850), tensor(0.1562), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7390), tensor(0.0732)], [tensor(0.1366), tensor(0.1227), tensor(0.0808), tensor(0.0714), tensor(0.1215), tensor(0.1047), tensor(0.1713), tensor(0.1909), tensor(0.1311), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7103), tensor(0.0602)]]
proposed candidate before processing: tensor([0.2486, 0.1345, 0.0601, 0.0962, 0.0560, 0.0464, 0.1603, 0.1979, 0.2362,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8100, 0.0813])
proposed candidate after normalizing: [tensor(0.2486), tensor(0.1345), tensor(0.0601), tensor(0.0962), tensor(0.0560), 0, tensor(0.1603), tensor(0.1979), 8, 1, 1, 1, 1, 1, 104, 0.08128713071346283]
iteration:  9
input_X:  [tensor(0.2486), tensor(0.1345), tensor(0.0601), tensor(0.0962), tensor(0.0560), 0, tensor(0.1603), tensor(0.1979), 8, 1, 1, 1, 1, 1, 104, 0.08128713071346283]
mixing data with method:  random
arranging lora config with parameters:  104 0.08128713071346283 8 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 57,081,856 || all params: 8,087,343,104 || trainable%: 0.7058
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.2486), tensor(0.1345), tensor(0.0601), tensor(0.0962), tensor(0.0560), 0, tensor(0.1603), tensor(0.1979)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.2486)
number of datapoints needed (ratio * total):  1243
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1345)
number of datapoints needed (ratio * total):  672
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0601)
number of datapoints needed (ratio * total):  300
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0962)
number of datapoints needed (ratio * total):  481
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0560)
number of datapoints needed (ratio * total):  280
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1603)
number of datapoints needed (ratio * total):  801
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1979)
number of datapoints needed (ratio * total):  989
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1243
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 672
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 300
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 481
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 280
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 801
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 989
})]
length of training data:  4766
training model...
trainable params: 57,081,856 || all params: 8,087,343,104 || trainable%: 0.7058
{'loss': 1.9577, 'grad_norm': 0.8321730494499207, 'learning_rate': 0.0002948805460750853, 'epoch': 0.03}
{'loss': 1.6345, 'grad_norm': 0.7486964464187622, 'learning_rate': 0.00028464163822525593, 'epoch': 0.07}
{'loss': 1.5101, 'grad_norm': 0.8400205969810486, 'learning_rate': 0.00027440273037542657, 'epoch': 0.1}
{'loss': 1.5442, 'grad_norm': 0.6537701487541199, 'learning_rate': 0.00026416382252559726, 'epoch': 0.13}
{'loss': 1.0749, 'grad_norm': 0.7100291848182678, 'learning_rate': 0.0002539249146757679, 'epoch': 0.17}
{'loss': 1.1823, 'grad_norm': 1.073449969291687, 'learning_rate': 0.00024368600682593853, 'epoch': 0.2}
{'loss': 1.5126, 'grad_norm': 0.49099117517471313, 'learning_rate': 0.0002334470989761092, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2884, 'train_samples_per_second': 47.523, 'train_steps_per_second': 5.943, 'train_loss': 1.490813893285291, 'epoch': 0.24}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_9/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.5
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), tensor(0.1469), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8667), tensor(0.0580)], [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), tensor(0.2273), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8683), tensor(0.0473)], [tensor(0.1745), tensor(0.0784), tensor(0.0793), tensor(0.0392), tensor(0.1523), tensor(0.0124), tensor(0.1925), tensor(0.2715), tensor(0.2022), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7432), tensor(0.0662)], [tensor(0.0894), tensor(0.0980), tensor(0.0828), tensor(0.0563), tensor(0.1756), tensor(0.0663), tensor(0.1821), tensor(0.2494), tensor(0.2663), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.0473)], [tensor(0.2018), tensor(0.1324), tensor(0.0700), tensor(0.0856), tensor(0.0794), tensor(0.0815), tensor(0.1644), tensor(0.1850), tensor(0.1562), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7390), tensor(0.0732)], [tensor(0.1366), tensor(0.1227), tensor(0.0808), tensor(0.0714), tensor(0.1215), tensor(0.1047), tensor(0.1713), tensor(0.1909), tensor(0.1311), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7103), tensor(0.0602)], [tensor(0.2486), tensor(0.1345), tensor(0.0601), tensor(0.0962), tensor(0.0560), tensor(0.0464), tensor(0.1603), tensor(0.1979), tensor(0.2362), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8100), tensor(0.0813)]]
proposed candidate before processing: tensor([0.1626, 0.0847, 0.1250, 0.0371, 0.1321, 0.1127, 0.1555, 0.1903, 0.1371,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7514, 0.0482])
proposed candidate after normalizing: [tensor(0.1626), tensor(0.0847), tensor(0.1250), 0, tensor(0.1321), tensor(0.1127), tensor(0.1555), tensor(0.1903), 4, 1, 1, 1, 1, 1, 96, 0.04819158464670181]
iteration:  10
input_X:  [tensor(0.1626), tensor(0.0847), tensor(0.1250), 0, tensor(0.1321), tensor(0.1127), tensor(0.1555), tensor(0.1903), 4, 1, 1, 1, 1, 1, 96, 0.04819158464670181]
mixing data with method:  random
arranging lora config with parameters:  96 0.04819158464670181 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 54,886,400 || all params: 8,085,147,648 || trainable%: 0.6789
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1626), tensor(0.0847), tensor(0.1250), 0, tensor(0.1321), tensor(0.1127), tensor(0.1555), tensor(0.1903)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1626)
number of datapoints needed (ratio * total):  812
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0847)
number of datapoints needed (ratio * total):  423
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1250)
number of datapoints needed (ratio * total):  624
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1321)
number of datapoints needed (ratio * total):  660
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1127)
number of datapoints needed (ratio * total):  563
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1555)
number of datapoints needed (ratio * total):  777
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1903)
number of datapoints needed (ratio * total):  951
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 812
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 423
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 624
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 660
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 563
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 777
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 951
})]
length of training data:  4810
training model...
trainable params: 54,886,400 || all params: 8,085,147,648 || trainable%: 0.6789
{'loss': 1.4135, 'grad_norm': 0.7214576005935669, 'learning_rate': 0.0002949324324324324, 'epoch': 0.03}
{'loss': 0.8577, 'grad_norm': 0.7677708864212036, 'learning_rate': 0.0002847972972972973, 'epoch': 0.07}
{'loss': 1.041, 'grad_norm': 0.5079242587089539, 'learning_rate': 0.0002746621621621621, 'epoch': 0.1}
{'loss': 1.1609, 'grad_norm': 0.5315475463867188, 'learning_rate': 0.000264527027027027, 'epoch': 0.13}
{'loss': 1.4596, 'grad_norm': 0.7503427863121033, 'learning_rate': 0.0002543918918918919, 'epoch': 0.17}
{'loss': 0.8785, 'grad_norm': 0.6482123732566833, 'learning_rate': 0.00024425675675675675, 'epoch': 0.2}
{'loss': 1.2482, 'grad_norm': 0.6871066689491272, 'learning_rate': 0.00023412162162162159, 'epoch': 0.23}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0849, 'train_samples_per_second': 48.059, 'train_steps_per_second': 6.015, 'train_loss': 1.1500230898523027, 'epoch': 0.26}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_10/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.66
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), tensor(0.1469), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8667), tensor(0.0580)], [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), tensor(0.2273), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8683), tensor(0.0473)], [tensor(0.1745), tensor(0.0784), tensor(0.0793), tensor(0.0392), tensor(0.1523), tensor(0.0124), tensor(0.1925), tensor(0.2715), tensor(0.2022), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7432), tensor(0.0662)], [tensor(0.0894), tensor(0.0980), tensor(0.0828), tensor(0.0563), tensor(0.1756), tensor(0.0663), tensor(0.1821), tensor(0.2494), tensor(0.2663), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.0473)], [tensor(0.2018), tensor(0.1324), tensor(0.0700), tensor(0.0856), tensor(0.0794), tensor(0.0815), tensor(0.1644), tensor(0.1850), tensor(0.1562), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7390), tensor(0.0732)], [tensor(0.1366), tensor(0.1227), tensor(0.0808), tensor(0.0714), tensor(0.1215), tensor(0.1047), tensor(0.1713), tensor(0.1909), tensor(0.1311), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7103), tensor(0.0602)], [tensor(0.2486), tensor(0.1345), tensor(0.0601), tensor(0.0962), tensor(0.0560), tensor(0.0464), tensor(0.1603), tensor(0.1979), tensor(0.2362), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8100), tensor(0.0813)], [tensor(0.1626), tensor(0.0847), tensor(0.1250), tensor(0.0371), tensor(0.1321), tensor(0.1127), tensor(0.1555), tensor(0.1903), tensor(0.1371), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7514), tensor(0.0482)]]
proposed candidate before processing: tensor([0.1612, 0.1044, 0.0457, 0.0568, 0.1502, 0.1033, 0.2165, 0.1620, 0.1549,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7439, 0.0659])
proposed candidate after normalizing: [tensor(0.1612), tensor(0.1044), 0, tensor(0.0568), tensor(0.1502), tensor(0.1033), tensor(0.2165), tensor(0.1620), 5, 1, 1, 1, 1, 1, 95, 0.06590280681848526]
iteration:  11
input_X:  [tensor(0.1612), tensor(0.1044), 0, tensor(0.0568), tensor(0.1502), tensor(0.1033), tensor(0.2165), tensor(0.1620), 5, 1, 1, 1, 1, 1, 95, 0.06590280681848526]
mixing data with method:  random
arranging lora config with parameters:  95 0.06590280681848526 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 32,588,800 || all params: 8,062,850,048 || trainable%: 0.4042
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1612), tensor(0.1044), 0, tensor(0.0568), tensor(0.1502), tensor(0.1033), tensor(0.2165), tensor(0.1620)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1612)
number of datapoints needed (ratio * total):  805
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1044)
number of datapoints needed (ratio * total):  521
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0568)
number of datapoints needed (ratio * total):  283
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1502)
number of datapoints needed (ratio * total):  751
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1033)
number of datapoints needed (ratio * total):  516
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.2165)
number of datapoints needed (ratio * total):  1082
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.1620)
number of datapoints needed (ratio * total):  810
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 805
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 521
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 283
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 751
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 516
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1082
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 810
})]
length of training data:  4768
training model...
trainable params: 32,588,800 || all params: 8,062,850,048 || trainable%: 0.4042
{'loss': 1.9817, 'grad_norm': 2.9337105751037598, 'learning_rate': 0.0002948805460750853, 'epoch': 0.03}
{'loss': 1.4006, 'grad_norm': 0.6811621189117432, 'learning_rate': 0.00028464163822525593, 'epoch': 0.07}
{'loss': 1.3984, 'grad_norm': 0.649252712726593, 'learning_rate': 0.00027440273037542657, 'epoch': 0.1}
{'loss': 1.4056, 'grad_norm': 0.6288476586341858, 'learning_rate': 0.00026416382252559726, 'epoch': 0.13}
{'loss': 1.1583, 'grad_norm': 0.8154523372650146, 'learning_rate': 0.0002539249146757679, 'epoch': 0.17}
{'loss': 1.118, 'grad_norm': 1.1747310161590576, 'learning_rate': 0.00024368600682593853, 'epoch': 0.2}
{'loss': 1.2846, 'grad_norm': 0.8456255197525024, 'learning_rate': 0.0002334470989761092, 'epoch': 0.23}
{'loss': 0.8655, 'grad_norm': 0.7855096459388733, 'learning_rate': 0.00022320819112627984, 'epoch': 0.27}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.2599, 'train_samples_per_second': 47.556, 'train_steps_per_second': 5.945, 'train_loss': 1.3242378997802735, 'epoch': 0.29}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_11/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.54
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), tensor(0.1469), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8667), tensor(0.0580)], [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), tensor(0.2273), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8683), tensor(0.0473)], [tensor(0.1745), tensor(0.0784), tensor(0.0793), tensor(0.0392), tensor(0.1523), tensor(0.0124), tensor(0.1925), tensor(0.2715), tensor(0.2022), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7432), tensor(0.0662)], [tensor(0.0894), tensor(0.0980), tensor(0.0828), tensor(0.0563), tensor(0.1756), tensor(0.0663), tensor(0.1821), tensor(0.2494), tensor(0.2663), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.0473)], [tensor(0.2018), tensor(0.1324), tensor(0.0700), tensor(0.0856), tensor(0.0794), tensor(0.0815), tensor(0.1644), tensor(0.1850), tensor(0.1562), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7390), tensor(0.0732)], [tensor(0.1366), tensor(0.1227), tensor(0.0808), tensor(0.0714), tensor(0.1215), tensor(0.1047), tensor(0.1713), tensor(0.1909), tensor(0.1311), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7103), tensor(0.0602)], [tensor(0.2486), tensor(0.1345), tensor(0.0601), tensor(0.0962), tensor(0.0560), tensor(0.0464), tensor(0.1603), tensor(0.1979), tensor(0.2362), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8100), tensor(0.0813)], [tensor(0.1626), tensor(0.0847), tensor(0.1250), tensor(0.0371), tensor(0.1321), tensor(0.1127), tensor(0.1555), tensor(0.1903), tensor(0.1371), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7514), tensor(0.0482)], [tensor(0.1612), tensor(0.1044), tensor(0.0457), tensor(0.0568), tensor(0.1502), tensor(0.1033), tensor(0.2165), tensor(0.1620), tensor(0.1549), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7439), tensor(0.0659)]]
proposed candidate before processing: tensor([0.1198, 0.1333, 0.1399, 0.0796, 0.0820, 0.0697, 0.1017, 0.2741, 0.1327,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7368, 0.0510])
proposed candidate after normalizing: [tensor(0.1198), tensor(0.1333), tensor(0.1399), tensor(0.0796), tensor(0.0820), tensor(0.0697), tensor(0.1017), tensor(0.2741), 4, 1, 1, 1, 1, 1, 94, 0.05101441219449043]
iteration:  12
input_X:  [tensor(0.1198), tensor(0.1333), tensor(0.1399), tensor(0.0796), tensor(0.0820), tensor(0.0697), tensor(0.1017), tensor(0.2741), 4, 1, 1, 1, 1, 1, 94, 0.05101441219449043]
mixing data with method:  random
arranging lora config with parameters:  94 0.05101441219449043 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 32,314,368 || all params: 8,062,575,616 || trainable%: 0.4008
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1198), tensor(0.1333), tensor(0.1399), tensor(0.0796), tensor(0.0820), tensor(0.0697), tensor(0.1017), tensor(0.2741)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1198)
number of datapoints needed (ratio * total):  598
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1333)
number of datapoints needed (ratio * total):  666
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1399)
number of datapoints needed (ratio * total):  699
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0796)
number of datapoints needed (ratio * total):  397
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0820)
number of datapoints needed (ratio * total):  409
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0697)
number of datapoints needed (ratio * total):  348
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1017)
number of datapoints needed (ratio * total):  508
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2741)
number of datapoints needed (ratio * total):  1370
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 598
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 666
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 699
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 397
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 409
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 348
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 508
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1370
})]
length of training data:  4995
training model...
trainable params: 32,314,368 || all params: 8,062,575,616 || trainable%: 0.4008
{'loss': 1.7406, 'grad_norm': 0.6322089433670044, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.3396, 'grad_norm': 1.8886704444885254, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.2272, 'grad_norm': 0.5231475830078125, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.491, 'grad_norm': 0.7619459629058838, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.582, 'grad_norm': 0.7838684320449829, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.2678, 'grad_norm': 0.6440011858940125, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.3366, 'grad_norm': 0.7887253761291504, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6446, 'train_samples_per_second': 49.63, 'train_steps_per_second': 6.21, 'train_loss': 1.4298043446998074, 'epoch': 0.23}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_12/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.65
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), tensor(0.1469), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8667), tensor(0.0580)], [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), tensor(0.2273), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8683), tensor(0.0473)], [tensor(0.1745), tensor(0.0784), tensor(0.0793), tensor(0.0392), tensor(0.1523), tensor(0.0124), tensor(0.1925), tensor(0.2715), tensor(0.2022), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7432), tensor(0.0662)], [tensor(0.0894), tensor(0.0980), tensor(0.0828), tensor(0.0563), tensor(0.1756), tensor(0.0663), tensor(0.1821), tensor(0.2494), tensor(0.2663), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.0473)], [tensor(0.2018), tensor(0.1324), tensor(0.0700), tensor(0.0856), tensor(0.0794), tensor(0.0815), tensor(0.1644), tensor(0.1850), tensor(0.1562), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7390), tensor(0.0732)], [tensor(0.1366), tensor(0.1227), tensor(0.0808), tensor(0.0714), tensor(0.1215), tensor(0.1047), tensor(0.1713), tensor(0.1909), tensor(0.1311), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7103), tensor(0.0602)], [tensor(0.2486), tensor(0.1345), tensor(0.0601), tensor(0.0962), tensor(0.0560), tensor(0.0464), tensor(0.1603), tensor(0.1979), tensor(0.2362), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8100), tensor(0.0813)], [tensor(0.1626), tensor(0.0847), tensor(0.1250), tensor(0.0371), tensor(0.1321), tensor(0.1127), tensor(0.1555), tensor(0.1903), tensor(0.1371), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7514), tensor(0.0482)], [tensor(0.1612), tensor(0.1044), tensor(0.0457), tensor(0.0568), tensor(0.1502), tensor(0.1033), tensor(0.2165), tensor(0.1620), tensor(0.1549), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7439), tensor(0.0659)], [tensor(0.1198), tensor(0.1333), tensor(0.1399), tensor(0.0796), tensor(0.0820), tensor(0.0697), tensor(0.1017), tensor(0.2741), tensor(0.1327), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7368), tensor(0.0510)]]
proposed candidate before processing: tensor([0.1271, 0.0905, 0.1501, 0.0864, 0.0689, 0.0798, 0.1566, 0.2406, 0.1533,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7507, 0.0373])
proposed candidate after normalizing: [tensor(0.1271), tensor(0.0905), tensor(0.1501), tensor(0.0864), tensor(0.0689), tensor(0.0798), tensor(0.1566), tensor(0.2406), 5, 1, 1, 1, 1, 1, 96, 0.03734041005373001]
iteration:  13
input_X:  [tensor(0.1271), tensor(0.0905), tensor(0.1501), tensor(0.0864), tensor(0.0689), tensor(0.0798), tensor(0.1566), tensor(0.2406), 5, 1, 1, 1, 1, 1, 96, 0.03734041005373001]
mixing data with method:  random
arranging lora config with parameters:  96 0.03734041005373001 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 32,931,840 || all params: 8,063,193,088 || trainable%: 0.4084
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1271), tensor(0.0905), tensor(0.1501), tensor(0.0864), tensor(0.0689), tensor(0.0798), tensor(0.1566), tensor(0.2406)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1271)
number of datapoints needed (ratio * total):  635
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.0905)
number of datapoints needed (ratio * total):  452
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.1501)
number of datapoints needed (ratio * total):  750
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0864)
number of datapoints needed (ratio * total):  432
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.0689)
number of datapoints needed (ratio * total):  344
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0798)
number of datapoints needed (ratio * total):  399
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1566)
number of datapoints needed (ratio * total):  782
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2406)
number of datapoints needed (ratio * total):  1203
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 635
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 452
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 750
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 432
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 344
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 399
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 782
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1203
})]
length of training data:  4997
training model...
trainable params: 32,931,840 || all params: 8,063,193,088 || trainable%: 0.4084
{'loss': 2.1109, 'grad_norm': 0.7569410800933838, 'learning_rate': 0.0002951219512195122, 'epoch': 0.03}
{'loss': 1.5793, 'grad_norm': 1.8558812141418457, 'learning_rate': 0.00028536585365853654, 'epoch': 0.06}
{'loss': 1.556, 'grad_norm': 0.6299065351486206, 'learning_rate': 0.00027560975609756093, 'epoch': 0.1}
{'loss': 1.1844, 'grad_norm': 0.7166881561279297, 'learning_rate': 0.0002658536585365854, 'epoch': 0.13}
{'loss': 1.4654, 'grad_norm': 0.8641175031661987, 'learning_rate': 0.0002560975609756097, 'epoch': 0.16}
{'loss': 1.4449, 'grad_norm': 0.5378794074058533, 'learning_rate': 0.00024634146341463416, 'epoch': 0.19}
{'loss': 1.4496, 'grad_norm': 0.9351765513420105, 'learning_rate': 0.0002365853658536585, 'epoch': 0.22}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0446, 'train_samples_per_second': 49.948, 'train_steps_per_second': 6.247, 'train_loss': 1.5391244421472083, 'epoch': 0.23}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_13/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.45
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), tensor(0.1469), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8667), tensor(0.0580)], [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), tensor(0.2273), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8683), tensor(0.0473)], [tensor(0.1745), tensor(0.0784), tensor(0.0793), tensor(0.0392), tensor(0.1523), tensor(0.0124), tensor(0.1925), tensor(0.2715), tensor(0.2022), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7432), tensor(0.0662)], [tensor(0.0894), tensor(0.0980), tensor(0.0828), tensor(0.0563), tensor(0.1756), tensor(0.0663), tensor(0.1821), tensor(0.2494), tensor(0.2663), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.0473)], [tensor(0.2018), tensor(0.1324), tensor(0.0700), tensor(0.0856), tensor(0.0794), tensor(0.0815), tensor(0.1644), tensor(0.1850), tensor(0.1562), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7390), tensor(0.0732)], [tensor(0.1366), tensor(0.1227), tensor(0.0808), tensor(0.0714), tensor(0.1215), tensor(0.1047), tensor(0.1713), tensor(0.1909), tensor(0.1311), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7103), tensor(0.0602)], [tensor(0.2486), tensor(0.1345), tensor(0.0601), tensor(0.0962), tensor(0.0560), tensor(0.0464), tensor(0.1603), tensor(0.1979), tensor(0.2362), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8100), tensor(0.0813)], [tensor(0.1626), tensor(0.0847), tensor(0.1250), tensor(0.0371), tensor(0.1321), tensor(0.1127), tensor(0.1555), tensor(0.1903), tensor(0.1371), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7514), tensor(0.0482)], [tensor(0.1612), tensor(0.1044), tensor(0.0457), tensor(0.0568), tensor(0.1502), tensor(0.1033), tensor(0.2165), tensor(0.1620), tensor(0.1549), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7439), tensor(0.0659)], [tensor(0.1198), tensor(0.1333), tensor(0.1399), tensor(0.0796), tensor(0.0820), tensor(0.0697), tensor(0.1017), tensor(0.2741), tensor(0.1327), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7368), tensor(0.0510)], [tensor(0.1271), tensor(0.0905), tensor(0.1501), tensor(0.0864), tensor(0.0689), tensor(0.0798), tensor(0.1566), tensor(0.2406), tensor(0.1533), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7507), tensor(0.0373)]]
proposed candidate before processing: tensor([0.1587, 0.1827, 0.0582, 0.0236, 0.1800, 0.0835, 0.0678, 0.2454, 0.1272,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7506, 0.0882])
proposed candidate after normalizing: [tensor(0.1587), tensor(0.1827), tensor(0.0582), 0, tensor(0.1800), tensor(0.0835), tensor(0.0678), tensor(0.2454), 4, 1, 1, 1, 1, 1, 96, 0.08815637230873108]
iteration:  14
input_X:  [tensor(0.1587), tensor(0.1827), tensor(0.0582), 0, tensor(0.1800), tensor(0.0835), tensor(0.0678), tensor(0.2454), 4, 1, 1, 1, 1, 1, 96, 0.08815637230873108]
mixing data with method:  random
arranging lora config with parameters:  96 0.08815637230873108 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 32,931,840 || all params: 8,063,193,088 || trainable%: 0.4084
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1587), tensor(0.1827), tensor(0.0582), 0, tensor(0.1800), tensor(0.0835), tensor(0.0678), tensor(0.2454)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1587)
number of datapoints needed (ratio * total):  793
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1827)
number of datapoints needed (ratio * total):  913
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0582)
number of datapoints needed (ratio * total):  291
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1800)
number of datapoints needed (ratio * total):  899
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0835)
number of datapoints needed (ratio * total):  417
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0678)
number of datapoints needed (ratio * total):  338
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2454)
number of datapoints needed (ratio * total):  1226
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 793
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 913
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 291
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 899
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 417
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 338
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1226
})]
length of training data:  4877
training model...
trainable params: 32,931,840 || all params: 8,063,193,088 || trainable%: 0.4084
{'loss': 1.6932, 'grad_norm': 0.7781372666358948, 'learning_rate': 0.00029499999999999996, 'epoch': 0.03}
{'loss': 1.6934, 'grad_norm': 0.9525348544120789, 'learning_rate': 0.000285, 'epoch': 0.07}
{'loss': 1.3585, 'grad_norm': 0.5278581380844116, 'learning_rate': 0.00027499999999999996, 'epoch': 0.1}
{'loss': 1.0717, 'grad_norm': 0.8128979206085205, 'learning_rate': 0.000265, 'epoch': 0.13}
{'loss': 1.2647, 'grad_norm': 0.5864421725273132, 'learning_rate': 0.00025499999999999996, 'epoch': 0.16}
{'loss': 1.0997, 'grad_norm': 0.4325940012931824, 'learning_rate': 0.000245, 'epoch': 0.2}
{'loss': 1.2994, 'grad_norm': 0.40993908047676086, 'learning_rate': 0.00023499999999999997, 'epoch': 0.23}
{'loss': 1.1684, 'grad_norm': 0.5108223557472229, 'learning_rate': 0.000225, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.1418, 'train_samples_per_second': 48.701, 'train_steps_per_second': 6.091, 'train_loss': 1.3220334940178449, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_14/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.69
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), tensor(0.1469), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8667), tensor(0.0580)], [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), tensor(0.2273), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8683), tensor(0.0473)], [tensor(0.1745), tensor(0.0784), tensor(0.0793), tensor(0.0392), tensor(0.1523), tensor(0.0124), tensor(0.1925), tensor(0.2715), tensor(0.2022), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7432), tensor(0.0662)], [tensor(0.0894), tensor(0.0980), tensor(0.0828), tensor(0.0563), tensor(0.1756), tensor(0.0663), tensor(0.1821), tensor(0.2494), tensor(0.2663), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.0473)], [tensor(0.2018), tensor(0.1324), tensor(0.0700), tensor(0.0856), tensor(0.0794), tensor(0.0815), tensor(0.1644), tensor(0.1850), tensor(0.1562), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7390), tensor(0.0732)], [tensor(0.1366), tensor(0.1227), tensor(0.0808), tensor(0.0714), tensor(0.1215), tensor(0.1047), tensor(0.1713), tensor(0.1909), tensor(0.1311), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7103), tensor(0.0602)], [tensor(0.2486), tensor(0.1345), tensor(0.0601), tensor(0.0962), tensor(0.0560), tensor(0.0464), tensor(0.1603), tensor(0.1979), tensor(0.2362), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8100), tensor(0.0813)], [tensor(0.1626), tensor(0.0847), tensor(0.1250), tensor(0.0371), tensor(0.1321), tensor(0.1127), tensor(0.1555), tensor(0.1903), tensor(0.1371), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7514), tensor(0.0482)], [tensor(0.1612), tensor(0.1044), tensor(0.0457), tensor(0.0568), tensor(0.1502), tensor(0.1033), tensor(0.2165), tensor(0.1620), tensor(0.1549), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7439), tensor(0.0659)], [tensor(0.1198), tensor(0.1333), tensor(0.1399), tensor(0.0796), tensor(0.0820), tensor(0.0697), tensor(0.1017), tensor(0.2741), tensor(0.1327), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7368), tensor(0.0510)], [tensor(0.1271), tensor(0.0905), tensor(0.1501), tensor(0.0864), tensor(0.0689), tensor(0.0798), tensor(0.1566), tensor(0.2406), tensor(0.1533), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7507), tensor(0.0373)], [tensor(0.1587), tensor(0.1827), tensor(0.0582), tensor(0.0236), tensor(0.1800), tensor(0.0835), tensor(0.0678), tensor(0.2454), tensor(0.1272), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7506), tensor(0.0882)]]
proposed candidate before processing: tensor([0.1591, 0.1857, 0.0595, 0.0229, 0.1806, 0.0858, 0.0606, 0.2457, 0.1294,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7492, 0.0871])
proposed candidate after normalizing: [tensor(0.1591), tensor(0.1857), tensor(0.0595), 0, tensor(0.1806), tensor(0.0858), tensor(0.0606), tensor(0.2457), 4, 1, 1, 1, 1, 1, 96, 0.08710644394159317]
iteration:  15
input_X:  [tensor(0.1591), tensor(0.1857), tensor(0.0595), 0, tensor(0.1806), tensor(0.0858), tensor(0.0606), tensor(0.2457), 4, 1, 1, 1, 1, 1, 96, 0.08710644394159317]
mixing data with method:  random
arranging lora config with parameters:  96 0.08710644394159317 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1591), tensor(0.1857), tensor(0.0595), 0, tensor(0.1806), tensor(0.0858), tensor(0.0606), tensor(0.2457)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1591)
number of datapoints needed (ratio * total):  795
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1857)
number of datapoints needed (ratio * total):  928
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0595)
number of datapoints needed (ratio * total):  297
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1806)
number of datapoints needed (ratio * total):  903
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0858)
number of datapoints needed (ratio * total):  429
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0606)
number of datapoints needed (ratio * total):  303
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2457)
number of datapoints needed (ratio * total):  1228
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 795
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 928
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 297
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 903
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 429
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 303
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1228
})]
length of training data:  4883
training model...
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
{'loss': 2.2547, 'grad_norm': 5.452254295349121, 'learning_rate': 0.00029500831946755406, 'epoch': 0.03}
{'loss': 1.462, 'grad_norm': 1.126683235168457, 'learning_rate': 0.00028502495840266223, 'epoch': 0.07}
{'loss': 1.4023, 'grad_norm': 0.8407842516899109, 'learning_rate': 0.00027504159733777035, 'epoch': 0.1}
{'loss': 1.3208, 'grad_norm': 1.1338483095169067, 'learning_rate': 0.0002650582362728785, 'epoch': 0.13}
{'loss': 1.3468, 'grad_norm': 1.1668720245361328, 'learning_rate': 0.0002550748752079867, 'epoch': 0.16}
{'loss': 1.3583, 'grad_norm': 0.7247395515441895, 'learning_rate': 0.0002450915141430948, 'epoch': 0.2}
{'loss': 1.4078, 'grad_norm': 0.5412048101425171, 'learning_rate': 0.00023510815307820296, 'epoch': 0.23}
{'loss': 1.4779, 'grad_norm': 0.7358411550521851, 'learning_rate': 0.00022512479201331113, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.0167, 'train_samples_per_second': 48.822, 'train_steps_per_second': 6.109, 'train_loss': 1.486851253728757, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_15/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.59
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), tensor(0.1469), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8667), tensor(0.0580)], [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), tensor(0.2273), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8683), tensor(0.0473)], [tensor(0.1745), tensor(0.0784), tensor(0.0793), tensor(0.0392), tensor(0.1523), tensor(0.0124), tensor(0.1925), tensor(0.2715), tensor(0.2022), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7432), tensor(0.0662)], [tensor(0.0894), tensor(0.0980), tensor(0.0828), tensor(0.0563), tensor(0.1756), tensor(0.0663), tensor(0.1821), tensor(0.2494), tensor(0.2663), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.0473)], [tensor(0.2018), tensor(0.1324), tensor(0.0700), tensor(0.0856), tensor(0.0794), tensor(0.0815), tensor(0.1644), tensor(0.1850), tensor(0.1562), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7390), tensor(0.0732)], [tensor(0.1366), tensor(0.1227), tensor(0.0808), tensor(0.0714), tensor(0.1215), tensor(0.1047), tensor(0.1713), tensor(0.1909), tensor(0.1311), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7103), tensor(0.0602)], [tensor(0.2486), tensor(0.1345), tensor(0.0601), tensor(0.0962), tensor(0.0560), tensor(0.0464), tensor(0.1603), tensor(0.1979), tensor(0.2362), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8100), tensor(0.0813)], [tensor(0.1626), tensor(0.0847), tensor(0.1250), tensor(0.0371), tensor(0.1321), tensor(0.1127), tensor(0.1555), tensor(0.1903), tensor(0.1371), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7514), tensor(0.0482)], [tensor(0.1612), tensor(0.1044), tensor(0.0457), tensor(0.0568), tensor(0.1502), tensor(0.1033), tensor(0.2165), tensor(0.1620), tensor(0.1549), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7439), tensor(0.0659)], [tensor(0.1198), tensor(0.1333), tensor(0.1399), tensor(0.0796), tensor(0.0820), tensor(0.0697), tensor(0.1017), tensor(0.2741), tensor(0.1327), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7368), tensor(0.0510)], [tensor(0.1271), tensor(0.0905), tensor(0.1501), tensor(0.0864), tensor(0.0689), tensor(0.0798), tensor(0.1566), tensor(0.2406), tensor(0.1533), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7507), tensor(0.0373)], [tensor(0.1587), tensor(0.1827), tensor(0.0582), tensor(0.0236), tensor(0.1800), tensor(0.0835), tensor(0.0678), tensor(0.2454), tensor(0.1272), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7506), tensor(0.0882)], [tensor(0.1591), tensor(0.1857), tensor(0.0595), tensor(0.0229), tensor(0.1806), tensor(0.0858), tensor(0.0606), tensor(0.2457), tensor(0.1294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7492), tensor(0.0871)]]
proposed candidate before processing: tensor([0.1299, 0.1931, 0.0822, 0.0439, 0.1703, 0.0399, 0.1247, 0.2161, 0.1279,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7583, 0.1000])
proposed candidate after normalizing: [tensor(0.1299), tensor(0.1931), tensor(0.0822), 0, tensor(0.1703), 0, tensor(0.1247), tensor(0.2161), 4, 1, 1, 1, 1, 1, 97, 0.10000000149011612]
iteration:  16
input_X:  [tensor(0.1299), tensor(0.1931), tensor(0.0822), 0, tensor(0.1703), 0, tensor(0.1247), tensor(0.2161), 4, 1, 1, 1, 1, 1, 97, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  97 0.10000000149011612 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 26,619,904 || all params: 8,056,881,152 || trainable%: 0.3304
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1299), tensor(0.1931), tensor(0.0822), 0, tensor(0.1703), 0, tensor(0.1247), tensor(0.2161)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1299)
number of datapoints needed (ratio * total):  649
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1931)
number of datapoints needed (ratio * total):  965
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0822)
number of datapoints needed (ratio * total):  410
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1703)
number of datapoints needed (ratio * total):  851
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1247)
number of datapoints needed (ratio * total):  623
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2161)
number of datapoints needed (ratio * total):  1080
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 649
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 965
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 410
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 851
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 623
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1080
})]
length of training data:  4578
training model...
trainable params: 26,619,904 || all params: 8,056,881,152 || trainable%: 0.3304
{'loss': 2.29, 'grad_norm': 1.0675888061523438, 'learning_rate': 0.00029467140319715805, 'epoch': 0.03}
{'loss': 1.4062, 'grad_norm': 1.9563993215560913, 'learning_rate': 0.0002840142095914742, 'epoch': 0.07}
{'loss': 1.5569, 'grad_norm': 1.1024296283721924, 'learning_rate': 0.0002733570159857904, 'epoch': 0.1}
{'loss': 1.0506, 'grad_norm': 0.6998422145843506, 'learning_rate': 0.0002626998223801066, 'epoch': 0.14}
{'loss': 1.2861, 'grad_norm': 0.7842277884483337, 'learning_rate': 0.00025204262877442273, 'epoch': 0.17}
{'loss': 1.4057, 'grad_norm': 0.5982063412666321, 'learning_rate': 0.00024138543516873887, 'epoch': 0.21}
{'loss': 1.4265, 'grad_norm': 1.1476070880889893, 'learning_rate': 0.00023072824156305502, 'epoch': 0.24}
{'loss': 1.3224, 'grad_norm': 0.6487073302268982, 'learning_rate': 0.00022007104795737122, 'epoch': 0.28}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.6901, 'train_samples_per_second': 45.466, 'train_steps_per_second': 5.691, 'train_loss': 1.4564018941219943, 'epoch': 0.28}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_16/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), tensor(0.1469), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8667), tensor(0.0580)], [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), tensor(0.2273), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8683), tensor(0.0473)], [tensor(0.1745), tensor(0.0784), tensor(0.0793), tensor(0.0392), tensor(0.1523), tensor(0.0124), tensor(0.1925), tensor(0.2715), tensor(0.2022), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7432), tensor(0.0662)], [tensor(0.0894), tensor(0.0980), tensor(0.0828), tensor(0.0563), tensor(0.1756), tensor(0.0663), tensor(0.1821), tensor(0.2494), tensor(0.2663), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.0473)], [tensor(0.2018), tensor(0.1324), tensor(0.0700), tensor(0.0856), tensor(0.0794), tensor(0.0815), tensor(0.1644), tensor(0.1850), tensor(0.1562), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7390), tensor(0.0732)], [tensor(0.1366), tensor(0.1227), tensor(0.0808), tensor(0.0714), tensor(0.1215), tensor(0.1047), tensor(0.1713), tensor(0.1909), tensor(0.1311), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7103), tensor(0.0602)], [tensor(0.2486), tensor(0.1345), tensor(0.0601), tensor(0.0962), tensor(0.0560), tensor(0.0464), tensor(0.1603), tensor(0.1979), tensor(0.2362), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8100), tensor(0.0813)], [tensor(0.1626), tensor(0.0847), tensor(0.1250), tensor(0.0371), tensor(0.1321), tensor(0.1127), tensor(0.1555), tensor(0.1903), tensor(0.1371), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7514), tensor(0.0482)], [tensor(0.1612), tensor(0.1044), tensor(0.0457), tensor(0.0568), tensor(0.1502), tensor(0.1033), tensor(0.2165), tensor(0.1620), tensor(0.1549), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7439), tensor(0.0659)], [tensor(0.1198), tensor(0.1333), tensor(0.1399), tensor(0.0796), tensor(0.0820), tensor(0.0697), tensor(0.1017), tensor(0.2741), tensor(0.1327), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7368), tensor(0.0510)], [tensor(0.1271), tensor(0.0905), tensor(0.1501), tensor(0.0864), tensor(0.0689), tensor(0.0798), tensor(0.1566), tensor(0.2406), tensor(0.1533), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7507), tensor(0.0373)], [tensor(0.1587), tensor(0.1827), tensor(0.0582), tensor(0.0236), tensor(0.1800), tensor(0.0835), tensor(0.0678), tensor(0.2454), tensor(0.1272), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7506), tensor(0.0882)], [tensor(0.1591), tensor(0.1857), tensor(0.0595), tensor(0.0229), tensor(0.1806), tensor(0.0858), tensor(0.0606), tensor(0.2457), tensor(0.1294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7492), tensor(0.0871)], [tensor(0.1299), tensor(0.1931), tensor(0.0822), tensor(0.0439), tensor(0.1703), tensor(0.0399), tensor(0.1247), tensor(0.2161), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7583), tensor(0.1000)]]
proposed candidate before processing: tensor([0.1509, 0.1288, 0.0340, 0.0449, 0.1492, 0.1236, 0.0927, 0.2759, 0.1403,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7500, 0.0807])
proposed candidate after normalizing: [tensor(0.1509), tensor(0.1288), 0, 0, tensor(0.1492), tensor(0.1236), tensor(0.0927), tensor(0.2759), 4, 1, 1, 1, 1, 1, 96, 0.08065231144428253]
iteration:  17
input_X:  [tensor(0.1509), tensor(0.1288), 0, 0, tensor(0.1492), tensor(0.1236), tensor(0.0927), tensor(0.2759), 4, 1, 1, 1, 1, 1, 96, 0.08065231144428253]
mixing data with method:  random
arranging lora config with parameters:  96 0.08065231144428253 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1509), tensor(0.1288), 0, 0, tensor(0.1492), tensor(0.1236), tensor(0.0927), tensor(0.2759)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1509)
number of datapoints needed (ratio * total):  754
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1288)
number of datapoints needed (ratio * total):  644
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1492)
number of datapoints needed (ratio * total):  746
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.1236)
number of datapoints needed (ratio * total):  618
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0927)
number of datapoints needed (ratio * total):  463
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2759)
number of datapoints needed (ratio * total):  1379
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 754
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 644
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 746
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 618
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 463
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1379
})]
length of training data:  4604
training model...
trainable params: 26,345,472 || all params: 8,056,606,720 || trainable%: 0.3270
{'loss': 2.5648, 'grad_norm': 0.8161123991012573, 'learning_rate': 0.0002946996466431095, 'epoch': 0.03}
{'loss': 1.4169, 'grad_norm': 0.97447270154953, 'learning_rate': 0.0002840989399293286, 'epoch': 0.07}
{'loss': 1.6798, 'grad_norm': 0.740597128868103, 'learning_rate': 0.0002734982332155477, 'epoch': 0.1}
{'loss': 1.3499, 'grad_norm': 0.7948805689811707, 'learning_rate': 0.00026289752650176677, 'epoch': 0.14}
{'loss': 1.6163, 'grad_norm': 0.456172376871109, 'learning_rate': 0.0002522968197879858, 'epoch': 0.17}
{'loss': 1.5158, 'grad_norm': 0.6676614284515381, 'learning_rate': 0.0002416961130742049, 'epoch': 0.21}
{'loss': 1.3643, 'grad_norm': 0.865531325340271, 'learning_rate': 0.00023109540636042402, 'epoch': 0.24}
{'loss': 1.4794, 'grad_norm': 0.7435101866722107, 'learning_rate': 0.0002204946996466431, 'epoch': 0.28}
{'loss': 1.4571, 'grad_norm': 0.7423043847084045, 'learning_rate': 0.00020989399293286216, 'epoch': 0.31}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3905, 'train_samples_per_second': 45.861, 'train_steps_per_second': 5.738, 'train_loss': 1.607796514221511, 'epoch': 0.33}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_17/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.62
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), tensor(0.1469), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8667), tensor(0.0580)], [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), tensor(0.2273), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8683), tensor(0.0473)], [tensor(0.1745), tensor(0.0784), tensor(0.0793), tensor(0.0392), tensor(0.1523), tensor(0.0124), tensor(0.1925), tensor(0.2715), tensor(0.2022), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7432), tensor(0.0662)], [tensor(0.0894), tensor(0.0980), tensor(0.0828), tensor(0.0563), tensor(0.1756), tensor(0.0663), tensor(0.1821), tensor(0.2494), tensor(0.2663), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.0473)], [tensor(0.2018), tensor(0.1324), tensor(0.0700), tensor(0.0856), tensor(0.0794), tensor(0.0815), tensor(0.1644), tensor(0.1850), tensor(0.1562), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7390), tensor(0.0732)], [tensor(0.1366), tensor(0.1227), tensor(0.0808), tensor(0.0714), tensor(0.1215), tensor(0.1047), tensor(0.1713), tensor(0.1909), tensor(0.1311), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7103), tensor(0.0602)], [tensor(0.2486), tensor(0.1345), tensor(0.0601), tensor(0.0962), tensor(0.0560), tensor(0.0464), tensor(0.1603), tensor(0.1979), tensor(0.2362), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8100), tensor(0.0813)], [tensor(0.1626), tensor(0.0847), tensor(0.1250), tensor(0.0371), tensor(0.1321), tensor(0.1127), tensor(0.1555), tensor(0.1903), tensor(0.1371), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7514), tensor(0.0482)], [tensor(0.1612), tensor(0.1044), tensor(0.0457), tensor(0.0568), tensor(0.1502), tensor(0.1033), tensor(0.2165), tensor(0.1620), tensor(0.1549), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7439), tensor(0.0659)], [tensor(0.1198), tensor(0.1333), tensor(0.1399), tensor(0.0796), tensor(0.0820), tensor(0.0697), tensor(0.1017), tensor(0.2741), tensor(0.1327), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7368), tensor(0.0510)], [tensor(0.1271), tensor(0.0905), tensor(0.1501), tensor(0.0864), tensor(0.0689), tensor(0.0798), tensor(0.1566), tensor(0.2406), tensor(0.1533), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7507), tensor(0.0373)], [tensor(0.1587), tensor(0.1827), tensor(0.0582), tensor(0.0236), tensor(0.1800), tensor(0.0835), tensor(0.0678), tensor(0.2454), tensor(0.1272), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7506), tensor(0.0882)], [tensor(0.1591), tensor(0.1857), tensor(0.0595), tensor(0.0229), tensor(0.1806), tensor(0.0858), tensor(0.0606), tensor(0.2457), tensor(0.1294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7492), tensor(0.0871)], [tensor(0.1299), tensor(0.1931), tensor(0.0822), tensor(0.0439), tensor(0.1703), tensor(0.0399), tensor(0.1247), tensor(0.2161), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7583), tensor(0.1000)], [tensor(0.1509), tensor(0.1288), tensor(0.0340), tensor(0.0449), tensor(0.1492), tensor(0.1236), tensor(0.0927), tensor(0.2759), tensor(0.1403), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7500), tensor(0.0807)]]
proposed candidate before processing: tensor([0.1719, 0.1764, 0.0743, 0.0294, 0.1686, 0.0658, 0.1000, 0.2136, 0.1413,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7605, 0.0143])
proposed candidate after normalizing: [tensor(0.1719), tensor(0.1764), tensor(0.0743), 0, tensor(0.1686), tensor(0.0658), tensor(0.1000), tensor(0.2136), 5, 1, 1, 1, 1, 1, 97, 0.014295365661382675]
iteration:  18
input_X:  [tensor(0.1719), tensor(0.1764), tensor(0.0743), 0, tensor(0.1686), tensor(0.0658), tensor(0.1000), tensor(0.2136), 5, 1, 1, 1, 1, 1, 97, 0.014295365661382675]
mixing data with method:  random
arranging lora config with parameters:  97 0.014295365661382675 5 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 33,274,880 || all params: 8,063,536,128 || trainable%: 0.4127
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1719), tensor(0.1764), tensor(0.0743), 0, tensor(0.1686), tensor(0.0658), tensor(0.1000), tensor(0.2136)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1719)
number of datapoints needed (ratio * total):  859
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1764)
number of datapoints needed (ratio * total):  881
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0743)
number of datapoints needed (ratio * total):  371
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  0
number of datapoints needed (ratio * total):  0
doing sampling for domain:  sciq
ratio:  tensor(0.1686)
number of datapoints needed (ratio * total):  843
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0658)
number of datapoints needed (ratio * total):  329
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.1000)
number of datapoints needed (ratio * total):  499
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2136)
number of datapoints needed (ratio * total):  1067
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 859
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 881
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 371
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 843
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 329
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 499
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1067
})]
length of training data:  4849
training model...
trainable params: 33,274,880 || all params: 8,063,536,128 || trainable%: 0.4127
{'loss': 2.0352, 'grad_norm': 1.1717153787612915, 'learning_rate': 0.0002949748743718593, 'epoch': 0.03}
{'loss': 1.6756, 'grad_norm': 1.131217122077942, 'learning_rate': 0.0002849246231155779, 'epoch': 0.07}
{'loss': 1.4527, 'grad_norm': 0.4177298843860626, 'learning_rate': 0.0002748743718592965, 'epoch': 0.1}
{'loss': 1.4186, 'grad_norm': 0.8230826258659363, 'learning_rate': 0.0002648241206030151, 'epoch': 0.13}
{'loss': 1.1711, 'grad_norm': 0.7900497913360596, 'learning_rate': 0.0002547738693467337, 'epoch': 0.16}
{'loss': 1.4109, 'grad_norm': 0.672850489616394, 'learning_rate': 0.00024472361809045227, 'epoch': 0.2}
{'loss': 1.266, 'grad_norm': 1.1922926902770996, 'learning_rate': 0.00023467336683417084, 'epoch': 0.23}
{'loss': 1.357, 'grad_norm': 0.4917769730091095, 'learning_rate': 0.00022462311557788943, 'epoch': 0.26}
⏰ Max training time of 100 seconds reached. Stopping.
{'train_runtime': 100.3293, 'train_samples_per_second': 48.331, 'train_steps_per_second': 6.05, 'train_loss': 1.4776191126349514, 'epoch': 0.27}
saving final model LoRA weights at:  LLM/BO/vJkkf/BO_run_18/final_model_after_training
evaluating...
creating HFLM wrapper for model_path
evaluating on tasks:  ['gsm8k']
deleting lora model after evaluation.
current iteration weighted performance:  0.63
GP past observed values (should be between [0,1]):  [[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.5, 1, 1, 1, 1, 1, 0.5625, 0.05], [tensor(0.1320), tensor(0.0412), tensor(0.0666), tensor(0.3804), tensor(0.0453), tensor(0.0872), tensor(0.0189), tensor(0.2285), tensor(0.7649), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0406), tensor(0.0897)], [tensor(0.1265), tensor(0.1256), tensor(0.0871), tensor(0.0900), tensor(0.1257), tensor(0.1015), tensor(0.1582), tensor(0.1853), tensor(0.2839), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7487), tensor(0.0549)], [tensor(0.1274), tensor(0.1260), tensor(0.0631), tensor(0.0679), tensor(0.1261), tensor(0.0866), tensor(0.1793), tensor(0.2236), tensor(0.1469), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8667), tensor(0.0580)], [tensor(0.0824), tensor(0.1702), tensor(0.0712), tensor(0.1161), tensor(0.1012), tensor(0.1705), tensor(0.1466), tensor(0.1418), tensor(0.2273), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8683), tensor(0.0473)], [tensor(0.1745), tensor(0.0784), tensor(0.0793), tensor(0.0392), tensor(0.1523), tensor(0.0124), tensor(0.1925), tensor(0.2715), tensor(0.2022), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7432), tensor(0.0662)], [tensor(0.0894), tensor(0.0980), tensor(0.0828), tensor(0.0563), tensor(0.1756), tensor(0.0663), tensor(0.1821), tensor(0.2494), tensor(0.2663), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8249), tensor(0.0473)], [tensor(0.2018), tensor(0.1324), tensor(0.0700), tensor(0.0856), tensor(0.0794), tensor(0.0815), tensor(0.1644), tensor(0.1850), tensor(0.1562), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7390), tensor(0.0732)], [tensor(0.1366), tensor(0.1227), tensor(0.0808), tensor(0.0714), tensor(0.1215), tensor(0.1047), tensor(0.1713), tensor(0.1909), tensor(0.1311), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7103), tensor(0.0602)], [tensor(0.2486), tensor(0.1345), tensor(0.0601), tensor(0.0962), tensor(0.0560), tensor(0.0464), tensor(0.1603), tensor(0.1979), tensor(0.2362), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.8100), tensor(0.0813)], [tensor(0.1626), tensor(0.0847), tensor(0.1250), tensor(0.0371), tensor(0.1321), tensor(0.1127), tensor(0.1555), tensor(0.1903), tensor(0.1371), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7514), tensor(0.0482)], [tensor(0.1612), tensor(0.1044), tensor(0.0457), tensor(0.0568), tensor(0.1502), tensor(0.1033), tensor(0.2165), tensor(0.1620), tensor(0.1549), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7439), tensor(0.0659)], [tensor(0.1198), tensor(0.1333), tensor(0.1399), tensor(0.0796), tensor(0.0820), tensor(0.0697), tensor(0.1017), tensor(0.2741), tensor(0.1327), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7368), tensor(0.0510)], [tensor(0.1271), tensor(0.0905), tensor(0.1501), tensor(0.0864), tensor(0.0689), tensor(0.0798), tensor(0.1566), tensor(0.2406), tensor(0.1533), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7507), tensor(0.0373)], [tensor(0.1587), tensor(0.1827), tensor(0.0582), tensor(0.0236), tensor(0.1800), tensor(0.0835), tensor(0.0678), tensor(0.2454), tensor(0.1272), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7506), tensor(0.0882)], [tensor(0.1591), tensor(0.1857), tensor(0.0595), tensor(0.0229), tensor(0.1806), tensor(0.0858), tensor(0.0606), tensor(0.2457), tensor(0.1294), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7492), tensor(0.0871)], [tensor(0.1299), tensor(0.1931), tensor(0.0822), tensor(0.0439), tensor(0.1703), tensor(0.0399), tensor(0.1247), tensor(0.2161), tensor(0.1279), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7583), tensor(0.1000)], [tensor(0.1509), tensor(0.1288), tensor(0.0340), tensor(0.0449), tensor(0.1492), tensor(0.1236), tensor(0.0927), tensor(0.2759), tensor(0.1403), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7500), tensor(0.0807)], [tensor(0.1719), tensor(0.1764), tensor(0.0743), tensor(0.0294), tensor(0.1686), tensor(0.0658), tensor(0.1000), tensor(0.2136), tensor(0.1413), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.7605), tensor(0.0143)]]
proposed candidate before processing: tensor([0.1650, 0.1269, 0.0890, 0.0751, 0.1827, 0.0742, 0.0810, 0.2060, 0.1386,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7683, 0.1000])
proposed candidate after normalizing: [tensor(0.1650), tensor(0.1269), tensor(0.0890), tensor(0.0751), tensor(0.1827), tensor(0.0742), tensor(0.0810), tensor(0.2060), 4, 1, 1, 1, 1, 1, 98, 0.10000000149011612]
iteration:  19
input_X:  [tensor(0.1650), tensor(0.1269), tensor(0.0890), tensor(0.0751), tensor(0.1827), tensor(0.0742), tensor(0.0810), tensor(0.2060), 4, 1, 1, 1, 1, 1, 98, 0.10000000149011612]
mixing data with method:  random
arranging lora config with parameters:  98 0.10000000149011612 4 [1, 1, 1, 1, 1]
['q_proj', 'v_proj', 'up_proj', 'down_proj', 'gate_proj']
trainable params: 33,549,312 || all params: 8,063,810,560 || trainable%: 0.4160
tokenizing all data into correct format...
iterating through each data domain and sampling the sufficient datapoints
mixing ratio:  [tensor(0.1650), tensor(0.1269), tensor(0.0890), tensor(0.0751), tensor(0.1827), tensor(0.0742), tensor(0.0810), tensor(0.2060)]
ALL DATA DOMAINS:  ['commonsense_qa', 'headqa_en', 'hellaswag', 'pubmedqa', 'sciq', 'triviaqa', 'truthfulqa_gen', 'wikitext']
doing sampling for domain:  commonsense_qa
ratio:  tensor(0.1650)
number of datapoints needed (ratio * total):  825
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  headqa_en
ratio:  tensor(0.1269)
number of datapoints needed (ratio * total):  634
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  hellaswag
ratio:  tensor(0.0890)
number of datapoints needed (ratio * total):  445
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  pubmedqa
ratio:  tensor(0.0751)
number of datapoints needed (ratio * total):  375
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  sciq
ratio:  tensor(0.1827)
number of datapoints needed (ratio * total):  913
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  triviaqa
ratio:  tensor(0.0742)
number of datapoints needed (ratio * total):  371
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  truthfulqa_gen
ratio:  tensor(0.0810)
number of datapoints needed (ratio * total):  405
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
doing sampling for domain:  wikitext
ratio:  tensor(0.2060)
number of datapoints needed (ratio * total):  1030
sampling...
method to use:  random
method is to randomly sample
done sampling training
method to use:  random
method is to randomly sample
done sampling validation
done mapping!
[Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 825
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 634
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 445
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 375
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 913
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 371
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 405
}), Dataset({
    features: ['input_ids', 'attention_mask', 'labels'],
    num_rows: 1030
})]
length of training data:  4998
training model...
trainable params: 33,549,312 || all params: 8,063,810,560 || trainable%: 0.4160
